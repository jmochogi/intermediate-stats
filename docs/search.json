[
  {
    "objectID": "problem-sets/ps_3.html",
    "href": "problem-sets/ps_3.html",
    "title": "Problem Set 3: Linear & Logistic Regression",
    "section": "",
    "text": "Please check back later!",
    "crumbs": [
      "Problem Sets",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "problem-sets/ps-5.html",
    "href": "problem-sets/ps-5.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "Please come back later!"
  },
  {
    "objectID": "problem-sets/ps-1.html",
    "href": "problem-sets/ps-1.html",
    "title": "Problem Set 1: Review basic Concepts",
    "section": "",
    "text": "This problem set is meant to refresh some basic ideas and concepts you ought to have learned in your previous statistics classes. For every question, I have indicated the relevant section in the textbook for your reference. Some of these questions are very open-ended and you may want to be creative. There may be no one right/wrong answer.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problem-sets/ps-1.html#problem-1",
    "href": "problem-sets/ps-1.html#problem-1",
    "title": "Problem Set 1: Review basic Concepts",
    "section": "Problem 1",
    "text": "Problem 1\nA study was conducted about the smoking habits of some UK residents. Below is a data frame displaying a portion of the data collected in this study. A blank cell indicates that data for that variable was not available for a given respondent.  \n\nHow many variables (list them) and how many observations/cases are in the data frame? (see section 1.2)\nFor each variable identified in (a) above, state whether it is numerical or categorical. If numerical, state whether it is continuous or discrete. If categorical, state whether it is nominal or ordinal. (see section 1.2.2)\nAssuming that someone’s age affects the amount of cigarettes they smoke per day, which variable is the explanatory variable and which is the response variable? (see section 1.2.3)\nWhat visualizations would you use to display the relationship between age and amount of cigarettes smoked per day? Which variable would you place on the y-axis and which one on the x-axis? Does it matter what axis you place the variables? (see section 1.3.1)",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "problem-sets/ps-1.html#problem-2",
    "href": "problem-sets/ps-1.html#problem-2",
    "title": "Problem Set 1: Review basic Concepts",
    "section": "Problem 2",
    "text": "Problem 2\nThe first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that the data are reliable and help achieve the research goals. Suppose you are conducting a study trying to answer the following research question:   Does a new drug (call it drug X) reduce the number of deaths in patients with severe heart disease?\n\nWhat type of study (experimental or observational) would you conduct? Explain why? (see section 1.4) \nBriefly describe how you would set up your study and the data you would collect to answer the above question. In your description describe the variables and their types (i.e., categorical or numerical). (see sections 1.2.1 - 1.2.2 and section 2.2)\nSuppose you wanted to conduct your study in Tompkins County. Explain what your population of interest? What would be your sample and how would you obtain it? (see section 2.1 and 2.2)\nWhat are some potential ethical concerns you would need to consider before conducting your study?",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "MATH 246 - Intermediate Statistics",
    "section": "",
    "text": "This class will mostly be taught using a Flipped Model. A flipped classroom is aimed at increasing your engagement and learning by having you complete some activities (readings and/or videos) ahead of class. At the end of each pre-class activity, you will complete a short quiz that we will call class preparation assignment (CPA). In most cases, these CPAs will be graded only for effort and completion. When doing CPAs, you should write down your notes and any questions that you have so we can address it in class or during open hours. More information about the CPA is provided under Assessment section of this syllabus. Here are some benefits associated with using Flipped Classroom Models:\n\nIndependent Learning Skills: not everything you learn in school will be applicable directly in your job or in the real world. In most cases, you need to transfer your knowledge to new contexts and that often involves new learning (often on your own). A flipped class model sets you up for success as an independent learner. You will learn how to learn on your own.\nActive Learning: In a flipped classroom, you will be actively engaged in the learning process. You will be asked to think, to write, to discuss, to solve problems, to analyze, to create, to evaluate, and to apply. This is a much more effective way to learn than passively listening to a lecture.\nCatching up: If you miss a given class and you had completed your CPA, that means you will still have some understanding of the basic ideas. You do not miss out entirely and that means catching up is easier.\n\n\n\n\nFlipped Classroom Model\n\n\n\n\n\nCovers statistical methods not typically covered in introductory statistics courses. Topics include multivariate analysis and nonparametric techniques, bootstrapping and jackknife methods, and two-way ANOVA. Emphasis will be placed on working with data sets from a broad variety of disciplines with an exploratory data analysis approach. The statistical software environment R will be used in analyzing data.\n\n\n\nMATH 14400, MATH 14500, MATH 21600, or PSYC 20700 with a grade of C- or better. (S,Y) Attributes: NS, QL  Number of Credits: 3 Credits\nWorkload Expectation:\nThis is a 3 credits course. Credit is earned at Ithaca College in credit hours as measured by the Carnegie unit. The Carnegie unit is defined as one hour of classroom instruction and two hours of assignments outside the classroom, for a period of 15 weeks for each unit (credit).\n\n\n\n\n\n\nDay\nRoom/Hall\nTime\n\n\n\n\nMonday\nWilliams Hall 202\n11:45 am - 1:00 pm\n\n\nWednesday\nWilliams Hall 202\n8:30 - 9:45 am\n\n\nFriday\nWilliams Hall 202\n10:05 - 11:20 am\n\n\n\n\n\n\nUpon successful completion of this course, students will be able to:\n\nImplement the model-simulate-evaluate process to address data-driven questions on real-world topics.\nChoose and use various statistical analysis techniques (exploratory and inferential) in a variety of “real life” contexts including study design.\nCritically analyze the use of statistics in research, in the media, and in various day-to-day life situations.\nCommunicate statistical ideas effectively and in context without relying on statistical jargon.\nUse technology (e.g., R) to perform statistical explorations and manipulations.\nEffectively collaborate with peers on projects and presentations.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#about-the-course",
    "href": "course-syllabus.html#about-the-course",
    "title": "MATH 246 - Intermediate Statistics",
    "section": "",
    "text": "This class will mostly be taught using a Flipped Model. A flipped classroom is aimed at increasing your engagement and learning by having you complete some activities (readings and/or videos) ahead of class. At the end of each pre-class activity, you will complete a short quiz that we will call class preparation assignment (CPA). In most cases, these CPAs will be graded only for effort and completion. When doing CPAs, you should write down your notes and any questions that you have so we can address it in class or during open hours. More information about the CPA is provided under Assessment section of this syllabus. Here are some benefits associated with using Flipped Classroom Models:\n\nIndependent Learning Skills: not everything you learn in school will be applicable directly in your job or in the real world. In most cases, you need to transfer your knowledge to new contexts and that often involves new learning (often on your own). A flipped class model sets you up for success as an independent learner. You will learn how to learn on your own.\nActive Learning: In a flipped classroom, you will be actively engaged in the learning process. You will be asked to think, to write, to discuss, to solve problems, to analyze, to create, to evaluate, and to apply. This is a much more effective way to learn than passively listening to a lecture.\nCatching up: If you miss a given class and you had completed your CPA, that means you will still have some understanding of the basic ideas. You do not miss out entirely and that means catching up is easier.\n\n\n\n\nFlipped Classroom Model\n\n\n\n\n\nCovers statistical methods not typically covered in introductory statistics courses. Topics include multivariate analysis and nonparametric techniques, bootstrapping and jackknife methods, and two-way ANOVA. Emphasis will be placed on working with data sets from a broad variety of disciplines with an exploratory data analysis approach. The statistical software environment R will be used in analyzing data.\n\n\n\nMATH 14400, MATH 14500, MATH 21600, or PSYC 20700 with a grade of C- or better. (S,Y) Attributes: NS, QL  Number of Credits: 3 Credits\nWorkload Expectation:\nThis is a 3 credits course. Credit is earned at Ithaca College in credit hours as measured by the Carnegie unit. The Carnegie unit is defined as one hour of classroom instruction and two hours of assignments outside the classroom, for a period of 15 weeks for each unit (credit).\n\n\n\n\n\n\nDay\nRoom/Hall\nTime\n\n\n\n\nMonday\nWilliams Hall 202\n11:45 am - 1:00 pm\n\n\nWednesday\nWilliams Hall 202\n8:30 - 9:45 am\n\n\nFriday\nWilliams Hall 202\n10:05 - 11:20 am\n\n\n\n\n\n\nUpon successful completion of this course, students will be able to:\n\nImplement the model-simulate-evaluate process to address data-driven questions on real-world topics.\nChoose and use various statistical analysis techniques (exploratory and inferential) in a variety of “real life” contexts including study design.\nCritically analyze the use of statistics in research, in the media, and in various day-to-day life situations.\nCommunicate statistical ideas effectively and in context without relying on statistical jargon.\nUse technology (e.g., R) to perform statistical explorations and manipulations.\nEffectively collaborate with peers on projects and presentations.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-resources",
    "href": "course-syllabus.html#course-resources",
    "title": "MATH 246 - Intermediate Statistics",
    "section": "Course Resources",
    "text": "Course Resources\n\nTextbooks\nFor most of our readings, we shall use the following open-source textbooks:\n\nIntroduction to Modern Statistics, Çetinkaya-Rundel, Hardin. OpenIntro Inc., 2nd Edition, 2023. Hard copy of the book available on Amazon.\nR for Data Science, 2e, Wickham, Çetinkaya-Rundel, Grolemund. O’Reilly, 2nd edition, 2023. Hard copy available on Amazon.\nIntermediate Statistics with R by Greenwood, Mark. The book can be found freely on this link.\n\n\n\nComputing:\n\nR: We will use R statistical environment with the RStudio interface. You’ll be using R primarily through a version of RStudio accessible on posit cloud. Once you are on this page, sign in or click on sign up to create a new account in case you don’t already have an account.\nGitHub Copilot: GitHub is a web-based platform that allows people to store, share, and manage versions of their code. For example, a team of programmers working on different parts of a certain project may use GitHub for collaboration. We shall use these features only sparingly in this course. Copilot, on the other hand, is a popular AI-powered tool used for a vast range of purposes including code generation. Copilot is a paid service but college students can access and use it for free via GitHub (GitHub Copilot). To use GitHub copilot and use it within RStudio, there are three steps involved:\n\nStep 1: Sign up for a free GitHub account at GitHub.\nStep 2: Sign up for a free GitHub Student Developer Pack account at GitHub Student Developer Pack.\nStep 3: Install the GitHub Copilot extension in RStudio. To do this,\n\nOptional: You may want to have a scientific calculator (one that has the ability to compute powers–e.g., \\((1.675)^3)\\) to use when you want to do quick calculations. You may also use online calculator tools such as desmos.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "MATH 246 - Intermediate Statistics",
    "section": "Assessment",
    "text": "Assessment\nYour learning will be assessed in a variety of ways including Attendance and Participation (AP), CPA’s, Labs, Problem Sets (PS), Projects, Exams, and a final Portfolio (takes the place of a final exam). Below are the details on these components.\n\nAttendance and Participation (AP)\nYou’re expected to attend all classes and participate during class discussions. Consistent attendance and participation are strong indicators of success in MAT 246. You will be responsible for all course material, announcements, quizzes, and exams made in class, whether you attended that day’s class or not. I may post the announcements and materials on Canvas and/or send them out as emails. If you anticipate needing to miss more than four class sessions, please make an appointment with me to discuss any restrictions on your availability early in the semester so we can find ways for you to participate and be successful in the course. Your attendance & participation will be gauged based on the following metrics:\n\nPunctuality to class and number of classes attended\nPreparedness for class (you will generally be considered prepared if you do the CPAs)\nLevel of engagement: Includes working collaboratively with others, sharing your thoughts during class discussions, and asking/answering questions.\nContributing to online collaboration work and discussion forums.\n\nFor a more detailed rubric on how attendance and participation will be graded, click on this link.\nFor more details about the college attendance policy, see the college-wide policies section.\n\n\nClass Preparation Assignments (CPAs)\nCPAs are done prior to class. They involve completing short readings or activities and/or watching videos then answering a few questions based on the covered material. CPA’s are meant to get you prepared for the material of the week/day so that you can contribute meaningfully during class discussions. For this reason, deadlines for CPAs cannot be extended.\nNote: CPAs will generally be graded for completion and effort. From time to time, there will be one question on the CPA that will be graded for accuracy. This question is meant to gauge the extent to which you understood the material on your own and answers may be discussed in class.\n\n\nProblem sets (PSs)\nProblem sets will be assigned after every three to four weeks or after completion of a major topic. Unlike the CPAs, the problem sets will be more open-ended, non-routine, and with a heavy emphasis on real life applications of the material learned. You are encouraged to consult with your classmates on the problems but write your own solutions. You may also come in for Student hours for further help if you need it. Do not wait until the deadline day to start your problem set.\n\n\nLabs\nIn labs, you will apply what you’ve learned in during class to complete data analysis tasks. You may discuss lab assignments with other students; however, lab should be completed and submitted individually. Lab assignments must be typed up using Quarto, all work must be rendered to pdf format and must be submitted on Canvas by the deadline.\nLabs are due at 8 am ET on the indicated due date (generally the Tuesday after the lab).\nThe lowest lab grade will be dropped at the end of the semester.\n\n\nQuizzes\nThere will be a short quiz in class almost every week (Fridays). The quiz will be based mainly on material learned in the CPA and/or discussed in class for the current or previous week. To do well in these quizzes, you should complete your CPAs accurately, and attend class. If there are concepts you have questions about, be sure to ask. All Quizzes will be closed book and closed notes.\n\n\nProjects\nThere will be two group projects for this class. For every project, a short proposal will be due roughly a week after the project is assigned. Your group will have 10-15 minutes to present your work in class and then submit the project report a day after presentation. Detailed information about the project, including a grading rubric, will be provided.\n\n\nMidterm Exams\nThere will be two mid-term exams during the semester. Each midterm exam will have an in class component and a take-home component.\n\nIn-class component: The in-class component will be closed-book and closed notes but calculators will be allowed (not on devices like cellphones and computers). You will be allowed to bring a 2-sided A4 size cheatsheet paper for the in-person component. These will be submitted alongside the exam.\nThe take-home component May require use of RStudio and will be availed in quarto (qmd) format. You will need to type in your answers into the quarto document and perform the analyses accordingly. You should submit a pdf format but if you face errors rendering due to errors in your code, you may submit the .qmd file.\n\nThe exams will focus on both conceptual understanding of the content and application through analysis and computational tasks. The content of the exam will be related to the content in CPAs, problem sets, quizzes, among others.\n\n\nFinal Portfolio\nThe final portfolio will consist of the following:\n\nRevised work (if necessary) on both mid-term exams. Again, you should point out the errors on every problem and explain how the revisions address the errors.\nRevised work (if necessary) on 2 problem sets with the lowest grades. Again, you need to have reflective statements on each problem.\nA reflective essay on your main takeaways from the course and how you may implement that knowledge in your field or other real-life situations.\n\nThe final portfolio will be due on the day at time of your final exam for this course.\n\n\nGrading Policy\nYour final letter grade in the course will be weighted by category as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nAttendance and Participation\n__\n\n\nClass Preparation Assignments\n10%\n\n\nProblem Sets\n15%\n\n\nLabs\n15%\n\n\nQuizzes\n10%\n\n\nMidterm Exams\n20%\n\n\nProjects\n15%\n\n\nFinal Portfolio\n15%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n\\(\\ge\\) 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n\\(&lt;\\) 60",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "MATH 246 - Intermediate Statistics",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nThe point is very simple - you should not cheat. You should not present “someone” else’s work as your own.\nAbide by the following guidelines:\n\nCollaboration:\n\nWork that is not assigned as a collaborative assignment should not be completed collaboratively. This does not mean you should not seek support from peers. If you seek help from your peers, be sure that you write your own solutions, otherwise the work will be similar and flagged for cheating. Submitting similar work will be considered a violation of the academic integrity policy by all students involved.\nFor team assignments, you may collaborate freely within your team. Each group will submit one document contained agreed upon responses. No multiple file submissions.\n\nOn individual assignments you may not directly share work with another student in this class, and on team assignments you may not directly share work with another team in this class.\n\nOnline resources: In this century, the internet is a go to place for many things. While much of the information on the internet is useful, I expect that you will use it responsibly. The course policy is that you may use online resources (e.g., StackOverflow, Wolfram Alpha, etc.) but you must explicitly cite where you obtained any solutions you directly use (or use as inspiration).\nUse of generative artificial intelligence (AI): Generative AI tools such as ChatGPT should be treated as other online resources. There are two guiding principles that govern how you can use AI in this course:\n\nCognitive dimension: Using AI tools should make you more efficient and productive rather than hampering your ability to think clearly and critically.\nEthical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content. AI is an integral part of this course and we will practice using generative AI tools responsibly without losing sight of the learning objectives.\n✅ AI tools for code: We will use GitHub Copilot for code generation in this course. However, we will only start using this tool after you have gained a basic understanding of the coding and syntax. While coding uses highly specified formats and even minor errors can cause problems, using github copilot can be thought of as “coding in English”. You will just need to prompt the tool effectively. You should also examine the code output as much as possible. One way to do this is to look at the output (if available) and work backwards. More details on this to follow.\n\nYou may use these guidelines for citing AI-generated content.\n\n❌ AI tools for narrative: You may not use generative AI tools such as ChatGPT to write narrative on assignments. Interpretation of software output, writing up the project report, among others should be written by a human (YOU). If you want to use these tools to check your grammar, you should feel free to do so but be sure to disclose and cite it accordingly.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please contact your instructor.\n\n\nLate Work\nThe due date & time for all assignments will be posted on Canvas and/or emailed out and/or announced in class. To enable me to prepare for class meetings and give you feedback, I will not accept late work except under extreme circumstances. If you know that you won’t be able to turn in an assignment on time, reach out to me in writing (email) at least one day before the due date to discuss your options. Note that there will be no extensions on CPA’s.\n\n\nMobile devices & Other Technologies\nThis class allows use of technology devices (e.g., computers, tablets, etc.) only for purposes of the course. Such purpose involves data analysis, writing reports, completing OneNote collaborative activities, among others. However, there will be moments (e.g., brief interactive discussion, completing paper activities,) when I require that students put away (or turn off) their technology devices. Use of these devices for purposes other than the one for the course is prohibited. Research on this matter shows that it distracts you as well as other students in class. Two violations per week will result in a 0 score on the next attendance and participation grade. Persistent violation may necessitate further action to prevent you from distracting other students.\n\n\nTeams\nThis class will mostly run through small group work (teams). You will be randomly assigned to a team at the start of the semester. About midway into the semester, I will switch people teams based partly on my professional judgement. I will allow you to suggest members you would like to work with but there are no guarantees that you will get grouped with all members of your choice. I generally seek to have mixed ability teams and to ensure that everyone has a chance to work with different people.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#getting-support",
    "href": "course-syllabus.html#getting-support",
    "title": "MATH 246 - Intermediate Statistics",
    "section": "Getting Support",
    "text": "Getting Support\nThere are various resources available to help you succeed in this course. Should you feel like you are struggling too much, please don’t hesitate to reach out to me so we can discuss possible ways forward. Below are some academic support services available to you.\n\nOpen Hours\nI will be available during open hours (Mon 12.00 - 01.00 pm) to answer questions or concerns that you may have in the course. You can simply walk in during the stated time above. More times may be available but you will need to check my schedule on this link. Open hours may be held in-person or virtually depending on circumstances of the day. Below are the zoom link and passcode for virtual meetings:\nZoom Link: click here\nPasscode: 850 424\n\n\nMath Tutoring Sessions\nThe mathematics department is committed to the success of all students enrolled in mathematics courses. Free one-on-one support for your mathematics coursework is available during select daytime and evening hours Monday-Friday at the Mathematics Room (Williams Hall 209). The Mathematics Room is staffed by mathematics faculty and vetted students. Student tutors offer support to fellow students in courses numbered 200 and below while math faculty offer support in any of the math courses. For more information and the schedule, please visit the Math Support Center.\n\n\nTutoring and Academic Enrichment Services\nAs a supplement to faculty advising and office hours, Tutoring and Academic Enrichment Services offers exceptional peer resources free of charge. Learning Coaches provide content-specific peer tutoring in a variety of courses. Peer Success Coaches mentor students who wish to develop collegiate-level academic and social engagement skills. To access these courses and for more information, please visit the Center for Student Success.\n\n\nWriting Center\nThe Writing Center aims to help students from all disciplines, backgrounds, and experiences to develop greater independence as writers. We are committed to helping students see writing as central to critical and creative thinking. The physical location in Smiddy 107 will not be open to clients. For more information and scheduling appointments please visit the writing center website.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#tips-for-success",
    "href": "course-syllabus.html#tips-for-success",
    "title": "MATH 246 - Intermediate Statistics",
    "section": "Tips for Success",
    "text": "Tips for Success\nHere are a few basic suggestions for how to succeed in this course:\n\nKeep up with Homework\nIt is absolutely essential that you understand how to solve the assigned homework problems/exercises and, more importantly, how and why the skills and techniques presented in the course are used in solving the problems/exercises. I suggest that you begin working on the how as soon as possible. Do not pile your haw or work near the deadline. The advantage of getting the homework done on time is that you will get timely feedback that will help you understand the material better and hence do well in other assessment categories such as exams.\n\n\nAttend Class\nAs noted earlier, attendance is a critical part of your success in this course. You should try to attend every class because it is during class time that we will delve deeper into the course material and practice with applications.\n\n\nStay Caught Up\nMost concepts in this course build on each other cumulatively and you need to stay on top of the material at every stage. If you are having difficulty, don’t expect that the problem will take care of itself and disappear later. Contact me immediately and discuss the problem.\n\n\nCollaborate with Peers\nMany students benefit from sharing their work with others or by having their work questioned by their peers. You should attempt homework problems ahead of time by yourself and then note down any difficulties/questions that you can discuss with your peers. Even if you have no difficulties, you may still learn different and perhaps more efficient ways of solving the same problem during collaborative work. Below are some of the ways through which you can do this: - Canvas Discussion Forums & One Note Collaboration Space - You can post questions and answer others’ questions here. I encourage you to scan hand-written work (if necessary) and upload it alongside your question so people can see how you are thinking.\n\nZoom sessions – If you cannot meet in person, you can initiate Zoom sessions for collaborating. Zoom whiteboards are available to write on or you may simply have discussions. You can record these for later playback. If you want to invite me to your Zoom session, please send me an email with the link ahead of time and I will let you know if I am available to join.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#college-wide-policies",
    "href": "course-syllabus.html#college-wide-policies",
    "title": "MATH 246 - Intermediate Statistics",
    "section": "College-wide Policies",
    "text": "College-wide Policies\n\nAttendance Policy\nStudents at Ithaca College are expected to attend all classes, and they are responsible for work missed during any absence from class. At the beginning of each semester, instructors must provide the students in their courses with written guidelines regarding possible penalties for failure to attend class. These guidelines may vary from course to course but are subject to the following conditions:\n\nIn accordance with Federal Law, students with a disability documented through Student Accessibility Services (SAS) may require reasonable accommodations to ensure equitable access. A student with an attendance accommodation, who misses a scheduled course time due to a documented disability, must be provided an equivalent opportunity to make up missed time and/or coursework within a reasonable time-frame. An accommodation that affects attendance is not an attendance waiver and no accommodation can fundamentally alter a course requirement. If a faculty member thinks an attendance-related accommodation would result in a fundamental alteration, concerns and potential alternatives should be discussed with SAS.\nIn accordance with New York State law, students who miss class due to their religious beliefs shall be excused from class or examinations on that day. The faculty member is responsible for providing the student with an equivalent opportunity to make up any examination, study, or work requirement that the student may have missed. Any such work is to be completed within a reasonable time frame, as determined by the faculty member.\nAny student who misses class due to a family or individual health emergency or to a required appearance in a court of law shall be excused. If the emergency is prolonged or if the student is incapacitated, the student or a family member/legal guardian should report the absence to the Dean of Students or the Dean of the academic school where the student’s program is housed. Students may consider a leave of absence, medical leave of absence, selected course withdrawals, etc., if they miss a significant portion of classwork. (Note: Graduate students may not take a leave of absence.)\nA student may be excused to participate in local, state, or federal elections. The student is responsible to make up any work that is missed due to the absence. Any such work is to be completed within a reasonable time frame, as determined by the faculty member.\n\nA student may be excused for participation in College-authorized co-curricular and extracurricular activities if, in the instructor’s judgment, this does not impair the specific student’s or the other students’ ability to succeed in the course. For all absences except those due to religious beliefs, the course instructor has the right to determine if the number of absences has been excessive in view of the nature of the class that was missed and the stated attendance policy.\nStudents should notify their instructors as soon as possible of any anticipated absences.\n\n\nStudent Accessibility Services\nIn compliance with Section 504 of the Rehabilitation Act of 1973 and the Americans with Disabilities Act, reasonable accommodation will be provided to students with documented disabilities on a case-by-case basis. Students must register with Student Accessibility Services and provide appropriate documentation to Ithaca College before any academic adjustment will be provided. Please note that accommodations are not retroactive, so timely contact with Student Accessibility Services is encouraged. To discuss accommodations or the accommodation process, students should schedule to meet with a SAS specialist. 607-274-1005 | sas@ithaca.edu.\n\n\nMental Health statement\nThe Ithaca College Center for Counseling and Psychological Services (CAPS) promotes and fosters the academic, personal, and interpersonal development of Ithaca College students by providing short-term individual, group, and relationship counseling, crisis intervention, educational programs to the campus community, and consultation for faculty, staff, parents, and students. Their team of licensed and licensed-eligible professionals value inclusivity, and they are dedicated to creating a diverse, accessible, and welcoming environment that is safe and comfortable for all those they serve and with whom they interact. CAPS sees students in-person at their offices in the Hammond Health building (side entrance), but Telehealth meetings through Zoom can be arranged in some circumstances. Staff in the office will answer questions by phone at 607-274-3136; please leave a voicemail if you do not reach a live person. You can also reach the office via email at counseling@ithaca.edu. CAPS hours remain Monday-Friday 8:30 a.m. to 5:00 p.m. After-hours connections to a live counselor are available by calling the CAPS number and following the prompts.\nIn the event I suspect you need additional support, expect that I will express to you my concerns. It is not my intent to know the details of what might be troubling you, but simply to let you know I am concerned and that help, if needed, is available. Remember, getting help is a smart and courageous thing to do.\n\n\nAcademic Integrity\nThe College is an academic community, which values academic integrity and takes seriously its responsibility for upholding academic honesty. All members of the academic community have an obligation to uphold high intellectual and ethical standards. All forms of dishonesty including cheating and plagiarism are unacceptable. Failure to appropriately cite material used in a paper is plagiarism. The minimum penalty for cheating or plagiarism is a zero for the test or paper in question. Referral to college judiciaries is also possible. For more information on academic integrity and academic dishonesty, please refer to the Student Handbook, the College Catalog and the Code of Student Conduct and Related Policies or ask your instructor.\n\n\nTitle IX\nAt Ithaca College, we believe that every individual has the right to be treated with respect and dignity and we support the creation and maintenance of a safe and positive living and learning environment. Students who experience sexual violence (including dating violence, stalking and sexual assault), sexual harassment, or discrimination based on gender or sexual identity) are encouraged to report their experience to the Title IX Coordinator, lkoenig@ithaca.edu to explore formal and informal reporting options, and explore the support and resources available. The Title IX Coordinator will work with you to determine the best way to proceed and enhance the safety of our community. For more information go to: https://www.ithaca.edu/share.\nInformation shared in class assignments, class discussions, and at public events do not constitute an official disclosure, and faculty and staff do not have to report these to the Title IX Coordinator. Faculty and staff should be sure that access to campus and community resources related to sexual misconduct are available to students in the case these subjects do arise. Any other disclosure to faculty and staff can be reported to the Title IX Coordinator.\n\n\nAcademic Advising Center\nStudents are asked to consult with their faculty advisor, or the advising contact within their school, for all advising matters. Faculty advisors will be able to assist students with most advising questions, or they may collaborate with the dean’s office for more complicated matters. Students can find the name of their assigned faculty advisor in Homer or in Degree Works.\n\n\nDiversity and Inclusion\nIthaca College values diversity because it enriches our community and the myriad experiences that characterize an Ithaca College education. Diversity encompasses multiple dimensions, including but not limited to race, culture, nationality, ethnicity, religion, ideas, beliefs, geographic origin, class, sexual orientation, gender, gender identity and expression, disability, and age. We are dedicated to addressing current and past injustices and promoting excellence and equity. Ithaca College continually strives to build an inclusive and welcoming community of individuals with diverse talents and skills from a multitude of backgrounds who are committed to civility, mutual respect, social justice, and the free and open exchange of ideas. We commit ourselves to change, growth, and actions that embrace diversity as an integral part of the educational experience and of the community we create.Please learn more about Ithaca College’s commitment to diversity, equity and inclusion: https://www.ithaca.edu/diversity-and-inclusion/diversity-statement.\n\n\n\n\n\n\nClick here for syllabus bounty!\n\n\n\n\n\nIf you’ve read this far in the syllabus, please email me a picture of your favorite scene in Ithaca or its environs if you have one. If not, just describe the place. I will include you on my special list of most serious students.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "MATH 246 - Intermediate Statistics",
    "section": "Important dates",
    "text": "Important dates\n\nAug 26: Classes begin, Request S/D/F Option\nSep 01: Last day to Drop/add a course\nSep 02: No classes (Labor Day)\nSep 13: Last day to request S/D/F Option\nSep 27: Last Day to Withdraw with “W”, Revoke S/D/F Option\nOct 08: Project 1 Presentations\nOct 09: Exam 1 (in-class)\nOct 10: Exam 1 take-home due\nOct 13: Midterm grades available online\nOct 17-18: Fall break - No classes\nNov 01: Last Day to Withdraw with “W”\nNov 15: Exam 2 (in-class)\nNov 16: Exam 2 take-home due\nNov 23-Dec 1: Thanksgiving Break - No classes\nDec 06: Project 2 presentations\nDec 11: Last day of classes\nDec 11: Final portfolio due\n\nAssignment deadlines are listed on the course schedule and in Canvas. Class ends on April 24, there is no final exam.\nFor more important dates, see the full Ithaca College Calendar.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "If you have an iPhone, you can use the Notes app to scan a document to a PDF. To do this, follow the following steps:\n\nOpen the Notes app\nCreate a new note and name it.\nTap the camera icon and select “Scan Documents”. I suggest you use the manual capture option as this allows you to capture only once the camera is in focus.\nPosition your phone over the document and tap the shutter button. Make sure the camera is directly over the document and is in focus.\nContinue to the next page if there are multiple pages.\nTap “Save” when you are finished scanning all the pages.\n\nIf you have an Android device, you can use the Google Drive app to scan a document to a PDF. To do this, follow the following steps:\n\nOpen the Google Drive app\nTap the “+” icon in the bottom right corner\nTap “Scan”\nPosition your phone over the document and tap the shutter button. Make sure the camera is directly over the document and is in focus.\nContinue to the next page if there are multiple pages.\nTap the checkmark when you are finished scanning all the pages.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#how-do-i-use-my-mobile-device-to-scan-a-document-to-a-pdf",
    "href": "course-faq.html#how-do-i-use-my-mobile-device-to-scan-a-document-to-a-pdf",
    "title": "FAQ",
    "section": "",
    "text": "If you have an iPhone, you can use the Notes app to scan a document to a PDF. To do this, follow the following steps:\n\nOpen the Notes app\nCreate a new note and name it.\nTap the camera icon and select “Scan Documents”. I suggest you use the manual capture option as this allows you to capture only once the camera is in focus.\nPosition your phone over the document and tap the shutter button. Make sure the camera is directly over the document and is in focus.\nContinue to the next page if there are multiple pages.\nTap “Save” when you are finished scanning all the pages.\n\nIf you have an Android device, you can use the Google Drive app to scan a document to a PDF. To do this, follow the following steps:\n\nOpen the Google Drive app\nTap the “+” icon in the bottom right corner\nTap “Scan”\nPosition your phone over the document and tap the shutter button. Make sure the camera is directly over the document and is in focus.\nContinue to the next page if there are multiple pages.\nTap the checkmark when you are finished scanning all the pages.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-canvas",
    "href": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-canvas",
    "title": "FAQ",
    "section": "How do I export my assignment PDF from RStudio to upload to Canvas?",
    "text": "How do I export my assignment PDF from RStudio to upload to Canvas?\nGo to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder).",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-canvas",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-canvas",
    "title": "FAQ",
    "section": "How can I submit my assignment to Canvas?",
    "text": "How can I submit my assignment to Canvas?\nThe instructions for submitting your assignment to Canvas can be found here. Please preview your submission to make sure it is the correct file before submitting.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the course TA are always happy to provide help with any computational questions when you’re working in the posit cloud. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R: https://cran.r-project.org/\nDownload and install RStudio: https://posit.co/download/rstudio-desktop/\nInstall Quarto: https://quarto.org/docs/download/prerelease.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"xxx\"), where xxx is the name of the package.\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the online computing environment (posit cloud).",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "project/tips-resources.html",
    "href": "project/tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "The project is very open ended. For instance, in creating a compelling visualization(s) of your data in R, there is no limit on what tools or packages you may use. You do not need to visualize all of the data at once. A single high quality visualization will receive a much higher grade than a large number of poor quality visualizations.\nBefore you finalize your write up, make sure the printing of code chunks is turned off with the option echo: false. In addition to code chunks, ensure all messages are turned off with the options warning: false and message: false.\nFinally, pay attention to details in your write-up and presentation. Neatness, coherency, and clarity will count."
  },
  {
    "objectID": "project/tips-resources.html#suppress-code-and-warnings",
    "href": "project/tips-resources.html#suppress-code-and-warnings",
    "title": "Project tips + resources",
    "section": "Suppress code and warnings",
    "text": "Suppress code and warnings\n\nInclude the following in the YAML of your report.qmd to suppress all code, warnings, and other messages.\n\nexecute:\n  echo: false\n  warning: false"
  },
  {
    "objectID": "project/tips-resources.html#headers",
    "href": "project/tips-resources.html#headers",
    "title": "Project tips + resources",
    "section": "Headers",
    "text": "Headers\nUse headers to clearly label each section. Make sure there is a space between the previous line and the header. Use appropriate header levels."
  },
  {
    "objectID": "project/tips-resources.html#references",
    "href": "project/tips-resources.html#references",
    "title": "Project tips + resources",
    "section": "References",
    "text": "References\nInclude all references in a section called “References” at the end of the report. This course does not have specific requirements for formatting citations and references. Optional: Use Quarto’s citation support for generating your reference. See Citations & Footnotes on the Quarto documentation for more on that."
  },
  {
    "objectID": "project/tips-resources.html#appendix",
    "href": "project/tips-resources.html#appendix",
    "title": "Project tips + resources",
    "section": "Appendix",
    "text": "Appendix\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”. The items in the appendix should be properly labeled. The appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix. We will not grade your appendix."
  },
  {
    "objectID": "project/tips-resources.html#resize-figures",
    "href": "project/tips-resources.html#resize-figures",
    "title": "Project tips + resources",
    "section": "Resize figures",
    "text": "Resize figures\nResize plots and figures, so you have more space for the narrative. Resize individual figures: Set fig-width and fig-height in chunk options, e.g.,\n#| echo: fenced\n#| label: plot1\n#| fig-height: 3\n#| fig-width: 5\nreplacing plot1 with a meaningful label and the height and width with values appropriate for your write up.\nResize all figures: Include the fig-height and fig-width options in the YAML header as shown below:\nexecute:\n  fig-height: 3\n  fig-width: 5\nReplace the height and width values with values appropriate for your write up."
  },
  {
    "objectID": "project/tips-resources.html#arranging-plots",
    "href": "project/tips-resources.html#arranging-plots",
    "title": "Project tips + resources",
    "section": "Arranging plots",
    "text": "Arranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid.\nMore to be added soon…"
  },
  {
    "objectID": "project/first_project.html",
    "href": "project/first_project.html",
    "title": "First Project",
    "section": "",
    "text": "The goal of this project is for you to demonstrate proficiency in the material we have covered in this class (and beyond, if you like) and apply them to a ‘novel’ dataset of your choosing in a meaningful way. You are not asked to do an exhaustive data analysis by generating every statistic/visualizations that we have learned of in the course. Rather, I expect you to demonstrate skills in asking meaningful questions and answering them using carefully selected data analysis techniques and tools. These analyses must be done in R in a reproducible manner (quarto).\nMultiple data frames are available but your group will choose just one to use for this project. After choosing the data frame, your will need to brainstorm on several issues about the data such as the sample and sampling methods that might have been used to gather the data, a possible target population, variables and their types (numerical vs categorical), among others. Your team will then come up with a research topic/goal and two research questions (directly related to the topic) that you will answer by conducting the analyses. Your analyses should include data visualizations, numerical statistics, and regression models. All these must be appropriately linked to help tell a coherent story.",
    "crumbs": [
      "Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "project/first_project.html#project-proposal",
    "href": "project/first_project.html#project-proposal",
    "title": "First Project",
    "section": "Milestone 1 - Proposal",
    "text": "Milestone 1 - Proposal\nThere are two main purposes of the project proposal:\n\nTo help you think about the project early, so you can get a head start on finding data, reading relevant literature, thinking about the questions you wish to answer, etc.\nTo ensure that the data you wish to analyze, methods you plan to use, and the scope of your analysis are feasible and will allow you to be successful for this project.\n\nThe project proposal will be due roughly one week after the project is assigned. It must contain the following details:\n\nTeam members and their roles: List the names of all team members and their roles in the project. Roles can include Data cleaning/wrangling, Data analysis (performing the analyses in RStudio), Writing (e.g., abstract, intro, literature review, findings, conclusion), Proof reading and typo fixing, Team leader (e.g., setting up meetings, follow up, communicate with professor), etc.\nResearch topic and Questions: As indicated earlier, this is a short statement that describes the general area of interest that your research questions will be addressing. You should have at least two research questions that you will answer by conducting the analyses.\nData: Describe the data you will be using. This should include the source of the data, the number of observations and variables, and a brief description of the variables and their types (e.g., numerical, categorical). You should also include a brief description of the data collection process. Note that the data collection process may not be available for all data sets. In this case, you are allowed to speculate (reasonably) on the data collection process and be sure to mention that in your proposal.\nAnalysis plan: Describe the analyses you plan to conduct. This should include the types of visualizations you plan to create, the numerical summaries you plan to calculate, and the regression models you plan to fit. You should also describe how you plan to link these analyses together to tell a coherent story.\n\nInstructions and grading criteria for this milestone are outlined in the grading rubric for Milestone 1.",
    "crumbs": [
      "Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "project/first_project.html#milestone-2---first-draft",
    "href": "project/first_project.html#milestone-2---first-draft",
    "title": "First Project",
    "section": "Milestone 2 - First Draft",
    "text": "Milestone 2 - First Draft\nYour project draft is a completed project, that we shall use to obtain feedback from the professor and peers in class. The draft should include the following sections:\n\nAbstract: This section should provide a brief overview of the research topic, questions, data, and analyses you are conducting. It should also provide a brief overview of the results you have obtained. Think of the abstract as an executive summary of your work that someone can read and understand without reading the rest of the report. You should write the abstract last, after you have completed the rest of the report.\nIntroduction: This section should introduce the research topic and questions you are addressing. It should also provide a brief overview of the data you are using and the analyses you plan to conduct. Cite a few sources that are relevant to your research topic.\nLiterature Review: This section should provide a brief overview of the literature related to your research topic. You should cite at least 5 sources and synthesize their methods and findings in relation to your own research. Guidelines for writing an effective literature review can be found here.\nMethods: This section should describe the data you are using and the analyses you plan to conduct. It should include the following subsections:\n\nData Description: This section should describe the data you are using. This should include the source of the data, the number of observations and variables, and a brief description of the variables and their types (e.g., numerical, categorical). You should also include a brief description of the data collection process. Note that the data collection process may not be available for all data sets. In this case, you are allowed to speculate (reasonably) on the data collection process and be sure to mention that in your proposal.\nResearch Questions and Analyses: This section should list the research questions you are addressing. You should also describe the variables of interest in your data set that will help you answer these questions. Discuus the methids you plan to use for each question and why you think they are appropriate.\nData Cleaning: This section should describe the steps you took to clean the data. This may include removing missing values, transforming variables, etc. Depending on your data set, you may not need to do much data cleaning. If this is the case, you should still describe the steps you took to clean the data, even if they are minimal.\n\nData Analysis and Results: This is one of the most important heavily weighted parts of your project. The analyses should be sound and justified appropriately (under methods). Here, you just need to implement the analyses described under the methodology section of your paper. Your analyses should demonstrate a thorough understanding of statistical concepts learned in this course (and beyond, if you like). Your data analyses must include both numerical statistics and data visualizations. You must also use regression modelling (linear or logistic) techniques as part of your analyses. Interpretations of visualizations created as well as the models created must be made in a coherent manner and in the context of the research questions. These are the results/findings of your project.\nConclusion: This section should summarize the research questions you addressed, the data you used, the analyses you conducted, and the results you obtained. Findings of your study must be tied back to those reported in the literature review; for example, do your findings concur with previous studies or do they contradict them? It should also discuss the implications of your results and suggest directions for future research. Be sure to state any limitations of your study and what would be done differently if you were to do the study again.\nReferences: This section should list all the sources you cited in your report. Use APA format for your references.\n\nInstructions and grading criteria for this milestone are outlined in Milestone 2: Proposal.",
    "crumbs": [
      "Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "project/first_project.html#peer-review",
    "href": "project/first_project.html#peer-review",
    "title": "First Project",
    "section": "Milestone 3 - Peer review",
    "text": "Milestone 3 - Peer review\nCritically reviewing others’ work is a crucial part of the scientific process, and MATH 246 is no exception. Your team will be assigned at least one project from another team to review. Team members should read individually before meeting during which the team will come up with collective feedback for the other team. The peer review process will be double blinded, meaning that you will not know who is reviewing your project, and you will not know whose project you are reviewing. This feedback is intended to help you create a high quality final project, as well as give you experience reading and constructively critiquing the work of others.\nInstructions and grading criteria for this milestone are outlined in Milestone 3: Peer review.",
    "crumbs": [
      "Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "project/first_project.html#milestone-4---final-writeup",
    "href": "project/first_project.html#milestone-4---final-writeup",
    "title": "First Project",
    "section": "Milestone 4 - Final Writeup",
    "text": "Milestone 4 - Final Writeup\nThe final write-up is the culmination of your project. It should include all the sections of the first draft, but with revisions based on the feedback you received from your peers and the professor. The final write-up should be a polished document that clearly communicates your research topic, questions, data, analyses, and results. The final write-up should be written in a clear, concise, and professional manner. It should be free of grammatical errors and typos. The final write-up should also include a title page with the title of your project, the names of all team members, and the date. A quarto template for the final write-up will be provided.\nInstructions and grading criteria for this milestone are outlined in Milestone 4: Writeup + presentation.",
    "crumbs": [
      "Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "project/first_project.html#milestone-5---presentation",
    "href": "project/first_project.html#milestone-5---presentation",
    "title": "First Project",
    "section": "Milestone 5 - Presentation",
    "text": "Milestone 5 - Presentation\nThe final milestone is a presentation of your project. The presentation should be about 10 minutes long and should include slides that summarize the key points and findings of your project. The presentation should be clear, concise, and professional. The presentation will be graded on the clarity of the slides, the clarity of the presentation, and the ability to communicate your research topic, questions, data, analyses, and results to an audience. The presentation will be followed by a question and answer session with the professor and your peers.",
    "crumbs": [
      "Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "project/first_project.html#overall-grading",
    "href": "project/first_project.html#overall-grading",
    "title": "First Project",
    "section": "Overall grading",
    "text": "Overall grading\nThe overall grade breakdown by milestone (\\(M_i\\)) is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nM1: Project Proposal\n10 pts\n\n\nM2: First Draft\n20 pts\n\n\nM3: Peer Review\n10 pts\n\n\nM4: Final Writeup\n40 pts\n\n\nM5: Slides + Presentation\n10 pts\n\n\nCollaboration & Teamwork\n10 pts",
    "crumbs": [
      "Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "project/first_project.html#grading-milestone-details",
    "href": "project/first_project.html#grading-milestone-details",
    "title": "First Project",
    "section": "Grading Milestone details",
    "text": "Grading Milestone details\n\n\\(M_1\\) - Project Proposal\n\nCompletion (2pts) - Did the team complete all parts of the proposal as required?\nContent (2pts) - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness (2pts) - Are the statistical procedures sound and reasonable for the stated questions?\nCommunication & Writing (2pts) - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought (2pts) - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\n\n\n\\(M_2\\) - First Draft\n\nAbstract (2 pts) - Does the abstract provide a brief overview of the research topic, questions, data, and analyses you are conducting?\nIntroduction (2 pts) - Does the introduction articulate the research topic and questions you are addressing? Does it provide a brief overview of the data? Does it provide a couple of citations to help you make the case for your chosen topic of study?\nLiterature Review (3 pts) - Does the literature review provide relevant in-text citations? Does it examine and synthesize the sources in context of the current study?\nMethods (3 pts) - Does the methods section describe the sample, the data, and the analysis methods used? Does it justify the choice of analysis methods? Does it describe the data cleaning processes?\nData Analysis and Results (6 pts) - Are the analyses implemented correctly? Are the interpretations for both research questions done correctly and in context of the study? Do the analyses demonstrate a thorough understanding of statistical concepts learned in the course?\nConclusion (2 pts) - Does the conclusion summarize the research questions you addressed, the data you used, the analyses you conducted, and the results you obtained? Does it discuss the implications of your results and suggest directions for future research?\nReferences (2 pts) - Are all the sources you cited in your report listed in the references section? Are they in APA format?\n\n\n\n\\(M_3\\) - Peer Review\nPeer reviews will be graded on the extent to which it comprehensively and constructively addresses the components of the reviewee’s team’s report.\nOnly the team members participating in the review process are eligible for points for the peer review. Teams may choose to meet in-person or virtually. The team should submit a single review document that includes the following:\n\nQuality of feedback (3pts) - Did the team provide constructive and actionable feedback? Please note that actionable does not mean telling the reviewee team what to do, but rather providing feedback that the reviewee team can use to improve its work.\nCorrectness (2pts) - Are the critiques provided sound and reasonable?\nCommunication & Writing (2pts) - Is the feedback report understandable and free from grammatical errors and typos?\nCompletion (2pts) - Did the team complete the review as required?\n\n\n\n\\(M_4\\) - Final Writeup\nThe grading of the final writeup follows the same grading criteria as Milestone 2 but gets additional points. To get maximum points on milestone 4, you should address the feedback given to you on your project draft by your peers and/or professor. If there is any feedback that you think does not need to be addressed, please provide a justification for why you think so.\n\nAbstract (4 pts) - Does the abstract provide a brief overview of the research topic, questions, data, and analyses you are conducting?\nIntroduction (4 pts) - Does the introduction articulate the research topic and questions you are addressing? Does it provide a brief overview of the data? Does it provide a couple of citations to help you make the case for your chosen topic of study?\nLiterature Review (6 pts) - Does the literature review provide relevant in-text citations? Does it examine and synthesize the sources in context of the current study?\nMethods (6 pts) - Does the methods section describe the sample, the data, and the analysis methods used? Does it justify the choice of analysis methods? Does it describe the data cleaning processes?\nData Analysis and Results (14 pts) - Are the analyses implemented correctly? Are the interpretations for both research questions done correctly and in context of the study? Do the analyses demonstrate a thorough understanding of statistical concepts learned in the course?\nConclusion (4 pts) - Does the conclusion summarize the research questions you addressed, the data you used, the analyses you conducted, and the results you obtained? Does it discuss the implications of your results and suggest directions for future research?\nReferences (2 pts) - Are all the sources you cited in your report listed in the references section? Are they in APA format?\n\n\n\n\\(M_5\\) - Presentation\n\nSlides (3 pts) - Are the slides clear, well-organized, and concise? Do they summarize the key points and findings of the project?\nPresentation (4 pts) - Is the presentation clear and does it show a thorough understanding of the material learned? Are speakers audible enough and facing the audience? Are they engaging and do they maintain eye contact with the audience?\nQuestion and Answer (Q&A) (3 pts) - How well does the team answer questions about their project? Are they able to communicate their research topic, questions, data, analyses, and results to an audience?",
    "crumbs": [
      "Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "project/first_project.html#submission",
    "href": "project/first_project.html#submission",
    "title": "First Project",
    "section": "Submission",
    "text": "Submission\nSubmissions for all milestones should be made through Canvas. You will see the assignments created in Canvas for each milestone with more detailed instructions.",
    "crumbs": [
      "Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "project/final_project.html",
    "href": "project/final_project.html",
    "title": "Final Project",
    "section": "",
    "text": "Please come back later!",
    "crumbs": [
      "Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "slides/slr.html#scatterplots-association-between-variables",
    "href": "slides/slr.html#scatterplots-association-between-variables",
    "title": "Simple Linear Regression",
    "section": "Scatterplots: Association between Variables",
    "text": "Scatterplots: Association between Variables\n\n\nScatterplots are great visualization tools for showing the relationship between two numerical variables. Consider the following scatter plot for high school graduation rates and poverty rates for various cities in the US."
  },
  {
    "objectID": "slides/slr.html#examining-scatterplots",
    "href": "slides/slr.html#examining-scatterplots",
    "title": "Simple Linear Regression",
    "section": "Examining Scatterplots",
    "text": "Examining Scatterplots\n\nEach point on the scatterplot presented earlier represents a case (in this case a city).\nFor each city, we have the high school graduation rate and the percentage poverty. The circled point represents a city with a HS graduation rate of about 87% and a poverty rate of about 17%.\nDirection of association: the scatterplot shows a negative association between the two variables. As the HS grad rate increases, the poverty rate decreases.\nType of relationship: the scatterplot shows a linear relationship between the two variables. The points are roughly aligned in a straight line.\nStrength of association: the relationship between the two variables is fairly strong. Most points are tightly clustered around the line. This can be subjective!"
  },
  {
    "objectID": "slides/slr.html#a-note",
    "href": "slides/slr.html#a-note",
    "title": "Simple Linear Regression",
    "section": "A Note",
    "text": "A Note\n\n\nWhen interpreting scatterplots, consider these guidelines:\n- What type or association is it? Linear, non-linear, no association?\n- What is the strength of the association? Strong, moderate, or weak?\n- What is the direction of the association? Positive or negative?"
  },
  {
    "objectID": "slides/slr.html#predictor-vs-response-variables",
    "href": "slides/slr.html#predictor-vs-response-variables",
    "title": "Simple Linear Regression",
    "section": "Predictor vs Response Variables",
    "text": "Predictor vs Response Variables\n\nWhen we suspect one variable might causally affect another, we label the first variable the explanatory variable and the second the response variable. If education level causes poverty rate to change, then education level is the explanatory variable and poverty rate is the response variable.\nWe also use the terms explanatory and response variables to describe situations where the response variable might be predicted using the explanatory variable even if there is no causal relationship.\nIn the earlier scatterplot for HS graduation and percentage poverty, HS grad can be considered explanatory variable and poverty the response variable."
  },
  {
    "objectID": "slides/slr.html#strength-of-linear-relationships",
    "href": "slides/slr.html#strength-of-linear-relationships",
    "title": "Simple Linear Regression",
    "section": "Strength of Linear Relationships",
    "text": "Strength of Linear Relationships\n\nWe use a statistic called correlation coefficient, \\(r\\), to measure the strength of a linear relationship between two variables. This is more objective than eyeballing the scatterplot.\nThe values of \\(r\\) ranges anywhere from -1 to 1.\nWhen \\(r\\) is near 0, this indicates no relationship while values near 1 indicate strong positive linear relationship. If the relationship is negative, \\(r\\) will be negative and the strength increases as you approach -1.\nThe closer the points are to a straight line, the stronger the correlation. See the scatterplots on the next slide and their correlation values.\nIf the points make an horizontal line, the correlation is 0."
  },
  {
    "objectID": "slides/slr.html#correlation",
    "href": "slides/slr.html#correlation",
    "title": "Simple Linear Regression",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "slides/slr.html#calculating-the-correlation-coefficient",
    "href": "slides/slr.html#calculating-the-correlation-coefficient",
    "title": "Simple Linear Regression",
    "section": "Calculating the Correlation Coefficient",
    "text": "Calculating the Correlation Coefficient\n\nThe formula for the correlation coefficient is a bit complex and involves the means and standard deviations of the two variables. Below is the formula for the correlation coefficient between two variables \\(X\\) and \\(Y\\). \\[r = \\frac{1}{n-1}\\sum_{i=1}^{n}\\frac{x_i -\\bar{x}}{s_x}\\frac{y_i-\\bar{y}}{s_y}\\] Where,\n\\(x_i\\) and \\(y_i\\) are the individual data points.\n\\(\\bar{x}\\) and \\(\\bar{y}\\) are the means of the two variables.\n\\(s_x\\) and \\(s_y\\) are the standard deviations of the two variables.\n\nWe won’t use this formula but you should understand it. For example, based on the formula, what can you say about the units for \\(r\\)?"
  },
  {
    "objectID": "slides/slr.html#fitting-a-line",
    "href": "slides/slr.html#fitting-a-line",
    "title": "Simple Linear Regression",
    "section": "Fitting a Line",
    "text": "Fitting a Line\n\nLinear regression is a statistical technique used to make estimated predictions of the outcome variable values based on the predictor variable values.\nTo do that, we have to fit a line through the cloud of points. This line is called the regression line.\nBut … how do we fit the line through the cloud of points?\nConsider the four lines in the scatterplot on the next slide. Which line do you think is the best fit? Why?"
  },
  {
    "objectID": "slides/slr.html#cont",
    "href": "slides/slr.html#cont",
    "title": "Simple Linear Regression",
    "section": "Cont …",
    "text": "Cont …"
  },
  {
    "objectID": "slides/slr.html#another-note",
    "href": "slides/slr.html#another-note",
    "title": "Simple Linear Regression",
    "section": "Another Note",
    "text": "Another Note\n\n\n- In a perfect scenario, we would have all points falling on the line, but this is not possible.\n- So, we choose a line for which all the points are on the line or close enough to the line.\n- Although this way of thinking is not mathematically rigorous, it is easier to make sense of. A more rigorous way is called the least squares regression line (more on this later)."
  },
  {
    "objectID": "slides/slr.html#regression-equation-and-residuals",
    "href": "slides/slr.html#regression-equation-and-residuals",
    "title": "Simple Linear Regression",
    "section": "Regression Equation and Residuals",
    "text": "Regression Equation and Residuals\n\nThe equation of the regression line is given by \\(\\widehat{y}=b_0+b_1x\\), where \\(\\widehat{y}\\) is the estimated value of the response variable, \\(x\\) is the explanatory variable, \\(b_0\\) is the y-intercept and \\(b_1\\) is the slope of the line.\nWe can use the equation of the regression line to make estimated predictions for the response variable.\nThe difference between the observed (actual) value of the response variable and the predicted (calculated) value is called the residual (or error). The best line is one that minimizes these residuals overall.\nThe residual of the \\(i^{th}\\) observation is given by \\(e_i=y_i-\\widehat{y}_i\\) where \\(y_i\\) is the observed value of the response variable and \\(\\widehat{y}_i\\) is the predicted value of the response variable."
  },
  {
    "objectID": "slides/slr.html#example",
    "href": "slides/slr.html#example",
    "title": "Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\n\nSuppose the equation of the best fitting line (the red line) is \\(\\widehat{y} = 17.5 - 0.05x\\). The circled point is approx \\((87, 17)\\).\nUsing the model, the predicted poverty rate for a city with a HS graduation rate of 87% is \\(17.5 - 0.05(87) = 13.15\\)%.\nThe residual for this city is \\[\\text{Actual poverty}-\\text{predicted}=17 - 13.15 = 3.85\\%.\\]\nHere, the model underestimates the poverty rate for this city by 3.85%."
  },
  {
    "objectID": "slides/slr.html#residual-plots",
    "href": "slides/slr.html#residual-plots",
    "title": "Simple Linear Regression",
    "section": "Residual Plots",
    "text": "Residual Plots\n\nEach observation on a scatterplot will have a residual (positive, negative, or zero). A residual of 0 means that the predicted value and the actual value are the same. These are points on the line.\nIn a perfect association (almost never happens in the real world), the residuals for each point is 0.\nA residual plot is often used for assessing how well the regression line fits the data. Points with a perfect fit are placed on an horizontal line marked with 0 while points above are place above the line.\nNext slide scatterplot and a corresponding residual plot."
  },
  {
    "objectID": "slides/slr.html#cont-1",
    "href": "slides/slr.html#cont-1",
    "title": "Simple Linear Regression",
    "section": "Cont…",
    "text": "Cont…"
  },
  {
    "objectID": "slides/slr.html#the-least-squares-regression-line",
    "href": "slides/slr.html#the-least-squares-regression-line",
    "title": "Simple Linear Regression",
    "section": "The Least Squares Regression Line",
    "text": "The Least Squares Regression Line\n\nThe least squares regression line is the line that minimizes the sum of the squared residuals.\nThe process for fitting the line is:\n\nFit an arbitrary line through the points. Find the equation of this line.\nCalculate the residual for each point.\nSquare each residual.\nAdd up all the squared residuals. This is called sum of squared errors (SSE).\nRepeat the process until you find the line for which the SSE is minimum.\n\nAs you may have suspected, doing this process manually can time wasting and prone to errors. We can do this fast and easily using a computer program."
  },
  {
    "objectID": "slides/slr.html#things-to-note",
    "href": "slides/slr.html#things-to-note",
    "title": "Simple Linear Regression",
    "section": "Things to Note",
    "text": "Things to Note\n\n\n- The point \\((\\bar{x},\\bar{y})\\) is known as the centroid and always falls on the regression line. Here, \\(\\bar{x}\\) is the average of the X variable and \\(\\bar{y}\\) is the average of the Y variable.\n- There is a relationship between the slope of regression line, \\(b_1\\) and the correlation coefficient, \\(r\\): \\[b_1 = r\\times\\frac{s_y}{s_x}\\] where,  \\(s_x\\) and \\(s_y\\) are the standard deviations of the two variables.\n- The intercept of the line, \\(b_0\\) is given by \\[b_0 = \\bar{y} - b_1\\bar{x}\\]"
  },
  {
    "objectID": "slides/slr.html#example-1",
    "href": "slides/slr.html#example-1",
    "title": "Simple Linear Regression",
    "section": "Example",
    "text": "Example\nThe table below shows the head length and total length of 9 fish. Find the equation of the regression line. Assume that the explanatory variable. The standard deviations of the two variables are \\(s_x = 2.74\\) and \\(s_y = 2.92\\).\n\n\n\nHead length\nTotal length\n\n\n\n\n5\n10\n\n\n6\n13\n\n\n7\n11\n\n\n8\n12\n\n\n9\n15\n\n\n10\n17\n\n\n11\n15\n\n\n12\n17\n\n\n13\n18"
  },
  {
    "objectID": "slides/slr.html#solution",
    "href": "slides/slr.html#solution",
    "title": "Simple Linear Regression",
    "section": "Solution",
    "text": "Solution\n\n\nWe are given that the explanatory variable (x-axis) is head length and the response variable (y-axis) is total length. We want to find the equation of the regression line.\nFirst, we create a scatterplot to visualize the relationship.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that the relationship is linear and strong. Now, calculate the means of the two variables. \\(\\bar{x} = 9\\) and \\(\\bar{y} = 14.33\\).\nNext, calculate the correlation coefficient: \\(r = 0.92\\).\nUsing the formula \\(b_1 = r\\frac{s_y}{s_x}\\), we get \\(b_1 = 0.92\\times \\frac{2.92}{2.74} = 0.98\\).\nFinally, using the formula \\(b_0 = \\bar{y}-b_1\\bar{x}\\), we get \\(b_0 = 14.33-0.98(9)=5.51\\). Thus, the equation is \\(\\hat{y} = 5.51 + 0.98x\\)."
  },
  {
    "objectID": "slides/slr.html#interpreting-slr-model-parameters",
    "href": "slides/slr.html#interpreting-slr-model-parameters",
    "title": "Simple Linear Regression",
    "section": "Interpreting SLR model parameters",
    "text": "Interpreting SLR model parameters\nSometimes we use the term linear model to refer to a linear regression equation (e.g., \\(\\hat{y}=5.51+0.98x\\)). The slope (\\(b_1\\)) and the intercept (\\(b_0\\)) are the model parameters. But, what do they mean?\n\n\nSlope: the slope of the line is 0.98, and is usually interpreted as, for every one unit increase in head length (the explanatory variable), the total length(response variable) increases by 0.98 units.\nIntercept: the intercept of the line is 5.51. This is the value of the response variable when the explanatory variable is 0. In this case, the intercept does not make sense because we can’t have a head length of 0."
  },
  {
    "objectID": "slides/slr.html#strength-of-fit-r-squared",
    "href": "slides/slr.html#strength-of-fit-r-squared",
    "title": "Simple Linear Regression",
    "section": "Strength of Fit (R-squared)",
    "text": "Strength of Fit (R-squared)\nThe coefficient of determination (abbreviated as \\(R^2\\)) of a linear model describes the amount of variation in the response variable that is explained by the least squares regression line (the model). \\(R^2\\) is computed as follows:\n\nCompute the squared residuals (i.e., \\((y_i-\\hat{y}_i)^2\\)), and sum them up (we call these sum of square errors or SSE).\nCompute the deviation of each value of the response variable (\\(y_i\\)) from the mean (\\(\\bar{y}\\)). Square these deviations and add the squares up. This is called the sum of squares total (SST). This is a measure of the total variation in the response variable.\nCompute the difference between SST and SSE. Divide the difference by the SST to get the \\(R^2\\). Mathematically, \\[R^2 = \\frac{SST-SSE}{SST}=1 - \\frac{SSE}{SST}\\]"
  },
  {
    "objectID": "slides/slr.html#example-2",
    "href": "slides/slr.html#example-2",
    "title": "Simple Linear Regression",
    "section": "Example",
    "text": "Example\nUsing the data from the previous example, we found the equation of the linear regression model to be \\(\\hat{y} = 5.51 + 0.98x\\). Calculate the \\(R^2\\) for this model, and interpret it in context.\n\nFirst, we calculate the SST: \\(SST = \\sum(y_i-\\bar{y})^2 = (10-14.33)^2+...+ (18-14.33)^2 \\approx 68\\).\nNext, we calculate the SSE: \\(SSE = \\sum(y_i-\\hat{y}_i)^2 = (10-10.41)^2+...+ 0.2518-18.25^2 \\approx 9.98\\).  Note that \\(\\hat{y}_1=10.41=5.51+0.98\\times 5\\)\nFinally, we calculate the \\(R^2\\): \\(R^2 = 1 - \\frac{9.98}{68} = 0.8532\\).\n\nInterpretation: 85.32% of the variation in the total length of the fish (response variable) is explained by the head length (predictor variable). This is a strong fit.\nNotice that the square root of \\(R^2\\) is approximately equal to the correlation coefficient \\(r\\). For SLR, \\[R^2 \\approx r^2\\]."
  },
  {
    "objectID": "slides/slr.html#outliers-in-linear-regression",
    "href": "slides/slr.html#outliers-in-linear-regression",
    "title": "Simple Linear Regression",
    "section": "Outliers in Linear Regression",
    "text": "Outliers in Linear Regression\n\nOutliers in regression are observations that tend to fall far from the cloud of points in a scatterplot.\nOutlying can happen in the x-direction, y-direction, or both.\nOutliers that fall horizontally away from the center of the cloud of points are called leverage points.\n\nOutlying point(s) may or may not have an impact on the regression line. Whenever the points influence the slope of the line, they are called influential points (see next slide).\nIt can be very tempting to remove outliers from the data. However, this should be done with caution. Models that ignore exceptional points often perform poorly.\nOne way to deal with outliers is to perform two separate analyses - one with the outliers and one without. This will give a more holistic picture."
  },
  {
    "objectID": "slides/slr.html#cont-2",
    "href": "slides/slr.html#cont-2",
    "title": "Simple Linear Regression",
    "section": "Cont …",
    "text": "Cont …\n\n\nA:  The circled point is an outlier in the y direction and from the model. The point appears to have only a minimal influence on the line.\nB:  The outlier is to the right in the x and y direction. Not an outlier from the model. Appears to barely influence the regression line.\nC:  Appears to have a strong influence on the regression line. The line would fit the data better without this point.\n\n\n\n\n\n\n\n\nIf a point(s) is both horizontally far from the cloud and the line, then, it is an influential point."
  },
  {
    "objectID": "slides/slr.html#categorical-variables-in-slr",
    "href": "slides/slr.html#categorical-variables-in-slr",
    "title": "Simple Linear Regression",
    "section": "Categorical variables in SLR",
    "text": "Categorical variables in SLR\n\n\n\n\nSo far, we have been dealing with numerical variables. But what if we have a categorical predictor variable?\nWe can still use linear regression, but we have to convert the categorical variable into a numerical variable using dummy coding. A categorical variable with 2 levels (e.g., used or new) can be converted to numerical by assigning a 0 to one of the levels and a 1 to the other (e.g., new=1).\nThe scatterplot to the right shows the relationship between total price of a game and the game condition (dummy coded as 0 for used and 1 for new).\n\n\n\n\nGame total price vs condition"
  },
  {
    "objectID": "slides/slr.html#cont-3",
    "href": "slides/slr.html#cont-3",
    "title": "Simple Linear Regression",
    "section": "Cont …",
    "text": "Cont …\n\nThe linear model for the scatterplot can be written as \\[\\widehat{price} = b_0 + b_1\\times cond_{new}.\\]\nNotice that, in the model equation, we use the level that was assigned 1, while the level that was assigned 0 is dropped. We call the level that was dropped a reference level (more on this later).\nWhen you use software, the model turns out to be, \\[\\widehat{price} = 42.87 + 10.9 \\times cond_{new}.\\]\nInterpretation: The intercept ($42.9) is the estimated average price for used games. The slope indicates that, on average, new games sell for about $10.9 more than used games. Here, one unit increase represents change from used to new.\n\n\n\n\n\n\n\nCaution\n\n\nThe interpretation of the slope compares used and new games. A common error is to say that new games cost $10.9. This is incorrect. The slope ($10.9) is the average price difference between new and used games."
  },
  {
    "objectID": "slides/slr.html#conditionsassumptions-for-running-slr",
    "href": "slides/slr.html#conditionsassumptions-for-running-slr",
    "title": "Simple Linear Regression",
    "section": "Conditions/Assumptions for running SLR",
    "text": "Conditions/Assumptions for running SLR\nBefore running a linear regression model, certain conditions/assumptions must be met:\n\nLinearity: The relationship between the (numerical) predictor and response variable is linear. To check this, you can use a scatterplot.\nIndependence: The observations/cases are independent of each other. This is often a matter of study design.\nNormality: The residuals are normally distributed. This can be checked using a histogram of the residuals.\nEqual variance: The variance of the residuals is constant across all levels of the predictor variable. This is also called homoscedasticity. This can be checked using a residual plot.\n\n\n\nTo remember these conditions/assumptions, think of the acronym LINE."
  },
  {
    "objectID": "slides/slr.html#a-final-note",
    "href": "slides/slr.html#a-final-note",
    "title": "Simple Linear Regression",
    "section": "A Final Note",
    "text": "A Final Note\n\n\nExtrapolation can be dangerous. This is the practice of using a regression line to make predictions for values of the explanatory variable that are outside the range of the data. This is risky because the relationship between the variables may not be linear outside the range of the data."
  },
  {
    "objectID": "exam-review/exam-1-review.html",
    "href": "exam-review/exam-1-review.html",
    "title": "Exam 1 Review",
    "section": "",
    "text": "Come back later!",
    "crumbs": [
      "Exam review",
      "Exam 1"
    ]
  },
  {
    "objectID": "exam-review/exam-2-review-A.html",
    "href": "exam-review/exam-2-review-A.html",
    "title": "Exam 2 Review",
    "section": "",
    "text": "Take a random sample of size 25, with replacement, from the original sample. Calculate the proportion of students in this simulated sample who work 5 or more hours. Repeat this process 1000 times to build the bootstrap distribution. Take the middle 95% of this distribution to construct a 95% confidence interval for the true proportion of statistics majors who work 5 or more hours.\nThe exact 95% CI is (40%, 80%). Answers reasonably close to the upper and lower bounds would be accepted.\n(e) None of the above. The correct interpretation is “We are 95% confident that 40% to 80% of statistics majors work at least 5 hours per week.”\n(c) For every additional $1,000 of annual salary, the model predicts the raise to be higher, on average, by 0.0155%.\n$R^2$ of raise_2_fit is higher than $R^2$ of raise_1_fit since raise_2_fit has one more predictor and $R^2$ always\nThe reference level of performance_rating is High, since it’s the first level alphabetically. Therefore, the coefficient -2.40% is the predicted difference in raise comparing High to Successful. In this context a negative coefficient makes sense since we would expect those with High performance rating to get higher raises than those with Successful performance.\n(a) “Poor”, “Successful”, “High”, “Top”.\nOption 3. It’s a linear model with no interaction effect, so parallel lines. And since the slope for salary_typeSalaried is positive, its intercept is higher. The equations of the lines are as follows:\n\nHourly:\n\\[\n\\begin{align*}\n\\widehat{percent\\_incr} &= 1.24 + 0.0000137 \\times annual\\_salary + 0.913 salary\\_typeSalaried \\\\\n&= 1.24 + 0.0000137 \\times annual\\_salary + 0.913 \\times 0 \\\\\n&= 1.24 + 0.0000137 \\times annual\\_salary\n\\end{align*}\n\\]\nSalaried:\n\\[\n\\begin{align*}\n\\widehat{percent\\_incr} &= 1.24 + 0.0000137 \\times annual\\_salary + 0.913 salary\\_typeSalaried \\\\\n&= 1.24 + 0.0000137 \\times annual\\_salary + 0.913 \\times 1 \\\\\n&= 2.153 + 0.0000137 \\times annual\\_salary\n\\end{align*}\n\\]\n\nA parsimonious model is the simplest model with the best predictive performance.\n(c) The model predicts that the percentage increase employees with Successful performance get, on average, is higher by a factor of 1025 compared to the employees with Poor performance rating.\\/(a) and (d).\n(a) and (d).\n(c) We are 95% confident that the mean number of texts per month of all American teens is between 1450 and 1550."
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Mine Çetinkaya-Rundel (she/her) is Professor of the Practice and Director of Undergraduate Studies at the Department of Statistical Science at Duke University. Mine’s work focuses on innovation in statistics and data science pedagogy, with an emphasis on computing, reproducible research, student-centered learning, and open-source education as well as pedagogical approaches for enhancing retention of women and under-represented minorities in STEM.\n\n\n\n\n\n\n\nOffice hours\nLocation\n\n\n\n\nTuesdays and Thursdays 1-1:30 pm (immediately after class)\nBio Sci 111\n\n\nThursdays 2-3 pm\nOld Chem 213"
  },
  {
    "objectID": "course-team.html#instructor",
    "href": "course-team.html#instructor",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Mine Çetinkaya-Rundel (she/her) is Professor of the Practice and Director of Undergraduate Studies at the Department of Statistical Science at Duke University. Mine’s work focuses on innovation in statistics and data science pedagogy, with an emphasis on computing, reproducible research, student-centered learning, and open-source education as well as pedagogical approaches for enhancing retention of women and under-represented minorities in STEM.\n\n\n\n\n\n\n\nOffice hours\nLocation\n\n\n\n\nTuesdays and Thursdays 1-1:30 pm (immediately after class)\nBio Sci 111\n\n\nThursdays 2-3 pm\nOld Chem 213"
  },
  {
    "objectID": "course-team.html#teaching-assistants",
    "href": "course-team.html#teaching-assistants",
    "title": "Teaching team",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\n\n\n\n\n\n\n\nFirst name\nLast name\nRole\n\n\n\nChris\nOswald\nLab leader: M 8:30AM - 9:45AM (Sec 1)\n\n\n\nJasmine\nWang\nLab helper: M 8:30AM - 9:45AM (Sec 1)\n\n\n\nBetsy\nBersson\nHead TA\nLab leader: M 10:05AM - 11:20AM (Sec 2)\n\n\n\nMert\nBildirici\nLab helper: M 10:05AM - 11:20AM (Sec 2)\n\n\n\nWill\nTirone\nLab leader: M 11:45AM - 1:00PM (Sec 3)\n\n\n\nKrish\nBansal\nLab helper: M 11:45AM - 1:00PM (Sec 3)\n\n\n\nCaitrin\nMurphy\nLab leader: M 1:25PM - 2:40PM (Sec 4)\n\n\n\nAvery\nHodges\nLab helper: M 1:25PM - 2:40PM (Sec 4)\n\n\n\nJon\nCampbell\nLab leader: M 1:25PM - 2:40PM (Sec 11)\n\n\n\nFoxx\nHart\nLab helper: M 1:25PM - 2:40PM (Sec 11)\nLecture helper (Tue)\n\n\n\nNetra\nMittal\nLab leader: M 3:05PM - 4:20PM (Sec 5)\n\n\n\nAlexa\nFahrer\nLab helper: M 3:05PM - 4:20PM (Sec 5)\n\n\n\nDevin\nJohnson\nLab leader: M 4:40PM - 5:55PM (Sec 6)\n\n\n\nKonnie\nHuang\nLab helper: M 4:40PM - 5:55PM (Sec 6)\n\n\n\nLi\nFan\nLab leader: M 4:40PM - 5:55PM (Sec 7)\n\n\n\nKelly\nHuang\nLab helper: M 4:40PM - 5:55PM (Sec 7)\n\n\n\nLeah\nJohnson\n\n\n\n\nLisa\nZhang\nLecture helper (Tue + Thur)\n\n\n\nMiles\nKing\nLecture helper (Tue + Thur)\n\n\n\nNoah\nObuya\nLecture helper (Thur)\n\n\n\nFor office hours, please see STA 199 - Office Hours sheet."
  },
  {
    "objectID": "course-team.html#course-coordinator",
    "href": "course-team.html#course-coordinator",
    "title": "Teaching team",
    "section": "Course coordinator",
    "text": "Course coordinator\n\nDr. Mary Knox (she/her) is the course coordinator for this course. You can reach out to her (at mary.knox@duke.edu) with any questions regarding missed work, extensions, etc. as well as registration questions."
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "About the Course",
    "section": "",
    "text": "Intermediate statistics with R. This course covers statistical methods not typically covered in introductory statistics courses. We shall use your statistical background to learn about modern statistical techniques to design, implement, analyze, and communicate research. We begin by reintroducing the key ideas (e.g., data and data distributions, confidence intervals, and hypothesis testing) using modern approaches where applicable and then extend these techniques to more sophisticated statistical analysis tools like multiple regression and Analysis of Variance (ANOVA). Real world applications will be emphasized.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#course-objectives",
    "href": "course-overview.html#course-objectives",
    "title": "About the Course",
    "section": "Course Objectives",
    "text": "Course Objectives\nUpon successful completion of this course, students will be able to:\n\nImplement the model-simulate-evaluate process to address data-driven questions on real-world topics.\nChoose and use various statistical analysis techniques (exploratory and inferential) in a variety of “real life” contexts including study design.\nCritically analyze the use of statistics in research, in the media, and in various day-to-day life situations.\nCommunicate statistical ideas effectively and in context without relying on statistical jargon.\nUse technology (e.g., R) to perform statistical explorations and manipulations.\nEffectively collaborate with peers on projects and presentations.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#meeting-times-location",
    "href": "course-overview.html#meeting-times-location",
    "title": "About the Course",
    "section": "Meeting Times & Location",
    "text": "Meeting Times & Location\n\n\n\nDay\nRoom/Hall\nTime\n\n\n\n\nMonday\nWilliams Hall 202\n11:45 am - 1:00 pm\n\n\nWednesday\nWilliams Hall 202\n8:30 - 9:45 am\n\n\nFriday\nWilliams Hall 202\n10:05 - 11:20 am",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#prerequisites",
    "href": "course-overview.html#prerequisites",
    "title": "About the Course",
    "section": "Prerequisites",
    "text": "Prerequisites\nPrerequisites include math placement in group 1, or 3, or completion of MATH 14400, MATH 14500, MATH 21600 or PSYC 207 with a grade of C- (minus) or better. Attributes: QL, LA, NS (non-ICC).",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#workload-expectations",
    "href": "course-overview.html#workload-expectations",
    "title": "About the Course",
    "section": "Workload Expectations",
    "text": "Workload Expectations\nThis is a 3 credits course. Credit is earned at Ithaca College in credit hours as measured by the Carnegie unit. The Carnegie unit is defined as one hour of classroom instruction and two hours of assignments outside the classroom, for a period of 15 weeks for each unit (credit).",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "ae/ae-08-data-import.html",
    "href": "ae/ae-08-data-import.html",
    "title": "AE 08: Data import",
    "section": "",
    "text": "We will use the following two packages in this application exercise.\n\ntidyverse: For data import, wrangling, and visualization.\nreadxl: For importing data from Excel.\n\n\nlibrary(tidyverse)\nlibrary(readxl)"
  },
  {
    "objectID": "ae/ae-08-data-import.html#packages",
    "href": "ae/ae-08-data-import.html#packages",
    "title": "AE 08: Data import",
    "section": "",
    "text": "We will use the following two packages in this application exercise.\n\ntidyverse: For data import, wrangling, and visualization.\nreadxl: For importing data from Excel.\n\n\nlibrary(tidyverse)\nlibrary(readxl)"
  },
  {
    "objectID": "ae/ae-02-bechdel-dataviz-A.html",
    "href": "ae/ae-02-bechdel-dataviz-A.html",
    "title": "AE 02: Bechdel + data visualization",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key.\nIn this mini analysis we work with the data used in the FiveThirtyEight story titled “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”.\nThis analysis is about the Bechdel test, a measure of the representation of women in fiction."
  },
  {
    "objectID": "ae/ae-02-bechdel-dataviz-A.html#getting-started",
    "href": "ae/ae-02-bechdel-dataviz-A.html#getting-started",
    "title": "AE 02: Bechdel + data visualization",
    "section": "Getting started",
    "text": "Getting started\n\nPackages\nWe start with loading the packages we’ll use: tidyverse for majority of the analysis and scales for pretty plot labels later on.\n\nlibrary(tidyverse)\nlibrary(scales)\n\n\n\nData\nThe data are stored as a CSV (comma separated values) file in the data folder of your repository. Let’s read it from there and save it as an object called bechdel.\n\nbechdel &lt;- read_csv(\"https://sta199-s24.github.io/data/bechdel.csv\")\n\n\n\nGet to know the data\nWe can use the glimpse function to get an overview (or “glimpse”) of the data.\n\nglimpse(bechdel)\n\nRows: 1,615\nColumns: 7\n$ title       &lt;chr&gt; \"21 & Over\", \"Dredd 3D\", \"12 Years a Slave\", \"2 Guns\", \"42…\n$ year        &lt;dbl&gt; 2013, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013…\n$ gross_2013  &lt;dbl&gt; 67878146, 55078343, 211714070, 208105475, 190040426, 18416…\n$ budget_2013 &lt;dbl&gt; 13000000, 45658735, 20000000, 61000000, 40000000, 22500000…\n$ roi         &lt;dbl&gt; 5.221396, 1.206305, 10.585703, 3.411565, 4.751011, 0.81851…\n$ binary      &lt;chr&gt; \"FAIL\", \"PASS\", \"FAIL\", \"FAIL\", \"FAIL\", \"FAIL\", \"FAIL\", \"P…\n$ clean_test  &lt;chr&gt; \"notalk\", \"ok\", \"notalk\", \"notalk\", \"men\", \"men\", \"notalk\"…\n\n\n\nWhat does each observation (row) in the data set represent?\n\nEach observation represents a movie.\n\nHow many observations (rows) are in the data set?\n\nThere are 1615 movies in the dataset.\n\nHow many variables (columns) are in the data set?\n\nThere are 7 columns in the dataset.\n\n\nVariables of interest\nThe variables we’ll focus on are the following:\n\nbudget_2013: Budget in 2013 inflation adjusted dollars.\ngross_2013: Gross (US and international combined) in 2013 inflation adjusted dollars.\nroi: Return on investment, calculated as the ratio of the gross to budget.\nclean_test: Bechdel test result:\n\nok = passes test\ndubious\nmen = women only talk about men\nnotalk = women don’t talk to each other\nnowomen = fewer than two women\n\nbinary: Bechdel Test PASS vs FAIL binary\n\nWe will also use the year of release in data prep and title of movie to take a deeper look at some outliers.\nThere are a few other variables in the dataset, but we won’t be using them in this analysis."
  },
  {
    "objectID": "ae/ae-02-bechdel-dataviz-A.html#visualizing-data-with-ggplot2",
    "href": "ae/ae-02-bechdel-dataviz-A.html#visualizing-data-with-ggplot2",
    "title": "AE 02: Bechdel + data visualization",
    "section": "Visualizing data with ggplot2",
    "text": "Visualizing data with ggplot2\nggplot2 is the package and ggplot() is the function in this package that is used to create a plot.\n\nggplot() creates the initial base coordinate system, and we will add layers to that base. We first specify the data set we will use with data = bechdel.\n\n\nggplot(data = bechdel)\n\n\n\n\n\n\n\n\n\nThe mapping argument is paired with an aesthetic (aes()), which tells us how the variables in our data set should be mapped to the visual properties of the graph.\n\n\nggplot(data = bechdel, \n       mapping = aes(x = budget_2013, y = gross_2013))\n\n\n\n\n\n\n\n\nAs we previously mentioned, we often omit the names of the first two arguments in R functions. So you’ll often see this written as:\n\nggplot(bechdel, \n       aes(x = budget_2013, y = gross_2013))\n\n\n\n\n\n\n\n\nNote that the result is exactly the same.\n\nThe geom_xx function specifies the type of plot we want to use to represent the data. In the code below, we use geom_point which creates a plot where each observation is represented by a point.\n\n\nggplot(bechdel, \n       aes(x = budget_2013, y = gross_2013)) +\n  geom_point()\n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nNote that this results in a warning as well. What does the warning mean?"
  },
  {
    "objectID": "ae/ae-02-bechdel-dataviz-A.html#gross-revenue-vs.-budget",
    "href": "ae/ae-02-bechdel-dataviz-A.html#gross-revenue-vs.-budget",
    "title": "AE 02: Bechdel + data visualization",
    "section": "Gross revenue vs. budget",
    "text": "Gross revenue vs. budget\n\nStep 1 - Your turn\nModify the following plot to change the color of all points to a different color.\n\n\n\n\n\n\nTip\n\n\n\nSee http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf for many color options you can use by name in R or use the hex code for a color of your choice.\n\n\n\nggplot(bechdel, \n       aes(x = budget_2013, y = gross_2013)) +\n  geom_point(color = \"coral\") \n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 2 - Your turn\nAdd labels for the title and x and y axes.\n\nggplot(bechdel, \n       aes(x = budget_2013, y = gross_2013))+\n  geom_point(color = \"deepskyblue3\") + \n  labs(\n    x = \"Budget (in 2013 $)\", \n    y = \"Gross revenue (in 2013 $)\", \n    title = \"Gross revenue vs. budget\"\n    )\n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 3 - Your turn\nAn aesthetic is a visual property of one of the objects in your plot. Commonly used aesthetic options are:\n\ncolor\nfill\nshape\nsize\nalpha (transparency)\n\nModify the plot below, so the color of the points is based on the variable binary.\n\nggplot(bechdel, \n       aes(x = budget_2013, y = gross_2013, color = binary)) +\n  geom_point() + \n  labs(\n    x = \"Budget (in 2013 $)\", \n    y = \"Gross revenue (in 2013 $)\", \n    title = \"Gross revenue vs. budget, by Bechdel test result\"\n    )\n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 4 - Your turn\nExpand on your plot from the previous step to make the size of your points based on roi.\n\nggplot(bechdel, \n       aes(x = budget_2013, y = gross_2013,\n           color = binary, size = roi)) +\n  geom_point() + \n  labs(\n    x = \"Budget (in 2013 $)\", \n    y = \"Gross revenue (in 2013 $)\", \n    title = \"Gross revenue vs. budget, by Bechdel test result\"\n    )\n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 5 - Your turn\nExpand on your plot from the previous step to make the transparency (alpha) of the points 0.5.\n\nggplot(bechdel, \n       aes(x = budget_2013, y = gross_2013,\n           color = binary, size = roi)) +\n  geom_point(alpha = 0.5) + \n  labs(\n    x = \"Budget (in 2013 $)\", \n    y = \"Gross revenue (in 2013 $)\", \n    title = \"Gross revenue vs. budget, by Bechdel test result\"\n    )\n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 6 - Your turn\nExpand on your plot from the previous step by using facet_wrap to display the association between budget and gross for different values of clean_test.\n\nggplot(bechdel, \n       aes(x = budget_2013, y = gross_2013,\n           color = binary, size = roi)) +\n  geom_point(alpha = 0.5) + \n  facet_wrap(~clean_test) +\n  labs(\n    x = \"Budget (in 2013 $)\", \n    y = \"Gross revenue (in 2013 $)\", \n    title = \"Gross revenue vs. budget, by Bechdel test result\"\n    )\n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 7 - Demo\nImprove your plot from the previous step by making the x and y scales more legible.\n\n\n\n\n\n\nTip\n\n\n\nMake use of the scales package, specifically the scale_x_continuous() and scale_y_continuous() functions.\n\n\n\nggplot(bechdel, \n       aes(x = budget_2013, y = gross_2013,\n           color = binary, size = roi)) +\n  geom_point(alpha = 0.5) + \n  facet_wrap(~clean_test) +\n  scale_x_continuous(labels = label_dollar(scale = 1/1000000)) +\n  scale_y_continuous(labels = label_dollar(scale = 1/1000000)) +\n  labs(\n    x = \"Budget (in 2013 $)\", \n    y = \"Gross revenue (in 2013 $)\", \n    title = \"Gross revenue vs. budget, by Bechdel test result\"\n    )\n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 8 - Your turn\nExpand on your plot from the previous step by using facet_grid to display the association between budget and gross for different combinations of clean_test and binary. Comment on whether this was a useful update.\n\nggplot(bechdel, \n       aes(x = budget_2013, y = gross_2013,\n           color = binary, size = roi)) +\n  geom_point(alpha = 0.5) + \n  facet_grid(binary~clean_test) +\n  scale_x_continuous(labels = label_dollar(scale = 1/1000000)) +\n  scale_y_continuous(labels = label_dollar(scale = 1/1000000)) +\n  labs(\n    x = \"Budget (in 2013 $)\", \n    y = \"Gross revenue (in 2013 $)\", \n    title = \"Gross revenue vs. budget, by Bechdel test result\"\n    )\n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nThis was not a useful update as one of the levels of clean_test maps directly to one of the levels of binary.\n\n\nStep 9 - Demo\nWhat other improvements could we make to this plot?\n\n# Answers may vary\n\n\n\nRender, commit, and push\n\nIf you made any changes since the last render, render again to get the final version of the AE.\nCheck the box next to each document in the Git tab (this is called “staging” the changes). Commit the changes you made using a simple and informative message.\nUse the green arrow to push your changes to your repo on GitHub.\nCheck your repo on GitHub and see the updated files. Once your updated files are in your repo on GitHub, you’re good to go!"
  },
  {
    "objectID": "ae/ae-02-bechdel-dataviz-A.html#return-on-investment",
    "href": "ae/ae-02-bechdel-dataviz-A.html#return-on-investment",
    "title": "AE 02: Bechdel + data visualization",
    "section": "Return-on-investment",
    "text": "Return-on-investment\nFinally, let’s take a look at return-on-investment (ROI).\n\nStep 1 - Your turn\nCreate side-by-side box plots of roi by clean_test where the boxes are colored by binary.\n\nggplot(bechdel, \n       aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    x = \"Detailed Bechdel result\",\n    y = \"Return-on-investment (gross / budget)\",\n    color = \"Bechdel\\nresult\"\n    )\n\nWarning: Removed 15 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nWhat are those movies with very high returns on investment?\n\nbechdel |&gt;\n  filter(roi &gt; 400) |&gt;\n  select(title, roi, budget_2013, gross_2013, year, clean_test)\n\n# A tibble: 3 × 6\n  title                     roi budget_2013 gross_2013  year clean_test\n  &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n1 Paranormal Activity      671.      505595  339424558  2007 dubious   \n2 The Blair Witch Project  648.      839077  543776715  1999 ok        \n3 El Mariachi              583.       11622    6778946  1992 nowomen   \n\n\n\n\nStep 2 - Demo\nExpand on your plot from the previous step to zoom in on movies with roi &lt; ___ to get a better view of how the medians across the categories compare.\n\nggplot(bechdel, \n       aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    x = \"Detailed Bechdel result\",\n    y = \"Return-on-investment (gross / budget)\",\n    color = \"Bechdel\\nresult\"\n    ) +\n  coord_cartesian(ylim = c(0, 18))\n\nWarning: Removed 15 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nWhat does this plot say about return-on-investment on movies that pass the Bechdel test?"
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap.html",
    "href": "ae/ae-15-duke-forest-bootstrap.html",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "",
    "text": "In this application exercise, we will\nThe dataset are on housing prices in Duke Forest – a dataset you’ve seen before! It’s called duke_forest and it’s in the openintro package. Additionally, we’ll use tidyverse and tidymodels packages.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)"
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap.html#exercise-1",
    "href": "ae/ae-15-duke-forest-bootstrap.html#exercise-1",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "Exercise 1",
    "text": "Exercise 1\nVisualize the distribution of sizes of houses in Duke Forest. What is the size of a typical house?\n\n# add code here"
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap.html#exercise-2",
    "href": "ae/ae-15-duke-forest-bootstrap.html#exercise-2",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "Exercise 2",
    "text": "Exercise 2\nConstruct a 95% confidence interval for the typical size of a house in Duke Forest. Interpret the interval in context of the data.\n\n# add code here\n\nAdd interpretation here."
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap.html#exercise-3",
    "href": "ae/ae-15-duke-forest-bootstrap.html#exercise-3",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "Exercise 3",
    "text": "Exercise 3\nWithout calculating it – would a 90% confidence interval be wider or narrower? Why?\nAdd response here."
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap.html#exercise-4",
    "href": "ae/ae-15-duke-forest-bootstrap.html#exercise-4",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "Exercise 4",
    "text": "Exercise 4\nConstruct the 90% confidence interval and interpret it.\n\n# add code here\n\nAdd interpretation here."
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap.html#exercise-5",
    "href": "ae/ae-15-duke-forest-bootstrap.html#exercise-5",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "Exercise 5",
    "text": "Exercise 5\nQuantify the uncertainty around this slope using a 95% bootstrap confidence interval and interpret the interval in context of the data.\n\n# add code here\n\nAdd interpretation here."
  },
  {
    "objectID": "ae/ae-04-flights-wrangling.html",
    "href": "ae/ae-04-flights-wrangling.html",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(nycflights13)"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling.html#exercise-1",
    "href": "ae/ae-04-flights-wrangling.html#exercise-1",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 1",
    "text": "Exercise 1\nYour turn: Fill in the blanks:\nThe flights data frame has ___ rows. Each row represents a ___."
  },
  {
    "objectID": "ae/ae-04-flights-wrangling.html#exercise-2",
    "href": "ae/ae-04-flights-wrangling.html#exercise-2",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 2",
    "text": "Exercise 2\nYour turn: What are the names of the variables in flights.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling.html#exercise-3---select",
    "href": "ae/ae-04-flights-wrangling.html#exercise-3---select",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 3 - select()",
    "text": "Exercise 3 - select()\n\nDemo: Make a data frame that only contains the variables dep_delay and arr_delay.\n\n\n# add code here\n\n\nDemo: Make a data frame that keeps every variable except dep_delay.\n\n\n# add code here\n\n\nDemo: Make a data frame that includes all variables between year through dep_delay (inclusive). These are all variables that provide information about the departure of each flight.\n\n\n# add code here\n\n\nDemo: Use the select helper contains() to make a data frame that includes the variables associated with the arrival, i.e., contains the string \"arr\\_\" in the name.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling.html#exercise-4---slice",
    "href": "ae/ae-04-flights-wrangling.html#exercise-4---slice",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 4 - slice()",
    "text": "Exercise 4 - slice()\n\nDemo: Display the first five rows of the flights data frame.\n\n\n# add code here\n\n\nDemo: Display the last two rows of the flights data frame.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling.html#exercise-5---arrange",
    "href": "ae/ae-04-flights-wrangling.html#exercise-5---arrange",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 5 - arrange()",
    "text": "Exercise 5 - arrange()\n\nDemo: Let’s arrange the data by departure delay, so the flights with the shortest departure delays will be at the top of the data frame.\n\n\n# add code here\n\n\nQuestion: What does it mean for the dep_delay to have a negative value?\n\nAdd your response here.\n\nDemo: Arrange the data by descending departure delay, so the flights with the longest departure delays will be at the top.\n\n\n# add code here\n\n\nYour turn: Create a data frame that only includes the plane tail number (tailnum), carrier (carrier), and departure delay for the flight with the longest departure delay. What is the plane tail number (tailnum) for this flight?\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling.html#exercise-6---filter",
    "href": "ae/ae-04-flights-wrangling.html#exercise-6---filter",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 6 - filter()",
    "text": "Exercise 6 - filter()\n\nDemo: Filter for all rows where the destination airport is RDU.\n\n\n# add code here\n\n\nDemo: Filter for all rows where the destination airport is RDU and the arrival delay is less than 0.\n\n\n# add code here\n\n\nYour turn: Describe what the code is doing in words.\n\nAdd response here.\n\nflights |&gt;\n  filter(\n    dest %in% c(\"RDU\", \"GSO\"),\n    arr_delay &lt; 0 | dep_delay &lt; 0\n  )\n\n# A tibble: 6,203 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      800            810       -10      949            955\n 2  2013     1     1      832            840        -8     1006           1030\n 3  2013     1     1      851            851         0     1032           1036\n 4  2013     1     1      917            920        -3     1052           1108\n 5  2013     1     1     1024           1030        -6     1204           1215\n 6  2013     1     1     1127           1129        -2     1303           1309\n 7  2013     1     1     1157           1205        -8     1342           1345\n 8  2013     1     1     1317           1325        -8     1454           1505\n 9  2013     1     1     1449           1450        -1     1651           1640\n10  2013     1     1     1505           1510        -5     1654           1655\n# ℹ 6,193 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nHint: Logical operators in R:\n\n\n\n\n\n\n\noperator\ndefinition\n\n\n\n\n&lt;\nis less than?\n\n\n&lt;=\nis less than or equal to?\n\n\n&gt;\nis greater than?\n\n\n&gt;=\nis greater than or equal to?\n\n\n==\nis exactly equal to?\n\n\n!=\nis not equal to?\n\n\nx & y\nis x AND y?\n\n\nx \\| y\nis x OR y?\n\n\nis.na(x)\nis x NA?\n\n\n!is.na(x)\nis x not NA?\n\n\nx %in% y\nis x in y?\n\n\n!(x %in% y)\nis x not in y?\n\n\n!x\nis not x? (only makes sense if x is TRUE or FALSE)"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling.html#exercise-7---count",
    "href": "ae/ae-04-flights-wrangling.html#exercise-7---count",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 7 - count()",
    "text": "Exercise 7 - count()\n\nDemo: Create a frequency table of the destination locations for flights from New York.\n\n\n# add code here\n\n\nDemo: In which month was there the fewest number of flights? How many flights were there in that month?\n\n\n# add code here\n\n\nYour turn: On which date (month + day) was there the largest number of flights? How many flights were there on that day?\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling.html#exercise-8---mutate",
    "href": "ae/ae-04-flights-wrangling.html#exercise-8---mutate",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 8 - mutate()",
    "text": "Exercise 8 - mutate()\n\nDemo: Convert air_time (minutes in the air) to hours and then create a new variable, mph, the miles per hour of the flight.\n\n\n# add code here\n\n\nYour turn: First, count the number of flights each month, and then calculate the proportion of flights in each month. What proportion of flights take place in July?\n\n\n# add code here\n\n\nDemo: Create a new variable, rdu_bound, which indicates whether the flight is to RDU or not. Then, for each departure airport (origin), calculate what proportion of flights originating from that airport are to RDU.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling.html#exercise-9---summarize",
    "href": "ae/ae-04-flights-wrangling.html#exercise-9---summarize",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 9 - summarize()",
    "text": "Exercise 9 - summarize()\n\nDemo: Find mean arrival delay for all flights.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling.html#exercise-10---group_by",
    "href": "ae/ae-04-flights-wrangling.html#exercise-10---group_by",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 10 - group_by()",
    "text": "Exercise 10 - group_by()\n\nDemo: Find mean arrival delay for for each month.\n\n\n# add code here\n\n\nYour turn: What is the median departure delay for each airports around NYC (origin)? Which airport has the shortest median departure delay?\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-03-duke-forest.html",
    "href": "ae/ae-03-duke-forest.html",
    "title": "AE 03: Duke Forest + data visualization",
    "section": "",
    "text": "library(tidyverse)\nlibrary(openintro)"
  },
  {
    "objectID": "ae/ae-03-duke-forest.html#exercise-1",
    "href": "ae/ae-03-duke-forest.html#exercise-1",
    "title": "AE 03: Duke Forest + data visualization",
    "section": "Exercise 1",
    "text": "Exercise 1\nSuppose you’re helping some family friends who are looking to buy a house in Duke Forest. As they browse Zillow listings, they realize some houses have garages and others don’t, and they wonder: Does having a garage make a difference?\nLuckily, you can help them answer this question with data visualization!\n\nMake histograms of the prices of houses in Duke Forest based on whether they have a garage.\n\nIn order to do this, you will first need to create a new variable called garage (with levels \"Garage\" and \"No garage\").\nBelow is the code for creating this new variable. Here, we mutate() the duke_forest data frame to add a new variable called garage which takes the value \"Garage\" if the text string \"Garage\" is detected in the parking variable and takes the test string \"No garage\" if not.\n\n\n\nduke_forest |&gt;\n  mutate(garage = if_else(str_detect(parking, \"Garage\"),   \"Garage\", \"No garage\"))\n\n\nThen, facet by garage and use different colors for the two facets.\nChoose an appropriate binwidth and decide whether a legend is needed, and turn it off if not.\nInclude informative title and axis labels.\nFinally, include a brief (2-3 sentence) narrative comparing the distributions of prices of Duke Forest houses that do and don’t have garages. Your narrative should touch on whether having a garage “makes a difference” in terms of the price of the house.\n\n\n# add code here\n\nAdd narrative here…\n\n\n\n\n\n\nImportant\n\n\n\nNow is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "ae/ae-03-duke-forest.html#exercise-2",
    "href": "ae/ae-03-duke-forest.html#exercise-2",
    "title": "AE 03: Duke Forest + data visualization",
    "section": "Exercise 2",
    "text": "Exercise 2\nIt’s expected that within any given marker larger houses will be priced higher. It’s also expected that the age of the house will have an effect on the price. However in some markets new houses might be more expensive while in others new construction might mean “no character” and hence be less expensive. So your family friends ask: “In Duke Forest, do houses that are bigger and more expensive tend to be newer ones than those that are smaller and cheaper?”\nOnce again, data visualization skills to the rescue!\n\nCreate a scatter plot to exploring the relationship between price and area, conditioning for year_built.\nUse geom_smooth() with the argument se = FALSE to add a smooth curve fit to the data and color the points by year_built.\nInclude informative title, axis, and legend labels.\nDiscuss each of the following claims (1-2 sentences per claim). Your discussion should touch on specific things you observe in your plot as evidence for or against the claims.\n\nClaim 1: Larger houses are priced higher.\nClaim 2: Newer houses are priced higher.\nClaim 3: Bigger and more expensive houses tend to be newer ones than smaller and cheaper ones.\n\n\n\n# add code here\n\nAdd narrative here…\n\n\n\n\n\n\nImportant\n\n\n\nNow is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "ae/ae-12-modeling-penguins-multi-A.html",
    "href": "ae/ae-12-modeling-penguins-multi-A.html",
    "title": "AE 12: Modeling penguins with multiple predictors",
    "section": "",
    "text": "In this application exercise we will be studying penguins. The data can be found in the palmerpenguins package and we will use tidyverse and tidymodels for data exploration and modeling, respectively.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nPlease read the following context and take a glimpse at the data set before we get started.\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\nOur goal is to understand better how various body measurements and attributes of penguins relate to their body mass."
  },
  {
    "objectID": "ae/ae-12-modeling-penguins-multi-A.html#additive-vs.-interaction-models",
    "href": "ae/ae-12-modeling-penguins-multi-A.html#additive-vs.-interaction-models",
    "title": "AE 12: Modeling penguins with multiple predictors",
    "section": "Additive vs. interaction models",
    "text": "Additive vs. interaction models\n\nYour turn: Run the two chunks of code below and create two separate plots. How are the two plots different than each other? Which plot does the model we fit above represent?\n\n# Plot A\nggplot(\n  penguins, \n  aes(x = flipper_length_mm, y = body_mass_g, color = island)\n  ) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Plot A - Interaction model\") +\n  theme(legend.position = \"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n# Plot B\nbm_fl_island_aug &lt;- augment(bm_fl_island_fit, new_data = penguins)\nggplot(\n  bm_fl_island_aug, \n  aes(x = flipper_length_mm, y = body_mass_g, color = island)\n  ) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(aes(y = .pred), method = \"lm\") +\n  labs(title = \"Plot B - Additive model\") +\n  theme(legend.position = \"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\nRemoved 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nPlot B represent the model we fit.\n\nYour turn: Interpret the slope coefficient for flipper length in the context of the data and the research question.\n\nFor every 1 millimeter the flipper is longer, we expect body mass to be higher, on average, by 44.5 grams, holding all else (the island) constant. In other words, this is true for penguins in a given island, regardless of the island.\n\nDemo: Predict the body mass of a Dream island penguin with a flipper length of 200 mm.\n\n\npenguin_200_Dream &lt;- tibble(\n  flipper_length_mm = 200,\n  island = \"Dream\"\n)\n\npredict(bm_fl_island_fit, new_data = penguin_200_Dream)\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1 4021.\n\n\n\nReview: Look back at Plot B. What assumption does the additive model make about the slopes between flipper length and body mass for each of the three islands?\n\nThe additive model assumes the same slope between body mass and flipper length for all three islands.\n\nDemo: Now fit the interaction model represented in Plot A and write the estimated regression model.\n\n\nbm_fl_island_int_fit &lt;- linear_reg() |&gt;\n  fit(body_mass_g ~ flipper_length_mm * island, data = penguins)\n\ntidy(bm_fl_island_int_fit)\n\n# A tibble: 6 × 5\n  term                              estimate std.error statistic  p.value\n  &lt;chr&gt;                                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                        -5464.     431.      -12.7  2.51e-30\n2 flipper_length_mm                     48.5      2.05     23.7  1.66e-73\n3 islandDream                         3551.     969.        3.66 2.89e- 4\n4 islandTorgersen                     3218.    1680.        1.92 5.62e- 2\n5 flipper_length_mm:islandDream        -19.4      4.94     -3.93 1.03e- 4\n6 flipper_length_mm:islandTorgersen    -17.4      8.73     -1.99 4.69e- 2\n\n\n\\[\n\\widehat{body~mass} = -5464 \\\\\n+ 48.5 \\times flipper~length \\\\\n+ 3551 \\times Dream + 3218 \\times Torgersen \\\\\n- 19.4 \\times flipper~length*Dream - 17.4 \\times flipper~length*Torgersen\n\\]\n\nReview: What does modeling body mass with an interaction effect get us that without doing so does not?\n\nThe interaction effect allows us to model the rate of change in estimated body mass as flipper length increases as different in the three islands.\n\nYour turn: Predict the body mass of a Dream island penguin with a flipper length of 200 mm.\n\n\npredict(bm_fl_island_int_fit, new_data = penguin_200_Dream)\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1 3915."
  },
  {
    "objectID": "ae/ae-12-modeling-penguins-multi-A.html#choosing-a-model",
    "href": "ae/ae-12-modeling-penguins-multi-A.html#choosing-a-model",
    "title": "AE 12: Modeling penguins with multiple predictors",
    "section": "Choosing a model",
    "text": "Choosing a model\nRule of thumb: Occam’s Razor - Don’t overcomplicate the situation! We prefer the simplest best model.\n\nglance(bm_fl_island_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.774         0.772  383.      386. 7.60e-109     3 -2517. 5045. 5064.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(bm_fl_island_int_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.786         0.783  374.      246. 4.55e-110     5 -2508. 5031. 5057.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nReview: What is R-squared? What is adjusted R-squared?\n\nR-squared is the percent variability in the response that is explained by our model. (Can use when models have same number of variables for model selection)\nAdjusted R-squared is similar, but has a penalty for the number of variables in the model. (Should use for model selection when models have different numbers of variables)."
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html",
    "href": "ae/ae-17-effective-dataviz-A.html",
    "title": "Trends instructional staff employees in universities",
    "section": "",
    "text": "The American Association of University Professors (AAUP) is a nonprofit membership association of faculty and other academic professionals. This report by the AAUP shows trends in instructional staff employees between 1975 and 2011, and contains the following image. What trends are apparent in this visualization?"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#packages",
    "href": "ae/ae-17-effective-dataviz-A.html#packages",
    "title": "Trends instructional staff employees in universities",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggthemes)"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#data",
    "href": "ae/ae-17-effective-dataviz-A.html#data",
    "title": "Trends instructional staff employees in universities",
    "section": "Data",
    "text": "Data\nEach row in this dataset represents a faculty type, and the columns are the years for which we have data. The values are percentage of hires of that type of faculty for each year.\n\nstaff &lt;- read_csv(\"https://sta199-s24.github.io/data/instructional-staff.csv\")\nstaff\n\n# A tibble: 5 × 12\n  faculty_type    `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Full-Time Tenu…   29     27.6   25     24.8   21.8   20.3   19.3   17.8   17.2\n2 Full-Time Tenu…   16.1   11.4   10.2    9.6    8.9    9.2    8.8    8.2    8  \n3 Full-Time Non-…   10.3   14.1   13.6   13.6   15.2   15.5   15     14.8   14.9\n4 Part-Time Facu…   24     30.4   33.1   33.2   35.5   36     37     39.3   40.5\n5 Graduate Stude…   20.5   16.5   18.1   18.8   18.7   19     20     19.9   19.5\n# ℹ 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt;"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#recreate",
    "href": "ae/ae-17-effective-dataviz-A.html#recreate",
    "title": "Trends instructional staff employees in universities",
    "section": "Recreate",
    "text": "Recreate\n\nYour turn (10 minutes): Recreate the visualization above. Try to match as many of the elements as possible. Hint: You might need to reshape your data first.\n\n\nstaff_long &lt;- staff |&gt;\n  pivot_longer(\n    cols = -faculty_type, names_to = \"year\",\n    values_to = \"percentage\"\n  ) |&gt;\n  mutate(\n    percentage = as.numeric(percentage),\n    faculty_type = fct_relevel(\n      faculty_type,\n      \"Full-Time Tenured Faculty\",\n      \"Full-Time Tenure-Track Faculty\",\n      \"Full-Time Non-Tenure-Track Faculty\",\n      \"Part-Time Faculty\",\n      \"Graduate Student Employees\"\n    )\n  )\n\n\nggplot(\n  staff_long,\n  aes(\n    x = str_wrap(faculty_type, 20),\n    y = percentage,\n    fill = year\n    )\n  ) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(breaks = seq(5, 45, 5), limits = c(0, 45)) +\n  labs(\n    x = NULL,\n    y = \"Percent of Total Instructional Staff\",\n    fill = NULL,\n    title = \"Trends in Instructional Staff Employment Status, 1975-2011\",\n    subtitle = \"All Institutions, National Totals\",\n    caption = \"Source: US Department of Education, IPEDS Fall Staff Survey\"\n  ) +\n  theme(\n    legend.position = c(0.4, 0.93),\n    legend.direction = \"horizontal\",\n    legend.key.size = unit(0.2, \"cm\"),\n    legend.key.height = unit(0.1, \"cm\"),\n    legend.text.align = 0,\n    legend.background = element_rect(color = \"black\", linewidth = 0.2),\n    legend.text = element_text(size = 7),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    plot.caption = element_text(size = 8, hjust = 0)\n  ) +\n  guides(fill = guide_legend(nrow = 1))"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#represent-percentages-as-parts-of-a-whole",
    "href": "ae/ae-17-effective-dataviz-A.html#represent-percentages-as-parts-of-a-whole",
    "title": "Trends instructional staff employees in universities",
    "section": "Represent percentages as parts of a whole",
    "text": "Represent percentages as parts of a whole\n\nDemo: Recreate the previous visualization where the percentages are represented as parts of a whole.\n\n\nggplot(\n  staff_long,\n  aes(\n    x = str_wrap(faculty_type, 20),\n    y = percentage,\n    fill = fct_rev(year)\n    )\n  ) +\n  geom_col(position = \"fill\", color = \"white\", linewidth = 0.2) +\n  scale_y_continuous(labels = label_percent()) +\n  labs(\n    x = NULL,\n    y = \"Percent of Total Instructional Staff\",\n    fill = NULL,\n    title = \"Trends in Instructional Staff Employment Status, 1975-2011\",\n    subtitle = \"All Institutions, National Totals\",\n    caption = \"Source: US Department of Education, IPEDS Fall Staff Survey\"\n  ) +\n  theme(\n    legend.text.align = 0,\n    legend.background = element_rect(color = \"black\", size = 0.2),\n    legend.text = element_text(size = 7),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    plot.caption = element_text(size = 8, hjust = 0)\n  )\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead."
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#place-time-on-x-axis",
    "href": "ae/ae-17-effective-dataviz-A.html#place-time-on-x-axis",
    "title": "Trends instructional staff employees in universities",
    "section": "Place time on x-axis",
    "text": "Place time on x-axis\n\nDemo: Convert the visualization to a line plot with time on the x-axis.\n\n\nggplot(\n  staff_long,\n  aes(\n    x = year,\n    y = percentage,\n    color = str_wrap(faculty_type, 20),\n    group = str_wrap(faculty_type, 20)\n    )\n  ) +\n  geom_line(linewidth = 1) +\n  labs(\n    x = NULL,\n    y = \"Percent of Total Instructional Staff\",\n    color = NULL,\n    title = \"Trends in Instructional Staff Employment Status, 1975-2011\",\n    subtitle = \"All Institutions, National Totals\",\n    caption = \"Source: US Department of Education, IPEDS Fall Staff Survey\"\n  ) +\n  scale_y_continuous(labels = label_percent(accuracy = 1, scale = 1)) +\n  theme(\n    legend.key.height = unit(1.5, \"cm\"),\n    plot.caption = element_text(size = 8, hjust = 0)\n  )"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#pay-attention-to-variable-types",
    "href": "ae/ae-17-effective-dataviz-A.html#pay-attention-to-variable-types",
    "title": "Trends instructional staff employees in universities",
    "section": "Pay attention to variable types",
    "text": "Pay attention to variable types\n\nQuestion: What is wrong with the x-axis of the plot above? How can you fix it?\n\nTime is represented as a character string (equally spaces between levels) instead of on a continuous scale (with spacing indicating numbers of years between ticks.\n\nYour turn: Implement the fix for the x-axis of the plot.\n\n\nstaff_long &lt;- staff_long |&gt;\n  mutate(year = as.numeric(year))\n\nggplot(\n  staff_long,\n  aes(\n    x = year,\n    y = percentage,\n    color = str_wrap(faculty_type, 20),\n    group = str_wrap(faculty_type, 20)\n  )\n) +\n  geom_line(linewidth = 1) +\n  labs(\n    x = NULL,\n    y = \"Percent of Total Instructional Staff\",\n    color = NULL,\n    title = \"Trends in Instructional Staff Employment Status, 1975-2011\",\n    subtitle = \"All Institutions, National Totals\",\n    caption = \"Source: US Department of Education, IPEDS Fall Staff Survey\"\n  ) +\n  scale_y_continuous(labels = label_percent(accuracy = 1, scale = 1)) +\n  theme(\n    legend.key.height = unit(1.5, \"cm\"),\n    plot.caption = element_text(size = 8, hjust = 0)\n  )"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#use-an-accessible-color-scale",
    "href": "ae/ae-17-effective-dataviz-A.html#use-an-accessible-color-scale",
    "title": "Trends instructional staff employees in universities",
    "section": "Use an accessible color scale",
    "text": "Use an accessible color scale\nQuestion: What do we mean by an accessible color scale? What types of color vision deficiencies are there?\n\nDemo: What does the plot look like to people with various color vision deficiencies?\nDemo: Remake the plot with an accessible color scale.\n\n\nggplot(\n  staff_long,\n  aes(\n    x = year,\n    y = percentage,\n    color = str_wrap(faculty_type, 20),\n    group = str_wrap(faculty_type, 20)\n    )\n  ) +\n  geom_line(linewidth = 1) +\n  labs(\n    x = NULL,\n    y = \"Percent of Total Instructional Staff\",\n    color = NULL,\n    title = \"Trends in Instructional Staff Employment Status, 1975-2011\",\n    subtitle = \"All Institutions, National Totals\",\n    caption = \"Source: US Department of Education, IPEDS Fall Staff Survey\"\n  ) +\n  scale_y_continuous(labels = label_percent(accuracy = 1, scale = 1)) +\n  theme(\n    legend.key.height = unit(1.5, \"cm\"),\n    plot.caption = element_text(size = 8, hjust = 0)\n  ) +\n  scale_color_colorblind() # from ggthemes package"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#use-direct-labeling",
    "href": "ae/ae-17-effective-dataviz-A.html#use-direct-labeling",
    "title": "Trends instructional staff employees in universities",
    "section": "Use direct labeling",
    "text": "Use direct labeling\n\nDemo: Remove the legend and add labels for each line at the end of the line (where x is the max(x) recorded).\n\n\nggplot(\n  staff_long,\n  aes(\n    x = year,\n    y = percentage,\n    color = faculty_type,\n    group = faculty_type\n    )\n  ) +\n  geom_line(linewidth = 1, show.legend = FALSE) +\n  geom_text(\n    data = staff_long |&gt; filter(year == max(year)),\n    aes(x = year + 1, y = percentage, label = faculty_type),\n    hjust = \"left\", show.legend = FALSE, size = 4\n  ) +\n  labs(\n    x = NULL,\n    y = \"Percent of Total Instructional Staff\",\n    color = NULL,\n    title = \"Trends in Instructional Staff Employment Status, 1975-2011\",\n    subtitle = \"All Institutions, National Totals\",\n    caption = \"Source: US Department of Education, IPEDS Fall Staff Survey\"\n  ) +\n  scale_y_continuous(labels = label_percent(accuracy = 1, scale = 1)) +\n  theme(\n    plot.caption = element_text(size = 8, hjust = 0),\n    plot.margin = margin(0.1, 2.5, 0.1, 0.1, unit = \"in\")\n  ) +\n  coord_cartesian(clip = \"off\") +\n  scale_color_colorblind()"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#use-color-to-draw-attention",
    "href": "ae/ae-17-effective-dataviz-A.html#use-color-to-draw-attention",
    "title": "Trends instructional staff employees in universities",
    "section": "Use color to draw attention",
    "text": "Use color to draw attention\n\nDemo: Redo the line plot where Part-time Faculty is red and the rest are gray.\n\n\nstaff_long &lt;- staff_long |&gt;\n  mutate(faculty_type_color = if_else(faculty_type == \"Part-Time Faculty\", \"firebrick3\", \"gray40\"))\n\n\nggplot(\n  staff_long,\n  aes(\n    x = year,\n    y = percentage,\n    color = faculty_type_color, group = faculty_type\n    )\n  ) +\n  geom_line(linewidth = 1, show.legend = FALSE) +\n  geom_text(\n    data = staff_long |&gt; filter(year == max(year)),\n    aes(x = year + 1, y = percentage, label = faculty_type),\n    hjust = \"left\", show.legend = FALSE, size = 4\n  ) +\n  labs(\n    x = NULL,\n    y = \"Percent of Total Instructional Staff\",\n    color = NULL,\n    title = \"Trends in Instructional Staff Employment Status, 1975-2011\",\n    subtitle = \"All Institutions, National Totals\",\n    caption = \"Source: US Department of Education, IPEDS Fall Staff Survey\"\n  ) +\n  scale_y_continuous(labels = label_percent(accuracy = 1, scale = 1)) +\n  scale_color_identity() +\n  theme(\n    plot.caption = element_text(size = 8, hjust = 0),\n    plot.margin = margin(0.1, 2.5, 0.1, 0.1, unit = \"in\")\n  ) +\n  coord_cartesian(clip = \"off\")"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#pick-a-purpose",
    "href": "ae/ae-17-effective-dataviz-A.html#pick-a-purpose",
    "title": "Trends instructional staff employees in universities",
    "section": "Pick a purpose",
    "text": "Pick a purpose\n\np &lt;- ggplot(\n  staff_long,\n  aes(\n    x = year,\n    y = percentage,\n    color = faculty_type_color, group = faculty_type\n    )\n  ) +\n  geom_line(linewidth = 1, show.legend = FALSE) +\n  labs(\n    x = NULL,\n    y = \"Percent of Total Instructional Staff\",\n    color = NULL,\n    title = \"Trends in Instructional Staff Employment Status, 1975-2011\",\n    subtitle = \"All Institutions, National Totals\",\n    caption = \"Source: US Department of Education, IPEDS Fall Staff Survey\"\n  ) +\n  scale_y_continuous(labels = label_percent(accuracy = 1, scale = 1)) +\n  scale_color_identity() +\n  theme(\n    plot.caption = element_text(size = 8, hjust = 0),\n    plot.margin = margin(0.1, 0.6, 0.1, 0.1, unit = \"in\")\n  ) +\n  coord_cartesian(clip = \"off\") +\n  annotate(\n    geom = \"text\",\n    x = 2012, y = 41, label = \"Part-Time\\nFaculty\",\n    color = \"firebrick3\", hjust = \"left\", size = 5\n  ) +\n  annotate(\n    geom = \"text\",\n    x = 2012, y = 13.5, label = \"Other\\nFaculty\",\n    color = \"gray40\", hjust = \"left\", size = 5\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = 2011.5, xend = 2011.5,\n    y = 7, yend = 20,\n    color = \"gray40\", linetype = \"dotted\"\n  )"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#use-labels-to-communicate-the-message",
    "href": "ae/ae-17-effective-dataviz-A.html#use-labels-to-communicate-the-message",
    "title": "Trends instructional staff employees in universities",
    "section": "Use labels to communicate the message",
    "text": "Use labels to communicate the message\n\np +\n  labs(\n    title = \"Instruction by part-time faculty on a steady increase\",\n    subtitle = \"Trends in Instructional Staff Employment Status, 1975-2011\\nAll Institutions, National Totals\",\n    caption = \"Source: US Department of Education, IPEDS Fall Staff Survey\",\n    y = \"Percent of Total Instructional Staff\",\n    x = NULL\n  )"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#simplify",
    "href": "ae/ae-17-effective-dataviz-A.html#simplify",
    "title": "Trends instructional staff employees in universities",
    "section": "Simplify",
    "text": "Simplify\n\np +\n  labs(\n    title = \"Instruction by part-time faculty on a steady increase\",\n    subtitle = \"Trends in Instructional Staff Employment Status, 1975-2011\\nAll Institutions, National Totals\",\n    caption = \"Source: US Department of Education, IPEDS Fall Staff Survey\",\n    y = \"Percent of Total Instructional Staff\",\n    x = NULL\n  ) +\n  theme(panel.grid.minor = element_blank())"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz-A.html#summary",
    "href": "ae/ae-17-effective-dataviz-A.html#summary",
    "title": "Trends instructional staff employees in universities",
    "section": "Summary",
    "text": "Summary\n\nRepresent percentages as parts of a whole\nPlace variables representing time on the x-axis when possible\nPay attention to data types, e.g., represent time as time on a continuous scale, not years as levels of a categorical variable\nPrefer direct labeling over legends\nUse accessible colors\nUse color to draw attention\nPick a purpose and label, color, annotate for that purpose\nCommunicate your main message directly in the plot labels\nSimplify before you call it done (a.k.a. “Before you leave the house, look in the mirror and take one thing off”)"
  },
  {
    "objectID": "ae/ae-00-unvotes.html",
    "href": "ae/ae-00-unvotes.html",
    "title": "UN Votes",
    "section": "",
    "text": "How do various countries vote in the United Nations General Assembly, how have their voting patterns evolved throughout time, and how similarly or differently do they view certain issues? Answering these questions (at a high level) is the focus of this analysis.\n\n\nWe will use the tidyverse, lubridate, and scales packages for data wrangling and visualization, and the DT package for interactive display of tabular output, and the unvotes package for the data.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(DT)\nlibrary(unvotes)\n\n\n\n\nThe data we’re using originally come from the unvotes package. In the chunk below we modify the data by joining the various data frames provided in the package to help you get started with the analysis.\n\nunvotes &lt;- un_votes |&gt;\n  inner_join(un_roll_calls, by = \"rcid\") |&gt;\n  inner_join(un_roll_call_issues, by = \"rcid\", relationship =\n  \"many-to-many\")"
  },
  {
    "objectID": "ae/ae-00-unvotes.html#introduction",
    "href": "ae/ae-00-unvotes.html#introduction",
    "title": "UN Votes",
    "section": "",
    "text": "How do various countries vote in the United Nations General Assembly, how have their voting patterns evolved throughout time, and how similarly or differently do they view certain issues? Answering these questions (at a high level) is the focus of this analysis.\n\n\nWe will use the tidyverse, lubridate, and scales packages for data wrangling and visualization, and the DT package for interactive display of tabular output, and the unvotes package for the data.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(DT)\nlibrary(unvotes)\n\n\n\n\nThe data we’re using originally come from the unvotes package. In the chunk below we modify the data by joining the various data frames provided in the package to help you get started with the analysis.\n\nunvotes &lt;- un_votes |&gt;\n  inner_join(un_roll_calls, by = \"rcid\") |&gt;\n  inner_join(un_roll_call_issues, by = \"rcid\", relationship =\n  \"many-to-many\")"
  },
  {
    "objectID": "ae/ae-00-unvotes.html#un-voting-patterns",
    "href": "ae/ae-00-unvotes.html#un-voting-patterns",
    "title": "UN Votes",
    "section": "UN voting patterns",
    "text": "UN voting patterns\nLet’s create a data visualisation that displays how the voting record of the UK & NI changed over time on a variety of issues, and compares it to two other countries: US and Turkey.\nWe can easily change which countries are being plotted by changing which countries the code above filters for. Note that the country name should be spelled and capitalized exactly the same way as it appears in the data. See the Appendix for a list of the countries in the data.\n\nunvotes |&gt;\n  filter(country %in% c(\"United Kingdom\", \"United States\", \"Turkey\")) |&gt;\n  mutate(year = year(date)) |&gt;\n  group_by(country, year, issue) |&gt;\n  summarize(percent_yes = mean(vote == \"yes\")) |&gt;\n  ggplot(mapping = aes(x = year, y = percent_yes, color = country)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  facet_wrap(~issue) +\n  scale_y_continuous(labels = percent) +\n  labs(\n    title = \"Percentage of 'Yes' votes in the UN General Assembly\",\n    subtitle = \"1946 to 2019\",\n    y = \"% Yes\",\n    x = \"Year\",\n    color = \"Country\"\n  )"
  },
  {
    "objectID": "ae/ae-00-unvotes.html#references",
    "href": "ae/ae-00-unvotes.html#references",
    "title": "UN Votes",
    "section": "References",
    "text": "References\n\nDavid Robinson (2017). unvotes: United Nations General Assembly Voting Data. R package version 0.2.0.\nErik Voeten “Data and Analyses of Voting in the UN General Assembly” Routledge Handbook of International Organization, edited by Bob Reinalda (published May 27, 2013).\nMuch of the analysis has been modeled on the examples presented in the unvotes package vignette."
  },
  {
    "objectID": "ae/ae-00-unvotes.html#appendix",
    "href": "ae/ae-00-unvotes.html#appendix",
    "title": "UN Votes",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of countries in the dataset:"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling-A.html",
    "href": "ae/ae-04-flights-wrangling-A.html",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key.\nlibrary(tidyverse)\nlibrary(nycflights13)"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling-A.html#exercise-1",
    "href": "ae/ae-04-flights-wrangling-A.html#exercise-1",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 1",
    "text": "Exercise 1\nYour turn: Fill in the blanks:\nThe flights data frame has 336776 rows. Each row represents a _flight_."
  },
  {
    "objectID": "ae/ae-04-flights-wrangling-A.html#exercise-2",
    "href": "ae/ae-04-flights-wrangling-A.html#exercise-2",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 2",
    "text": "Exercise 2\nYour turn: What are the names of the variables in flights.\n\nnames(flights)\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\""
  },
  {
    "objectID": "ae/ae-04-flights-wrangling-A.html#exercise-3---select",
    "href": "ae/ae-04-flights-wrangling-A.html#exercise-3---select",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 3 - select()",
    "text": "Exercise 3 - select()\n\nDemo: Make a data frame that only contains the variables dep_delay and arr_delay.\n\n\nflights |&gt;\n  select(dep_delay, arr_delay)\n\n# A tibble: 336,776 × 2\n   dep_delay arr_delay\n       &lt;dbl&gt;     &lt;dbl&gt;\n 1         2        11\n 2         4        20\n 3         2        33\n 4        -1       -18\n 5        -6       -25\n 6        -4        12\n 7        -5        19\n 8        -3       -14\n 9        -3        -8\n10        -2         8\n# ℹ 336,766 more rows\n\n\n\nDemo: Make a data frame that keeps every variable except dep_delay.\n\n\nflights |&gt;\n  select(-dep_delay)\n\n# A tibble: 336,776 × 18\n    year month   day dep_time sched_dep_time arr_time sched_arr_time arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     1      517            515      830            819        11\n 2  2013     1     1      533            529      850            830        20\n 3  2013     1     1      542            540      923            850        33\n 4  2013     1     1      544            545     1004           1022       -18\n 5  2013     1     1      554            600      812            837       -25\n 6  2013     1     1      554            558      740            728        12\n 7  2013     1     1      555            600      913            854        19\n 8  2013     1     1      557            600      709            723       -14\n 9  2013     1     1      557            600      838            846        -8\n10  2013     1     1      558            600      753            745         8\n# ℹ 336,766 more rows\n# ℹ 10 more variables: carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,\n#   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n#   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nDemo: Make a data frame that includes all variables between year through dep_delay (inclusive). These are all variables that provide information about the departure of each flight.\n\n\nflights |&gt;\n  select(year:dep_delay)\n\n# A tibble: 336,776 × 6\n    year month   day dep_time sched_dep_time dep_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1  2013     1     1      517            515         2\n 2  2013     1     1      533            529         4\n 3  2013     1     1      542            540         2\n 4  2013     1     1      544            545        -1\n 5  2013     1     1      554            600        -6\n 6  2013     1     1      554            558        -4\n 7  2013     1     1      555            600        -5\n 8  2013     1     1      557            600        -3\n 9  2013     1     1      557            600        -3\n10  2013     1     1      558            600        -2\n# ℹ 336,766 more rows\n\n\n\nDemo: Use the select helper contains() to make a data frame that includes the variables associated with the arrival, i.e., contains the string \"arr\\_\" in the name.\n\n\nflights |&gt;\n  select(contains(\"arr_\"))\n\n# A tibble: 336,776 × 3\n   arr_time sched_arr_time arr_delay\n      &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n 1      830            819        11\n 2      850            830        20\n 3      923            850        33\n 4     1004           1022       -18\n 5      812            837       -25\n 6      740            728        12\n 7      913            854        19\n 8      709            723       -14\n 9      838            846        -8\n10      753            745         8\n# ℹ 336,766 more rows"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling-A.html#exercise-4---slice",
    "href": "ae/ae-04-flights-wrangling-A.html#exercise-4---slice",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 4 - slice()",
    "text": "Exercise 4 - slice()\n\nDemo: Display the first five rows of the flights data frame.\n\n\nflights |&gt;\n  slice(1:5)\n\n# A tibble: 5 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      544            545        -1     1004           1022\n5  2013     1     1      554            600        -6      812            837\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nDemo: Display the last two rows of the flights data frame.\n\n\nflights |&gt;\n  slice((n()-1):n())\n\n# A tibble: 2 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     9    30       NA           1159        NA       NA           1344\n2  2013     9    30       NA            840        NA       NA           1020\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling-A.html#exercise-5---arrange",
    "href": "ae/ae-04-flights-wrangling-A.html#exercise-5---arrange",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 5 - arrange()",
    "text": "Exercise 5 - arrange()\n\nDemo: Let’s arrange the data by departure delay, so the flights with the shortest departure delays will be at the top of the data frame.\n\n\nflights |&gt;\n  arrange(dep_delay)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013    12     7     2040           2123       -43       40           2352\n 2  2013     2     3     2022           2055       -33     2240           2338\n 3  2013    11    10     1408           1440       -32     1549           1559\n 4  2013     1    11     1900           1930       -30     2233           2243\n 5  2013     1    29     1703           1730       -27     1947           1957\n 6  2013     8     9      729            755       -26     1002            955\n 7  2013    10    23     1907           1932       -25     2143           2143\n 8  2013     3    30     2030           2055       -25     2213           2250\n 9  2013     3     2     1431           1455       -24     1601           1631\n10  2013     5     5      934            958       -24     1225           1309\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nQuestion: What does it mean for the dep_delay to have a negative value?\n\nArrived early.\n\nDemo: Arrange the data by descending departure delay, so the flights with the longest departure delays will be at the top.\n\n\nflights |&gt;\n  arrange(desc(dep_delay))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     9      641            900      1301     1242           1530\n 2  2013     6    15     1432           1935      1137     1607           2120\n 3  2013     1    10     1121           1635      1126     1239           1810\n 4  2013     9    20     1139           1845      1014     1457           2210\n 5  2013     7    22      845           1600      1005     1044           1815\n 6  2013     4    10     1100           1900       960     1342           2211\n 7  2013     3    17     2321            810       911      135           1020\n 8  2013     6    27      959           1900       899     1236           2226\n 9  2013     7    22     2257            759       898      121           1026\n10  2013    12     5      756           1700       896     1058           2020\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nYour turn: Create a data frame that only includes the plane tail number (tailnum), carrier (carrier), and departure delay for the flight with the longest departure delay. What is the plane tail number (tailnum) for this flight?\n\n\nflights |&gt;\n  select(tailnum, carrier, dep_delay) %&gt;%\n  arrange(dep_delay) |&gt;\n  slice(1)\n\n# A tibble: 1 × 3\n  tailnum carrier dep_delay\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;\n1 N592JB  B6            -43"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling-A.html#exercise-6---filter",
    "href": "ae/ae-04-flights-wrangling-A.html#exercise-6---filter",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 6 - filter()",
    "text": "Exercise 6 - filter()\n\nDemo: Filter the data frame by selecting the rows where the destination airport is RDU.\n\n\nflights |&gt;\n  filter(dest == \"RDU\")\n\n# A tibble: 8,163 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      800            810       -10      949            955\n 2  2013     1     1      832            840        -8     1006           1030\n 3  2013     1     1      851            851         0     1032           1036\n 4  2013     1     1      917            920        -3     1052           1108\n 5  2013     1     1     1024           1030        -6     1204           1215\n 6  2013     1     1     1127           1129        -2     1303           1309\n 7  2013     1     1     1157           1205        -8     1342           1345\n 8  2013     1     1     1240           1235         5     1415           1415\n 9  2013     1     1     1317           1325        -8     1454           1505\n10  2013     1     1     1449           1450        -1     1651           1640\n# ℹ 8,153 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nDemo: We can also filter using more than one condition. Here we select all rows where the destination airport is RDU and the arrival delay is less than 0.\n\n\nflights |&gt;\n  filter(dest == \"RDU\", arr_delay &lt; 0)\n\n# A tibble: 4,232 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      800            810       -10      949            955\n 2  2013     1     1      832            840        -8     1006           1030\n 3  2013     1     1      851            851         0     1032           1036\n 4  2013     1     1      917            920        -3     1052           1108\n 5  2013     1     1     1024           1030        -6     1204           1215\n 6  2013     1     1     1127           1129        -2     1303           1309\n 7  2013     1     1     1157           1205        -8     1342           1345\n 8  2013     1     1     1317           1325        -8     1454           1505\n 9  2013     1     1     1505           1510        -5     1654           1655\n10  2013     1     1     1800           1800         0     1945           1951\n# ℹ 4,222 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nYour turn: Describe what the code is doing in words.\n\n\nflights |&gt;\n  filter(\n    dest %in% c(\"RDU\", \"GSO\"),\n    arr_delay &lt; 0 | dep_delay &lt; 0\n    )\n\n# A tibble: 6,203 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      800            810       -10      949            955\n 2  2013     1     1      832            840        -8     1006           1030\n 3  2013     1     1      851            851         0     1032           1036\n 4  2013     1     1      917            920        -3     1052           1108\n 5  2013     1     1     1024           1030        -6     1204           1215\n 6  2013     1     1     1127           1129        -2     1303           1309\n 7  2013     1     1     1157           1205        -8     1342           1345\n 8  2013     1     1     1317           1325        -8     1454           1505\n 9  2013     1     1     1449           1450        -1     1651           1640\n10  2013     1     1     1505           1510        -5     1654           1655\n# ℹ 6,193 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nHint: Logical operators in R:\n\n\n\n\n\n\n\noperator\ndefinition\n\n\n\n\n&lt;\nis less than?\n\n\n&lt;=\nis less than or equal to?\n\n\n&gt;\nis greater than?\n\n\n&gt;=\nis greater than or equal to?\n\n\n==\nis exactly equal to?\n\n\n!=\nis not equal to?\n\n\nx & y\nis x AND y?\n\n\nx \\| y\nis x OR y?\n\n\nis.na(x)\nis x NA?\n\n\n!is.na(x)\nis x not NA?\n\n\nx %in% y\nis x in y?\n\n\n!(x %in% y)\nis x not in y?\n\n\n!x\nis not x? (only makes sense if x is TRUE or FALSE)"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling-A.html#exercise-7---count",
    "href": "ae/ae-04-flights-wrangling-A.html#exercise-7---count",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 7 - count()",
    "text": "Exercise 7 - count()\n\nDemo: Create a frequency table of the destination locations for flights from New York.\n\n\nflights |&gt;\n  count(dest)\n\n# A tibble: 105 × 2\n   dest      n\n   &lt;chr&gt; &lt;int&gt;\n 1 ABQ     254\n 2 ACK     265\n 3 ALB     439\n 4 ANC       8\n 5 ATL   17215\n 6 AUS    2439\n 7 AVL     275\n 8 BDL     443\n 9 BGR     375\n10 BHM     297\n# ℹ 95 more rows\n\n\n\nDemo: In which month was there the fewest number of flights? How many flights were there in that month?\n\n\nflights |&gt;\n  count(month) |&gt;\n  filter(n == min(n))\n\n# A tibble: 1 × 2\n  month     n\n  &lt;int&gt; &lt;int&gt;\n1     2 24951\n\n\n\nYour turn: On which date (month + day) was there the largest number of flights? How many flights were there on that day?\n\n\nflights |&gt;\n  count(month, day) |&gt;\n  filter(n == max(n))\n\n# A tibble: 1 × 3\n  month   day     n\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1    11    27  1014"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling-A.html#exercise-8---mutate",
    "href": "ae/ae-04-flights-wrangling-A.html#exercise-8---mutate",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 8 - mutate()",
    "text": "Exercise 8 - mutate()\n\nDemo: Convert air_time (minutes in the air) to hours and then create a new variable, mph, the miles per hour of the flight.\n\n\nflights |&gt;\n  mutate(\n    hours = air_time / 60,\n    mph = distance / hours\n    ) |&gt;\n  select(air_time, distance, hours, mph)\n\n# A tibble: 336,776 × 4\n   air_time distance hours   mph\n      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      227     1400 3.78   370.\n 2      227     1416 3.78   374.\n 3      160     1089 2.67   408.\n 4      183     1576 3.05   517.\n 5      116      762 1.93   394.\n 6      150      719 2.5    288.\n 7      158     1065 2.63   404.\n 8       53      229 0.883  259.\n 9      140      944 2.33   405.\n10      138      733 2.3    319.\n# ℹ 336,766 more rows\n\n\n\nYour turn: Create a new variable to calculate the percentage of flights in each month. What percentage of flights take place in July?\n\n\nflights |&gt;\n  count(month) |&gt;\n  mutate(perc = n / sum(n) * 100)\n\n# A tibble: 12 × 3\n   month     n  perc\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1     1 27004  8.02\n 2     2 24951  7.41\n 3     3 28834  8.56\n 4     4 28330  8.41\n 5     5 28796  8.55\n 6     6 28243  8.39\n 7     7 29425  8.74\n 8     8 29327  8.71\n 9     9 27574  8.19\n10    10 28889  8.58\n11    11 27268  8.10\n12    12 28135  8.35\n\n\n\nDemo: Create a new variable, rdu_bound, which indicates whether the flight is to RDU or not. Then, for each departure airport (origin), calculate what proportion of flights originating from that airport are to RDU.\n\n\nflights |&gt;\n  mutate(rdu_bound = if_else(dest == \"RDU\", \"Yes\", \"No\")) |&gt;\n  count(origin, rdu_bound) |&gt;\n  group_by(origin) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  filter(rdu_bound == \"Yes\")\n\n# A tibble: 3 × 4\n# Groups:   origin [3]\n  origin rdu_bound     n   prop\n  &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n1 EWR    Yes        1482 0.0123\n2 JFK    Yes        3100 0.0279\n3 LGA    Yes        3581 0.0342"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling-A.html#exercise-9---summarize",
    "href": "ae/ae-04-flights-wrangling-A.html#exercise-9---summarize",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 9 - summarize()",
    "text": "Exercise 9 - summarize()\n\nDemo: Find mean arrival delay for all flights.\n\n\nflights |&gt;\n  summarize(mean_dep_delay = mean(dep_delay))\n\n# A tibble: 1 × 1\n  mean_dep_delay\n           &lt;dbl&gt;\n1             NA"
  },
  {
    "objectID": "ae/ae-04-flights-wrangling-A.html#exercise-10---group_by",
    "href": "ae/ae-04-flights-wrangling-A.html#exercise-10---group_by",
    "title": "AE 04: NYC flights + data wrangling",
    "section": "Exercise 10 - group_by()",
    "text": "Exercise 10 - group_by()\n\nDemo: Find mean arrival delay for for each month.\n\n\nflights |&gt;\n  group_by(month) |&gt;\n  summarize(mean_arr_delay = mean(arr_delay, na.rm = TRUE))\n\n# A tibble: 12 × 2\n   month mean_arr_delay\n   &lt;int&gt;          &lt;dbl&gt;\n 1     1          6.13 \n 2     2          5.61 \n 3     3          5.81 \n 4     4         11.2  \n 5     5          3.52 \n 6     6         16.5  \n 7     7         16.7  \n 8     8          6.04 \n 9     9         -4.02 \n10    10         -0.167\n11    11          0.461\n12    12         14.9  \n\n\n\nYour turn: What is the median departure delay for each airports around NYC (origin)? Which airport has the shortest median departure delay?\n\n\nflights |&gt;\n  group_by(origin) |&gt;\n  summarize(med_dep_delay = median(dep_delay, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  origin med_dep_delay\n  &lt;chr&gt;          &lt;dbl&gt;\n1 EWR               -1\n2 JFK               -1\n3 LGA               -3"
  },
  {
    "objectID": "ae/ae-18-second-to-last-ae.html",
    "href": "ae/ae-18-second-to-last-ae.html",
    "title": "Second to last AE",
    "section": "",
    "text": "Make a change, any change, to this document. Render, commit, and push by Sunday evening."
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html",
    "href": "ae/ae-16-equality-randomization.html",
    "title": "Equality",
    "section": "",
    "text": "In this application exercise, we’ll do inference on two population proportions."
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-1",
    "href": "ae/ae-16-equality-randomization.html#exercise-1",
    "title": "Equality",
    "section": "Exercise 1",
    "text": "Exercise 1\nThe two populations of interest in this survey are 18-24 year olds and 25+ year olds. State the hypotheses for evaluating whether there is a discernible difference between the proportions of those who think “The country needs to continue to make changes to give women equal rights to men.” (need more changes) in the two age groups.\nAdd response here."
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-2",
    "href": "ae/ae-16-equality-randomization.html#exercise-2",
    "title": "Equality",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat proportion of 18-24 year olds think “The country needs to continue to make changes to give women equal rights to men”? What proportion of 25+ year olds? Calculate and visualize these proportions.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-3",
    "href": "ae/ae-16-equality-randomization.html#exercise-3",
    "title": "Equality",
    "section": "Exercise 3",
    "text": "Exercise 3\nCalculate the observed sample statistic, i.e., the difference between the proportions of respondents who think “The country needs to continue to make changes to give women equal rights to men” between the two age groups.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-4",
    "href": "ae/ae-16-equality-randomization.html#exercise-4",
    "title": "Equality",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat is the parameter of interest?\nAdd response here."
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-5",
    "href": "ae/ae-16-equality-randomization.html#exercise-5",
    "title": "Equality",
    "section": "Exercise 5",
    "text": "Exercise 5\nExplain how you can set up a simulation for this hypothesis test.\nAdd response here."
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-6",
    "href": "ae/ae-16-equality-randomization.html#exercise-6",
    "title": "Equality",
    "section": "Exercise 6",
    "text": "Exercise 6\nConduct the hypothesis test using randomization and visualize and report the p-value.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-7",
    "href": "ae/ae-16-equality-randomization.html#exercise-7",
    "title": "Equality",
    "section": "Exercise 7",
    "text": "Exercise 7\nWhat is the conclusion of the hypothesis test?\nAdd response here."
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-8",
    "href": "ae/ae-16-equality-randomization.html#exercise-8",
    "title": "Equality",
    "section": "Exercise 8",
    "text": "Exercise 8\nInterpret the p-value in the context of the data and the hypotheses.\nAdd response here."
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-9",
    "href": "ae/ae-16-equality-randomization.html#exercise-9",
    "title": "Equality",
    "section": "Exercise 9",
    "text": "Exercise 9\nEstimate the difference in population proportions of 18-24 year old NC voters and 25+ year old NC voters using a 95% bootstrap interval.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-10",
    "href": "ae/ae-16-equality-randomization.html#exercise-10",
    "title": "Equality",
    "section": "Exercise 10",
    "text": "Exercise 10\nInterpret the confidence interval in context of the data.\nAdd response here."
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-11",
    "href": "ae/ae-16-equality-randomization.html#exercise-11",
    "title": "Equality",
    "section": "Exercise 11",
    "text": "Exercise 11\nDescribe how the simulation scheme for bootstrapping is different than that for the hypothesis test.\nAdd response here."
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-12",
    "href": "ae/ae-16-equality-randomization.html#exercise-12",
    "title": "Equality",
    "section": "Exercise 12",
    "text": "Exercise 12\nWhat is \\(p\\) vs. \\(\\hat{p}\\) vs. p-value. Explain generically as well as in the context of these data and research question.\nAdd response here."
  },
  {
    "objectID": "ae/ae-16-equality-randomization.html#exercise-13",
    "href": "ae/ae-16-equality-randomization.html#exercise-13",
    "title": "Equality",
    "section": "Exercise 13",
    "text": "Exercise 13\nWhat is bootstrap distribution vs. null distribution? Explain generically as well as in the context of these data and research question.\nAdd response here."
  },
  {
    "objectID": "ae/ae-02-bechdel-dataviz.html",
    "href": "ae/ae-02-bechdel-dataviz.html",
    "title": "AE 02: Bechdel + data visualization",
    "section": "",
    "text": "In this mini analysis we work with the data used in the FiveThirtyEight story titled “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”.\nThis analysis is about the Bechdel test, a measure of the representation of women in fiction."
  },
  {
    "objectID": "ae/ae-02-bechdel-dataviz.html#getting-started",
    "href": "ae/ae-02-bechdel-dataviz.html#getting-started",
    "title": "AE 02: Bechdel + data visualization",
    "section": "Getting started",
    "text": "Getting started\n\nPackages\nWe start with loading the packages we’ll use: tidyverse for majority of the analysis and scales for pretty plot labels later on.\n\nlibrary(tidyverse)\nlibrary(scales)\n\n\n\nData\nThe data are stored as a CSV (comma separated values) file in the data folder of your repository. Let’s read it from there and save it as an object called bechdel.\n\nbechdel &lt;- read_csv(\"https://sta199-s24.github.io/data/bechdel.csv\")\n\n\n\nGet to know the data\nWe can use the glimpse function to get an overview (or “glimpse”) of the data.\n\n# add code here\n\n\nWhat does each observation (row) in the data set represent?\n\nEach observation represents a ___.\n\nHow many observations (rows) are in the data set?\n\nThere are 1615 movies in the dataset.\n\nHow many variables (columns) are in the data set?\n\nThere are ___ columns in the dataset.\n\n\nVariables of interest\nThe variables we’ll focus on are the following:\n\nbudget_2013: Budget in 2013 inflation adjusted dollars.\ngross_2013: Gross (US and international combined) in 2013 inflation adjusted dollars.\nroi: Return on investment, calculated as the ratio of the gross to budget.\nclean_test: Bechdel test result:\n\nok = passes test\ndubious\nmen = women only talk about men\nnotalk = women don’t talk to each other\nnowomen = fewer than two women\n\nbinary: Bechdel Test PASS vs FAIL binary\n\nWe will also use the year of release in data prep and title of movie to take a deeper look at some outliers.\nThere are a few other variables in the dataset, but we won’t be using them in this analysis."
  },
  {
    "objectID": "ae/ae-02-bechdel-dataviz.html#visualizing-data-with-ggplot2",
    "href": "ae/ae-02-bechdel-dataviz.html#visualizing-data-with-ggplot2",
    "title": "AE 02: Bechdel + data visualization",
    "section": "Visualizing data with ggplot2",
    "text": "Visualizing data with ggplot2\nggplot2 is the package and ggplot() is the function in this package that is used to create a plot.\n\nggplot() creates the initial base coordinate system, and we will add layers to that base. We first specify the data set we will use with data = bechdel.\n\n\nggplot(data = bechdel)\n\n\n\n\n\n\n\n\n\nThe mapping argument is paired with an aesthetic (aes()), which tells us how the variables in our data set should be mapped to the visual properties of the graph.\n\n\nggplot(\n  data = bechdel, \n  mapping = aes(x = budget_2013, y = gross_2013)\n)\n\n\n\n\n\n\n\n\nAs we previously mentioned, we often omit the names of the first two arguments in R functions. So you’ll often see this written as:\n\nggplot(bechdel, aes(x = budget_2013, y = gross_2013))\n\n\n\n\n\n\n\n\nNote that the result is exactly the same.\n\nThe geom_xx function specifies the type of plot we want to use to represent the data. In the code below, we use geom_point which creates a plot where each observation is represented by a point.\n\n\nggplot(bechdel, aes(x = budget_2013, y = gross_2013)) +\n  geom_point()\n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nNote that this results in a warning as well. What does the warning mean?"
  },
  {
    "objectID": "ae/ae-02-bechdel-dataviz.html#gross-revenue-vs.-budget",
    "href": "ae/ae-02-bechdel-dataviz.html#gross-revenue-vs.-budget",
    "title": "AE 02: Bechdel + data visualization",
    "section": "Gross revenue vs. budget",
    "text": "Gross revenue vs. budget\n\nStep 1 - Your turn\nModify the following plot to change the color of all points to a different color.\n\n\n\n\n\n\nTip\n\n\n\nSee http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf for many color options you can use by name in R or use the hex code for a color of your choice.\n\n\n\nggplot(bechdel, aes(x = budget_2013, y = gross_2013)) +\n  geom_point(color = \"deepskyblue3\") \n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 2 - Your turn\nAdd labels for the title and x and y axes.\n\nggplot(bechdel, aes(x = budget_2013, y = gross_2013))+\n  geom_point(color = \"deepskyblue3\") + \n  labs(\n    x = \"___\", \n    y = \"___\", \n    title = \"___\"\n  )\n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 3 - Your turn\nAn aesthetic is a visual property of one of the objects in your plot. Commonly used aesthetic options are:\n\ncolor\nfill\nshape\nsize\nalpha (transparency)\n\nModify the plot below, so the color of the points is based on the variable binary.\n\nggplot(bechdel, aes(x = budget_2013, y = gross_2013)) +\n  geom_point() + \n  labs(\n    x = \"Budget (in 2013 $)\", \n    y = \"Gross revenue (in 2013 $)\", \n    title = \"Gross revenue vs. budget, by ___\"\n  )\n\nWarning: Removed 15 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStep 4 - Your turn\nExpand on your plot from the previous step to make the size of your points based on roi.\n\n# add code here\n\n\n\nStep 5 - Your turn\nExpand on your plot from the previous step to make the transparency (alpha) of the points 0.5.\n\n# add code here\n\n\n\nStep 6 - Your turn\nExpand on your plot from the previous step by using facet_wrap to display the association between budget and gross for different values of clean_test.\n\n# add code here\n\n\n\nStep 7 - Demo\nImprove your plot from the previous step by making the x and y scales more legible.\n\n\n\n\n\n\nTip\n\n\n\nMake use of the scales package, specifically the scale_x_continuous() and scale_y_continuous() functions.\n\n\n\n# add code here\n\n\n\nStep 8 - Your turn\nExpand on your plot from the previous step by using facet_grid to display the association between budget and gross for different combinations of clean_test and binary. Comment on whether this was a useful update.\n\n# add code here\n\nAdd comment here…\n\n\nStep 9 - Demo\nWhat other improvements could we make to this plot?\n\n# add code here\n\n\n\nRender, commit, and push\n\nIf you made any changes since the last render, render again to get the final version of the AE.\nCheck the box next to each document in the Git tab (this is called “staging” the changes). Commit the changes you made using a simple and informative message.\nUse the green arrow to push your changes to your repo on GitHub.\nCheck your repo on GitHub and see the updated files. Once your updated files are in your repo on GitHub, you’re good to go!"
  },
  {
    "objectID": "ae/ae-02-bechdel-dataviz.html#return-on-investment",
    "href": "ae/ae-02-bechdel-dataviz.html#return-on-investment",
    "title": "AE 02: Bechdel + data visualization",
    "section": "Return-on-investment",
    "text": "Return-on-investment\nFinally, let’s take a look at return-on-investment (ROI).\n\nStep 1 - Your turn\nCreate side-by-side box plots of roi by clean_test where the boxes are colored by binary.\n\n# add code here\n\nWhat are those movies with very high returns on investment?\n\nbechdel |&gt;\n  filter(roi &gt; 400) |&gt;\n  select(title, roi, budget_2013, gross_2013, year, clean_test)\n\n# A tibble: 3 × 6\n  title                     roi budget_2013 gross_2013  year clean_test\n  &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n1 Paranormal Activity      671.      505595  339424558  2007 dubious   \n2 The Blair Witch Project  648.      839077  543776715  1999 ok        \n3 El Mariachi              583.       11622    6778946  1992 nowomen   \n\n\n\n\nStep 2 - Demo\nExpand on your plot from the previous step to zoom in on movies with roi &lt; ___ to get a better view of how the medians across the categories compare.\n\n# add code here\n\nWhat does this plot say about return-on-investment on movies that pass the Bechdel test?\n\n\nRender, commit, and push\n\nIf you made any changes since the last render, render again to get the final version of the AE.\nCheck the box next to each document in the Git tab (this is called “staging” the changes). Commit the changes you made using a simple and informative message.\nUse the green arrow to push your changes to your repo on GitHub.\nCheck your repo on GitHub and see the updated files. Once your updated files are in your repo on GitHub, you’re good to go!"
  },
  {
    "objectID": "ae/ae-09-chronicle-scrape-A.html",
    "href": "ae/ae-09-chronicle-scrape-A.html",
    "title": "AE 09: Opinion articles in The Chronicle",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key."
  },
  {
    "objectID": "ae/ae-09-chronicle-scrape-A.html#part-1---data-scraping",
    "href": "ae/ae-09-chronicle-scrape-A.html#part-1---data-scraping",
    "title": "AE 09: Opinion articles in The Chronicle",
    "section": "Part 1 - Data scraping",
    "text": "Part 1 - Data scraping\nSee chronicle-scrape.R for suggested scraping code."
  },
  {
    "objectID": "ae/ae-09-chronicle-scrape-A.html#part-2---data-analysis",
    "href": "ae/ae-09-chronicle-scrape-A.html#part-2---data-analysis",
    "title": "AE 09: Opinion articles in The Chronicle",
    "section": "Part 2 - Data analysis",
    "text": "Part 2 - Data analysis\nLet’s start by loading the packages we will need:\n\nlibrary(tidyverse)\n\n\nYour turn (1 minute): Load the data you saved into the data folder and name it chronicle.\n\n\nchronicle &lt;- read_csv(\"data/chronicle.csv\")\n\n\nYour turn (3 minutes): Who are the most prolific authors of the 100 most recent opinion articles in The Chronicle?\n\n\nchronicle |&gt;\n  count(author, sort = TRUE)\n\n# A tibble: 204 × 2\n   author                        n\n   &lt;chr&gt;                     &lt;int&gt;\n 1 Luke A. Powery               30\n 2 Heidi Smith                  27\n 3 Advikaa Anand                22\n 4 Monday Monday                17\n 5 Monika Narain                16\n 6 Community Editorial Board    12\n 7 Linda Cao                    12\n 8 Sonia Green                  12\n 9 Valerie Tan                  11\n10 Nathan Luzum                 10\n# ℹ 194 more rows\n\n\n\nDemo: Draw a line plot of the number of opinion articles published per day in The Chronicle.\n\n\nchronicle |&gt;\n  count(date) |&gt;\n  ggplot(aes(x = date, y = n, group = 1)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nDemo: What percent of the most recent 100 opinion articles in The Chronicle mention “climate” in their title?\n\n\nchronicle |&gt;\n  mutate(\n    title = str_to_lower(title),\n    climate = if_else(str_detect(title, \"climate\"), \"mentioned\", \"not mentioned\")\n    ) |&gt;\n  count(climate) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  climate           n  prop\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt;\n1 mentioned        10  0.02\n2 not mentioned   490  0.98\n\n\n\nYour turn (5 minutes): What percent of the most recent 100 opinion articles in The Chronicle mention “climate” in their title or abstract?\n\n\nchronicle |&gt;\n  mutate(\n    title = str_to_lower(title),\n    abstract = str_to_lower(abstract),\n    climate = if_else(\n      str_detect(title, \"climate\") | str_detect(abstract, \"climate\"), \n      \"mentioned\", \n      \"not mentioned\"\n      )\n    ) |&gt;\n  count(climate) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 3 × 3\n  climate           n  prop\n  &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt;\n1 mentioned        14 0.028\n2 not mentioned   482 0.964\n3 &lt;NA&gt;              4 0.008\n\n\n\nTime permitting: Come up with another question and try to answer it using the data.\n\n\n# add code here\n\n\nTime permitting:"
  },
  {
    "objectID": "ae/ae-13-modeling-loans-A.html",
    "href": "ae/ae-13-modeling-loans-A.html",
    "title": "Modelling loan interest rates",
    "section": "",
    "text": "In this application exercise we will be studying loan interest rates. The dataset is one you’ve come across before in your reading – the dataset about loans from the peer-to-peer lender, Lending Club, from the openintro package. We will use tidyverse and tidymodels for data exploration and modeling, respectively.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nBefore we use the dataset, we’ll make a few transformations to it.\nAdd response here.\nloans &lt;- loans_full_schema |&gt;\n  mutate(\n    credit_util = total_credit_utilized / total_credit_limit,\n    bankruptcy = as.factor(if_else(public_record_bankrupt == 0, 0, 1)),\n    verified_income = droplevels(verified_income),\n    homeownership = str_to_title(homeownership),\n    homeownership = fct_relevel(homeownership, \"Rent\", \"Mortgage\", \"Own\")\n  ) |&gt;\n  rename(credit_checks = inquiries_last_12m) |&gt;\n  select(\n    interest_rate, loan_amount, verified_income, \n    debt_to_income, credit_util, bankruptcy, term, \n    credit_checks, issue_month, homeownership\n  )\nHere is a glimpse at the data:\nglimpse(loans)\n\nRows: 10,000\nColumns: 10\n$ interest_rate   &lt;dbl&gt; 14.07, 12.61, 17.09, 6.72, 14.07, 6.72, 13.59, 11.99, …\n$ loan_amount     &lt;int&gt; 28000, 5000, 2000, 21600, 23000, 5000, 24000, 20000, 2…\n$ verified_income &lt;fct&gt; Verified, Not Verified, Source Verified, Not Verified,…\n$ debt_to_income  &lt;dbl&gt; 18.01, 5.04, 21.15, 10.16, 57.96, 6.46, 23.66, 16.19, …\n$ credit_util     &lt;dbl&gt; 0.54759517, 0.15003472, 0.66134832, 0.19673228, 0.7549…\n$ bankruptcy      &lt;fct&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, …\n$ term            &lt;dbl&gt; 60, 36, 36, 36, 36, 36, 60, 60, 36, 36, 60, 60, 36, 60…\n$ credit_checks   &lt;int&gt; 6, 1, 4, 0, 7, 6, 1, 1, 3, 0, 4, 4, 8, 6, 0, 0, 4, 6, …\n$ issue_month     &lt;fct&gt; Mar-2018, Feb-2018, Feb-2018, Jan-2018, Mar-2018, Jan-…\n$ homeownership   &lt;fct&gt; Mortgage, Rent, Rent, Rent, Rent, Own, Mortgage, Mortg…"
  },
  {
    "objectID": "ae/ae-13-modeling-loans-A.html#main-effects-model",
    "href": "ae/ae-13-modeling-loans-A.html#main-effects-model",
    "title": "Modelling loan interest rates",
    "section": "Main effects model",
    "text": "Main effects model\n\nDemo: Fit a model to predict interest rate from credit utilization and homeownership, without an interaction effect between the two predictors. Display the summary output and write out the estimated regression equation.\n\n\nrate_util_home_fit &lt;- linear_reg() |&gt;\n  fit(interest_rate ~ credit_util + homeownership, data = loans)\n\ntidy(rate_util_home_fit)\n\n# A tibble: 4 × 5\n  term                  estimate std.error statistic   p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)              9.93      0.140    70.8   0        \n2 credit_util              5.34      0.207    25.7   2.20e-141\n3 homeownershipMortgage    0.696     0.121     5.76  8.71e-  9\n4 homeownershipOwn         0.128     0.155     0.827 4.08e-  1\n\n\n\\[\n\\widehat{interest~rate} = 9.93 + 5.34 \\times credit~util + 0.696 \\times Mortgage + 0.128 \\times Own\n\\]\n\nDemo: Write the estimated regression equation for loan applications from each of the homeownership groups separately.\n\nRent: \\(\\widehat{interest~rate} = 9.93 + 5.34 \\times credit~util\\)\nMortgage: \\(\\widehat{interest~rate} = 10.626 + 5.34 \\times credit~util\\)\nOwn: \\(\\widehat{interest~rate} = 10.058 + 5.34 \\times credit~util\\)\n\nQuestion: How does the model predict the interest rate to vary as credit utilization varies for loan applicants with different homeownership status. Are the rates the same or different?\n\nThe same."
  },
  {
    "objectID": "ae/ae-13-modeling-loans-A.html#interaction-effects-model",
    "href": "ae/ae-13-modeling-loans-A.html#interaction-effects-model",
    "title": "Modelling loan interest rates",
    "section": "Interaction effects model",
    "text": "Interaction effects model\n\nDemo: Fit a model to predict interest rate from credit utilization and homeownership, with an interaction effect between the two predictors. Display the summary output and write out the estimated regression equation.\n\n\nrate_util_home_int_fit &lt;- linear_reg() |&gt;\n  fit(interest_rate ~ credit_util * homeownership, data = loans)\n\ntidy(rate_util_home_int_fit)\n\n# A tibble: 6 × 5\n  term                              estimate std.error statistic  p.value\n  &lt;chr&gt;                                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                          9.44      0.199     47.5  0       \n2 credit_util                          6.20      0.325     19.1  1.01e-79\n3 homeownershipMortgage                1.39      0.228      6.11 1.04e- 9\n4 homeownershipOwn                     0.697     0.316      2.20 2.75e- 2\n5 credit_util:homeownershipMortgage   -1.64      0.457     -3.58 3.49e- 4\n6 credit_util:homeownershipOwn        -1.06      0.590     -1.80 7.24e- 2\n\n\n\\[\n\\widehat{interest~rate} = 9.44 + 6.20 \\times credit~util + 1.39 \\times Mortgage + 0.697 \\times Own - 1.64 \\times credit_util:Mortgage - 1.06 \\times credit_util:Own\n\\]\n\nDemo: Write the estimated regression equation for loan applications from each of the homeownership groups separately.\n\nRent: \\(\\widehat{interest~rate} = 9.44 + 6.20 \\times credit~util\\)\nMortgage: \\(\\widehat{interest~rate} = 10.83 + 4.56 \\times credit~util\\)\nOwn: \\(\\widehat{interest~rate} = 10.137 + 5.14 \\times credit~util\\)\n\nQuestion: How does the model predict the interest rate to vary as credit utilization varies for loan applicants with different homeownership status. Are the rates the same or different?\n\nDifferent."
  },
  {
    "objectID": "ae/ae-13-modeling-loans-A.html#choosing-a-model",
    "href": "ae/ae-13-modeling-loans-A.html#choosing-a-model",
    "title": "Modelling loan interest rates",
    "section": "Choosing a model",
    "text": "Choosing a model\nRule of thumb: Occam’s Razor - Don’t overcomplicate the situation! We prefer the simplest best model.\n\nglance(rate_util_home_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    0.0682        0.0679  4.83      244. 1.25e-152     3 -29926. 59861. 59897.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(rate_util_home_int_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    0.0694        0.0689  4.83      149. 4.79e-153     5 -29919. 59852. 59903.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nReview: What is R-squared? What is adjusted R-squared?\n\nR-squared is the percent variability in the response that is explained by our model. (Can use when models have same number of variables for model selection)\nAdjusted R-squared is similar, but has a penalty for the number of variables in the model. (Should use for model selection when models have different numbers of variables).\n\nQuestion: Based on the adjusted \\(R^2\\)s of these two models, which one do we prefer?\n\nThe interaction effects model, though just barely."
  },
  {
    "objectID": "ae/ae-11-modeling-penguins-A.html",
    "href": "ae/ae-11-modeling-penguins-A.html",
    "title": "AE 11: Modelling penguins",
    "section": "",
    "text": "In this application exercise we will be studying penguins. The data can be found in the palmerpenguins package and we will use tidyverse and tidymodels for data exploration and modeling, respectively.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\nPlease read the following context and take a glimpse at the data set before we get started.\n\nThis data set comprising various measurements of three different penguin species, namely Adelie, Gentoo, and Chinstrap. The rigorous study was conducted in the islands of the Palmer Archipelago, Antarctica. These data were collected from 2007 to 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. The data set is called penguins.\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nOur goal is to understand better how various body measurements and attributes of penguins relate to their body mass. First, we are going to investigate the relationship between a penguins’ flipper lengths and their body masses.\n\nQuestion: Based on our research focus, which variable is the response variable?\n\nBody mass.\n\nDemo: Visualize the relationship between flipper length and body mass of penguins.\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nCorrelation\n\nYour turn (5 minutes):\n\nWhat is correlation? What values can correlation take?\nStrength and direction of a linear relationship. It’s bounded by -1 and 1.\nAre you good at guessing correlation? Give it a try! https://www.rossmanchance.com/applets/2021/guesscorrelation/GuessCorrelation.html\n\nDemo: What is the correlation between flipper length and body mass of penguins?\n\n\n# option 1\npenguins |&gt;\n  summarize(r = cor(flipper_length_mm, body_mass_g, use = \"complete.obs\"))\n\n# A tibble: 1 × 1\n      r\n  &lt;dbl&gt;\n1 0.871\n\n# option 2\npenguins |&gt;\n  drop_na(flipper_length_mm, body_mass_g) |&gt;\n  summarize(r = cor(flipper_length_mm, body_mass_g))\n\n# A tibble: 1 × 1\n      r\n  &lt;dbl&gt;\n1 0.871\n\n\n\n\nDefining, fitting, and summarizing a model\n\nDemo: Write the population model below that explains the relationship between body mass and flipper length.\n\n\\[\nbody~mass = \\beta_0 + \\beta_1 \\times flipper~length + \\epsilon\n\\]\n\nDemo: Fit the linear regression model and display the results. Write the estimated model output below.\n\n\nbm_fl_fit &lt;- linear_reg() |&gt;\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\ntidy(bm_fl_fit)\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\n\\[\n\\widehat{body~mass} = -5781 + 49.7 \\times flipper~length\n\\]\n\nYour turn: Interpret the slope and the intercept in the context of the data.\n\nIntercept: Penguins with 0 flipper length are expected, on average, to weigh 5,781 grams.\nSlopes: For each additional millimeter of a penguin;s flipper length, the weight of their penguin is expected to be higher, on average, by 49.7 grams.\n\nYour turn: Recreate the visualization from above, this time adding a regression line to the visualization geom_smooth(method = \"lm\").\n\n\nggplot(penguins,\n       aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nYour turn: What is the estimated body mass for a penguin with a flipper length of 210?\n\n\nbm_fl_fit |&gt;\n  predict(new_data = tibble(flipper_length_mm = 210))\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1 4653.\n\n\n\nDemo: What is the estimated body mass for a penguin with a flipper length of 100?\n\nBut we shouldn’t do this prediction based on this model since 100 mm is outside of the range of the data (extrapolation).\n\nbm_fl_fit |&gt;\n  predict(new_data = tibble(flipper_length_mm = 100))\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1 -812.\n\n\n\n\nAnother model\n\nDemo: A different researcher wants to look at body weight of penguins based on the island they were recorded on. How are the variables involved in this analysis different?\n\nPredictor is categorical.\n\nDemo: Make an appropriate visualization to investigate this relationship below. Additionally, calculate the mean body mass by island.\n\n\nggplot(penguins, aes(x = island, y = body_mass_g)) +\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\npenguins |&gt;\n  group_by(island) |&gt;\n  summarize(mean_bm = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  island    mean_bm\n  &lt;fct&gt;       &lt;dbl&gt;\n1 Biscoe      4716.\n2 Dream       3713.\n3 Torgersen   3706.\n\n\n\nDemo: Change the geom of your previous plot to geom_point(). Use this plot to think about how R models these data.\n\n\nggplot(penguins, aes(x = island, y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nYour turn: Fit the linear regression model and display the results. Write the estimated model output below.\n\n\nbm_island_fit &lt;- linear_reg() |&gt;\n  fit(body_mass_g ~ island, data = penguins)\n\ntidy(bm_island_fit)\n\n# A tibble: 3 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        4716.      48.5      97.3 8.93e-250\n2 islandDream       -1003.      74.2     -13.5 1.42e- 33\n3 islandTorgersen   -1010.     100.      -10.1 4.66e- 21\n\n\n\nDemo: Interpret each coefficient in context of the problem.\n\nIntercept: Penguins from Biscoe island are expected to weigh, on average, 4,716 grams.\nSlopes:\n\nPenguins from Dream island are expected to weigh, on average, 1,003 grams less than those from Biscoe island.\nPenguins from Torgersen island are expected to weigh, on average, 1,010 grams less than those from Biscoe island.\n\n\nDemo: What is the estimated body weight of a penguin on Biscoe island? What are the estimated body weights of penguins on Dream and Torgersen islands?\n\n\nbm_island_fit |&gt;\n  predict(new_data = tibble(island = c(\"Biscoe\", \"Dream\", \"Torgersen\")))\n\n# A tibble: 3 × 1\n  .pred\n  &lt;dbl&gt;\n1 4716.\n2 3713.\n3 3706."
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap-A.html",
    "href": "ae/ae-15-duke-forest-bootstrap-A.html",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "",
    "text": "In this application exercise, we will\nThe dataset are on housing prices in Duke Forest – a dataset you’ve seen before! It’s called duke_forest and it’s in the openintro package. Additionally, we’ll use tidyverse and tidymodels packages.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)"
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap-A.html#exercise-1",
    "href": "ae/ae-15-duke-forest-bootstrap-A.html#exercise-1",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "Exercise 1",
    "text": "Exercise 1\nVisualize the distribution of sizes of houses in Duke Forest. What is the size of a typical house?\n\nggplot(duke_forest, aes(x = area)) +\n  geom_histogram(binwidth = 250)"
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap-A.html#exercise-2",
    "href": "ae/ae-15-duke-forest-bootstrap-A.html#exercise-2",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "Exercise 2",
    "text": "Exercise 2\nConstruct a 95% confidence interval for the typical size of a house in Duke Forest. Interpret the interval in context of the data.\n\nset.seed(12345)\ndf_araa_median_boot &lt;- duke_forest |&gt;\n  specify(response = area) |&gt;\n  generate(reps = 100, type = \"bootstrap\") |&gt;\n  calculate(stat = \"median\")\n\nggplot(df_araa_median_boot, aes(x = stat)) +\n  geom_histogram(binwidth = 50)\n\n\n\n\n\n\n\ndf_araa_median_boot |&gt;\n  summarize(\n    l = quantile(stat, 0.025),\n    u = quantile(stat, 0.975)\n  )\n\n# A tibble: 1 × 2\n      l     u\n  &lt;dbl&gt; &lt;dbl&gt;\n1 2365. 2836.\n\n\nWe are 95% confident that the median house in Duke Forest is between 2,365 and 2,836 square feet."
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap-A.html#exercise-3",
    "href": "ae/ae-15-duke-forest-bootstrap-A.html#exercise-3",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "Exercise 3",
    "text": "Exercise 3\nWithout calculating it – would a 90% confidence interval be wider or narrower? Why?\nNarrower, lower confidence level needed so we can be more precise."
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap-A.html#exercise-4",
    "href": "ae/ae-15-duke-forest-bootstrap-A.html#exercise-4",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "Exercise 4",
    "text": "Exercise 4\nConstruct the 90% confidence interval and interpret it.\n\ndf_araa_median_boot |&gt;\n  summarize(\n    l = quantile(stat, 0.05),\n    u = quantile(stat, 0.95)\n  )\n\n# A tibble: 1 × 2\n      l     u\n  &lt;dbl&gt; &lt;dbl&gt;\n1 2395.  2830\n\n\nWe are 95% confident that the median house in Duke Forest is between 2,395 and 2,830 square feet."
  },
  {
    "objectID": "ae/ae-15-duke-forest-bootstrap-A.html#exercise-5",
    "href": "ae/ae-15-duke-forest-bootstrap-A.html#exercise-5",
    "title": "AE 15: Modeling houses in Duke Forest",
    "section": "Exercise 5",
    "text": "Exercise 5\nQuantify the uncertainty around this slope using a 95% bootstrap confidence interval and interpret the interval in context of the data.\n\ndf_price_area_boot &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"slope\")\n\nggplot(df_price_area_boot, aes(x = stat)) +\n  geom_histogram(binwidth = 10)\n\n\n\n\n\n\n\ndf_price_area_boot |&gt;\n  summarize(\n    l = quantile(stat, 0.025),\n    u = quantile(stat, 0.975)\n  )\n\n# A tibble: 1 × 2\n      l     u\n  &lt;dbl&gt; &lt;dbl&gt;\n1  89.9  214.\n\n\nWe are 95% confident that, for each additional square feet, the model predicts that prices of houses in Duke Forest are higher by $89.9 to $214, on average."
  },
  {
    "objectID": "ae/ae-14-spam-filter-A.html",
    "href": "ae/ae-14-spam-filter-A.html",
    "title": "Building a spam filter",
    "section": "",
    "text": "In this application exercise, we will\nTo illustrate logistic regression, we will build a spam filter from email data.\nThe data come from incoming emails in David Diez’s (one of the authors of OpenIntro textbooks) Gmail account for the first three months of 2012. All personally identifiable information has been removed.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nglimpse(email)\n\nRows: 3,921\nColumns: 21\n$ spam         &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ from         &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, …\n$ sent_email   &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n$ time         &lt;dttm&gt; 2012-01-01 01:16:41, 2012-01-01 02:03:59, 2012-01-01 11:…\n$ image        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       &lt;dbl&gt; 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, …\n$ winner       &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     &lt;dbl&gt; 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ num_char     &lt;dbl&gt; 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421…\n$ line_breaks  &lt;int&gt; 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191…\n$ format       &lt;fct&gt; 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ re_subj      &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, …\n$ exclaim_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ urgent_subj  &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess &lt;dbl&gt; 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0…\n$ number       &lt;fct&gt; big, small, small, small, none, none, big, small, small, …\nThe variables we’ll use in this analysis are\nGoal: Use the number of exclamation points in an email to predict whether or not it is spam."
  },
  {
    "objectID": "ae/ae-14-spam-filter-A.html#exercise-1",
    "href": "ae/ae-14-spam-filter-A.html#exercise-1",
    "title": "Building a spam filter",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nDemo: Fit the logistic regression model using the number of exclamation points to predict the probability an email is spam.\n\n\nlog_fit &lt;- logistic_reg() |&gt;\n  fit(spam ~ exclaim_mess, data = email)\n\ntidy(log_fit)\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -2.27      0.0553     -41.1     0    \n2 exclaim_mess  0.000272  0.000949     0.287   0.774\n\n\n\nYour turn: How does the code above differ from previous code we’ve used to fit regression models? Compare your summary output to the estimated model below.\n\n\\[\\log\\Big(\\frac{p}{1-p}\\Big) = -2.27 - 0.000272 \\times exclaim\\_mess\\]\nWe use logistic instead of linear regression."
  },
  {
    "objectID": "ae/ae-14-spam-filter-A.html#exercise-2",
    "href": "ae/ae-14-spam-filter-A.html#exercise-2",
    "title": "Building a spam filter",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat is the probability the email is spam if it contains 10 exclamation points? Answer the question using the predict() function.\nWe can use the predict function in R to produce the probability as well.\n\nep_10 &lt;- tibble(exclaim_mess = 10)\npredict(log_fit, ep_10, type = \"prob\")\n\n# A tibble: 1 × 2\n  .pred_0 .pred_1\n    &lt;dbl&gt;   &lt;dbl&gt;\n1   0.906  0.0937"
  },
  {
    "objectID": "ae/ae-14-spam-filter-A.html#exercise-3",
    "href": "ae/ae-14-spam-filter-A.html#exercise-3",
    "title": "Building a spam filter",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe have the probability an email is spam, but ultimately we want to use the probability to classify an email as spam or not spam. Therefore, we need to set a decision-making threshold, such that an email is classified as spam if the predicted probability is greater than the threshold and not spam otherwise.\nSuppose you are a data scientist working on a spam filter. You must determine how high the predicted probability must be before you think it would be reasonable to call it spam and put it in the junk folder (which the user is unlikely to check).\nYour turn: What are some trade offs you would consider as you set the decision-making threshold? Discuss with your neighbor.\nAnswers will vary.\n\naugment(log_fit, email) |&gt;\n  select(spam, exclaim_mess, .pred_class) |&gt;\n  ggplot(aes(x = exclaim_mess, y = spam, color = .pred_class)) +\n  geom_jitter(alpha = 0.5)"
  },
  {
    "objectID": "ae/ae-14-spam-filter-A.html#exercise-4",
    "href": "ae/ae-14-spam-filter-A.html#exercise-4",
    "title": "Building a spam filter",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit a model with all variables in the dataset as predictors and recreate the visualization above for this model.\n\nlog_fit2 &lt;- logistic_reg() |&gt;\n  fit(spam ~ ., data = email)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nlog_aug2 &lt;- augment(log_fit2, email)\n\nggplot(log_aug2, aes(x = exclaim_mess, y = spam, color = .pred_class)) +\n  geom_jitter(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nlog_aug2 |&gt;\n  count(spam, .pred_class) |&gt;\n  group_by(spam) |&gt;\n  mutate(p = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   spam [2]\n  spam  .pred_class     n       p\n  &lt;fct&gt; &lt;fct&gt;       &lt;int&gt;   &lt;dbl&gt;\n1 0     0            3521 0.991  \n2 0     1              33 0.00929\n3 1     0             299 0.815  \n4 1     1              68 0.185"
  },
  {
    "objectID": "ae/ae-05-majors-tidying-A.html",
    "href": "ae/ae-05-majors-tidying-A.html",
    "title": "AE 05: Tidying StatSci Majors",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key."
  },
  {
    "objectID": "ae/ae-05-majors-tidying-A.html#goal",
    "href": "ae/ae-05-majors-tidying-A.html#goal",
    "title": "AE 05: Tidying StatSci Majors",
    "section": "Goal",
    "text": "Goal\nOur ultimate goal in this application exercise is to make the following data visualization."
  },
  {
    "objectID": "ae/ae-05-majors-tidying-A.html#data",
    "href": "ae/ae-05-majors-tidying-A.html#data",
    "title": "AE 05: Tidying StatSci Majors",
    "section": "Data",
    "text": "Data\nThe data come from the Office of the University Registrar. They make the data available as a table that you can download as a PDF, but I’ve put the data exported in a CSV file for you. Let’s load that in.\n\nlibrary(tidyverse)\n\nstatsci &lt;- read_csv(\"https://sta199-s24.github.io/data/statsci.csv\")\n\nAnd let’s take a look at the data.\n\nstatsci\n\n# A tibble: 4 × 14\n  degree   `2011` `2012` `2013` `2014` `2015` `2016` `2017` `2018` `2019` `2020`\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Statist…     NA      1     NA     NA      4      4      1     NA     NA      1\n2 Statist…      2      2      4      1      3      6      3      4      4      1\n3 Statist…      2      6      1     NA      5      6      6      8      8     17\n4 Statist…      5      9      4     13     10     17     24     21     26     27\n# ℹ 3 more variables: `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;, `2023` &lt;dbl&gt;"
  },
  {
    "objectID": "ae/ae-05-majors-tidying-A.html#pivoting",
    "href": "ae/ae-05-majors-tidying-A.html#pivoting",
    "title": "AE 05: Tidying StatSci Majors",
    "section": "Pivoting",
    "text": "Pivoting\n\nDemo: Pivot the statsci data frame longer such that each row represents a degree type / year combination and year and number of graduates for that year are columns in the data frame.\n\n\nstatsci |&gt;\n  pivot_longer(\n    cols = -degree,\n    names_to = \"year\",\n    values_to = \"n\"\n  )\n\n# A tibble: 52 × 3\n   degree                    year      n\n   &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt;\n 1 Statistical Science (AB2) 2011     NA\n 2 Statistical Science (AB2) 2012      1\n 3 Statistical Science (AB2) 2013     NA\n 4 Statistical Science (AB2) 2014     NA\n 5 Statistical Science (AB2) 2015      4\n 6 Statistical Science (AB2) 2016      4\n 7 Statistical Science (AB2) 2017      1\n 8 Statistical Science (AB2) 2018     NA\n 9 Statistical Science (AB2) 2019     NA\n10 Statistical Science (AB2) 2020      1\n# ℹ 42 more rows\n\n\n\nQuestion: What is the type of the year variable? Why? What should it be?\n\nIt’s a character (chr) variable since the information came from the columns of the original data frame and R cannot know that these character strings represent years. The variable type should be numeric.\n\nDemo: Start over with pivoting, and this time also make sure year is a numerical variable in the resulting data frame.\n\n\nstatsci |&gt;\n  pivot_longer(\n    cols = -degree,\n    names_to = \"year\",\n    names_transform = as.numeric,\n    values_to = \"n\"\n  )\n\n# A tibble: 52 × 3\n   degree                     year     n\n   &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt;\n 1 Statistical Science (AB2)  2011    NA\n 2 Statistical Science (AB2)  2012     1\n 3 Statistical Science (AB2)  2013    NA\n 4 Statistical Science (AB2)  2014    NA\n 5 Statistical Science (AB2)  2015     4\n 6 Statistical Science (AB2)  2016     4\n 7 Statistical Science (AB2)  2017     1\n 8 Statistical Science (AB2)  2018    NA\n 9 Statistical Science (AB2)  2019    NA\n10 Statistical Science (AB2)  2020     1\n# ℹ 42 more rows\n\n\n\nQuestion: What does an NA mean in this context? Hint: The data come from the university registrar, and they have records on every single graduates, there shouldn’t be anything “unknown” to them about who graduated when.\n\nNAs should actually be 0s.\n\nDemo: Add on to your pipeline that you started with pivoting and convert NAs in n to 0s.\n\n\nstatsci |&gt;\n  pivot_longer(\n    cols = -degree,\n    names_to = \"year\",\n    names_transform = as.numeric,\n    values_to = \"n\"\n  ) |&gt;\n  mutate(n = if_else(is.na(n), 0, n))\n\n# A tibble: 52 × 3\n   degree                     year     n\n   &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt;\n 1 Statistical Science (AB2)  2011     0\n 2 Statistical Science (AB2)  2012     1\n 3 Statistical Science (AB2)  2013     0\n 4 Statistical Science (AB2)  2014     0\n 5 Statistical Science (AB2)  2015     4\n 6 Statistical Science (AB2)  2016     4\n 7 Statistical Science (AB2)  2017     1\n 8 Statistical Science (AB2)  2018     0\n 9 Statistical Science (AB2)  2019     0\n10 Statistical Science (AB2)  2020     1\n# ℹ 42 more rows\n\n\n\nDemo: In our plot the degree types are BS, BS2, AB, and AB2. This information is in our dataset, in the degree column, but this column also has additional characters we don’t need. Create a new column called degree_type with levels BS, BS2, AB, and AB2 (in this order) based on degree. Do this by adding on to your pipeline from earlier.\n\n\nstatsci |&gt;\n  pivot_longer(\n    cols = -degree,\n    names_to = \"year\",\n    names_transform = as.numeric,\n    values_to = \"n\"\n  ) |&gt;\n  mutate(n = if_else(is.na(n), 0, n)) |&gt;\n  separate(degree, sep = \" \\\\(\", into = c(\"major\", \"degree_type\")) |&gt;\n  mutate(\n    degree_type = str_remove(degree_type, \"\\\\)\"),\n    degree_type = fct_relevel(degree_type, \"BS\", \"BS2\", \"AB\", \"AB2\")\n    )\n\n# A tibble: 52 × 4\n   major               degree_type  year     n\n   &lt;chr&gt;               &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 Statistical Science AB2          2011     0\n 2 Statistical Science AB2          2012     1\n 3 Statistical Science AB2          2013     0\n 4 Statistical Science AB2          2014     0\n 5 Statistical Science AB2          2015     4\n 6 Statistical Science AB2          2016     4\n 7 Statistical Science AB2          2017     1\n 8 Statistical Science AB2          2018     0\n 9 Statistical Science AB2          2019     0\n10 Statistical Science AB2          2020     1\n# ℹ 42 more rows\n\n\n\nYour turn (5 minutes): Now we start making our plot, but let’s not get too fancy right away. Create the following plot, which will serve as the “first draft” on the way to our Goal. Do this by adding on to your pipeline from earlier.\n\n\n\n\n\n\n\nstatsci |&gt;\n  pivot_longer(\n    cols = -degree,\n    names_to = \"year\",\n    names_transform = as.numeric,\n    values_to = \"n\"\n  ) |&gt;\n  mutate(n = if_else(is.na(n), 0, n)) |&gt;\n  separate(degree, sep = \" \\\\(\", into = c(\"major\", \"degree_type\")) |&gt;\n  mutate(\n    degree_type = str_remove(degree_type, \"\\\\)\"),\n    degree_type = fct_relevel(degree_type, \"BS\", \"BS2\", \"AB\", \"AB2\")\n    ) |&gt;\n  ggplot(aes(x = year, y = n, color = degree_type)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\n\nYour turn (4 minutes): What aspects of the plot need to be updated to go from the draft you created above to the Goal plot at the beginning of this application exercise.\n\nx-axis scale: need to go from 2011 to 2023 in increments of 2 years\nline colors\naxis labels: title, subtitle, x, y, caption\ntheme\nlegend position and border\n\nDemo: Update x-axis scale such that the years displayed go from 2011 to 2023 in increments of 2 years. Do this by adding on to your pipeline from earlier.\n\n\nstatsci |&gt;\n  pivot_longer(\n    cols = -degree,\n    names_to = \"year\",\n    names_transform = as.numeric,\n    values_to = \"n\"\n  ) |&gt;\n  mutate(n = if_else(is.na(n), 0, n)) |&gt;\n  separate(degree, sep = \" \\\\(\", into = c(\"major\", \"degree_type\")) |&gt;\n  mutate(\n    degree_type = str_remove(degree_type, \"\\\\)\"),\n    degree_type = fct_relevel(degree_type, \"BS\", \"BS2\", \"AB\", \"AB2\")\n    ) |&gt;\n  ggplot(aes(x = year, y = n, color = degree_type)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(2011, 2023, 2))\n\n\n\n\n\n\n\n\n\nDemo: Update line colors using the following level / color assignments. Once again, do this by adding on to your pipeline from earlier.\n\n“BS” = “cadetblue4”\n“BS2” = “cadetblue3”\n“AB” = “lightgoldenrod4”\n“AB2” = “lightgoldenrod3”\n\n\n\nstatsci |&gt;\n  pivot_longer(\n    cols = -degree,\n    names_to = \"year\",\n    names_transform = as.numeric,\n    values_to = \"n\"\n  ) |&gt;\n  mutate(n = if_else(is.na(n), 0, n)) |&gt;\n  separate(degree, sep = \" \\\\(\", into = c(\"major\", \"degree_type\")) |&gt;\n  mutate(\n    degree_type = str_remove(degree_type, \"\\\\)\"),\n    degree_type = fct_relevel(degree_type, \"BS\", \"BS2\", \"AB\", \"AB2\")\n    ) |&gt;\n  ggplot(aes(x = year, y = n, color = degree_type)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(2011, 2023, 2)) +\n  scale_color_manual(\n    values = c(\"BS\" = \"cadetblue4\", \n               \"BS2\" = \"cadetblue3\", \n               \"AB\" = \"lightgoldenrod4\", \n               \"AB2\" = \"lightgoldenrod3\"))\n\n\n\n\n\n\n\n\n\nYour turn (4 minutes): Update the plot labels (title, subtitle, x, y, and caption) and use theme_minimal(). Once again, do this by adding on to your pipeline from earlier.\n\n\nstatsci |&gt;\n  pivot_longer(\n    cols = -degree,\n    names_to = \"year\",\n    names_transform = as.numeric,\n    values_to = \"n\"\n  ) |&gt;\n  mutate(n = if_else(is.na(n), 0, n)) |&gt;\n  separate(degree, sep = \" \\\\(\", into = c(\"major\", \"degree_type\")) |&gt;\n  mutate(\n    degree_type = str_remove(degree_type, \"\\\\)\"),\n    degree_type = fct_relevel(degree_type, \"BS\", \"BS2\", \"AB\", \"AB2\")\n    ) |&gt;\n  ggplot(aes(x = year, y = n, color = degree_type)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(2011, 2023, 2)) +\n  scale_color_manual(\n    values = c(\"BS\" = \"cadetblue4\",\n               \"BS2\" = \"cadetblue3\",\n               \"AB\" = \"lightgoldenrod4\",\n               \"AB2\" = \"lightgoldenrod3\")) +\n  labs(\n    x = \"Graduation year\",\n    y = \"Number of majors graduating\",\n    color = \"Degree type\",\n    title = \"Statistical Science majors over the years\",\n    subtitle = \"Academic years 2011 - 2023\",\n    caption = \"Source: Office of the University Registrar\\nhttps://registrar.duke.edu/registration/enrollment-statistics\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nDemo: Finally, adding to your pipeline you’ve developed so far, move the legend into the plot, make its background white, and its border gray. Set fig-width: 7 and fig-height: 5 for your plot in the chunk options.\n\n\nstatsci |&gt;\n  pivot_longer(\n    cols = -degree,\n    names_to = \"year\",\n    names_transform = as.numeric,\n    values_to = \"n\"\n  ) |&gt;\n  mutate(n = if_else(is.na(n), 0, n)) |&gt;\n  separate(degree, sep = \" \\\\(\", into = c(\"major\", \"degree_type\")) |&gt;\n  mutate(\n    degree_type = str_remove(degree_type, \"\\\\)\"),\n    degree_type = fct_relevel(degree_type, \"BS\", \"BS2\", \"AB\", \"AB2\")\n    ) |&gt;\n  ggplot(aes(x = year, y = n, color = degree_type)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = seq(2011, 2023, 2)) +\n  scale_color_manual(\n    values = c(\"BS\" = \"cadetblue4\",\n               \"BS2\" = \"cadetblue3\",\n               \"AB\" = \"lightgoldenrod4\",\n               \"AB2\" = \"lightgoldenrod3\")) +\n  labs(\n    x = \"Graduation year\",\n    y = \"Number of majors graduating\",\n    color = \"Degree type\",\n    title = \"Statistical Science majors over the years\",\n    subtitle = \"Academic years 2011 - 2023\",\n    caption = \"Source: Office of the University Registrar\\nhttps://registrar.duke.edu/registration/enrollment-statistics\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = c(0.2, 0.8),\n    legend.background = element_rect(fill = \"white\", color = \"gray\")\n    )"
  },
  {
    "objectID": "labs/Lab_4.html",
    "href": "labs/Lab_4.html",
    "title": "MATH 246",
    "section": "",
    "text": "Coming soon!",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/Lab_3.html",
    "href": "labs/Lab_3.html",
    "title": "Lab 3 - Multiple Linear Regression",
    "section": "",
    "text": "Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, “Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity” by Hamermesh and Parker found that instructors who are viewed to be better looking receive higher instructional ratings. In this lab, you will analyze the data from this study in order to learn what goes into a positive professor evaluation. You will also use the GGally package for visualization relationships between many variables pairs at once and the broom package to tidy your regression outputs. We will start by a brief revisiting of the simple linear regression and then extend it to multiple regression (adding more predictor variables to the model)",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#introduction",
    "href": "labs/Lab_3.html#introduction",
    "title": "Lab 3 - Multiple Linear Regression",
    "section": "",
    "text": "Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, “Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity” by Hamermesh and Parker found that instructors who are viewed to be better looking receive higher instructional ratings. In this lab, you will analyze the data from this study in order to learn what goes into a positive professor evaluation. You will also use the GGally package for visualization relationships between many variables pairs at once and the broom package to tidy your regression outputs. We will start by a brief revisiting of the simple linear regression and then extend it to multiple regression (adding more predictor variables to the model)",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#creating-a-quarto-file",
    "href": "labs/Lab_3.html#creating-a-quarto-file",
    "title": "Lab 3 - Multiple Linear Regression",
    "section": "Creating a quarto file",
    "text": "Creating a quarto file\n\nCreate a new Quarto document with the title Multiple Linear Regression. Change the output format to pdf (note that it is set to html by default). Refer to lab_00_Guide if you don’t remember how to create a new quarto file. Save the file as lab_03.\nNote that if you created the file correctly, it should appear under files with a .qmd extension(i.e., lab_03.qmd). If you do not have this file exactly as described, please STOP and make sure you have it done correctly before you proceed.\nAfter correctly creating the file, click on Render to see the output in pdf format. Note that it may pop up in a new window. This step is just for ensuring that your document created properly and that you are able to generate a pdf, the format in which you will submit the your lab.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#packages",
    "href": "labs/Lab_3.html#packages",
    "title": "Lab 3 - Multiple Linear Regression",
    "section": "Packages",
    "text": "Packages\nYou will need the following packages for this lab: openintro, tidyverse, statsr, GGally, and broom. Recall that we have previously installed all these packages. All we need to do is load them into our workspace. Use code below:\nlibrary(openintro)\nlibrary(tidyverse)\nlibrary(statsr)\nlibrary(broom)\nlibrary(GGally)\nBe sure to run the packages code chunk above to ensure that they are all loaded correctly. Remember to use include=F option (i.e., {r, include=F}) to prevent the code output for loading packages from showing up in your rendered report.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#loading-and-viewing-data",
    "href": "labs/Lab_3.html#loading-and-viewing-data",
    "title": "Lab 3 - Multiple Linear Regression",
    "section": "Loading (and viewing) Data",
    "text": "Loading (and viewing) Data\nWe will use a data frame called evals contained in the openintro package. The data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors’ physical appearance (bty). The result is a data frame where each row contains a different course and columns represent variables about the courses and professors.\nSince we have already activated openintro, we can load the data by running the data command. See below:\n\ndata(evals)\n\nOnce you run the chunk above, you should be able to see a new object called evals in the environment area. Click on it to examine the data.\nYou can learn more about the data by running the following command in the console:\n?evals",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#multiple-linear-regression-modeling",
    "href": "labs/Lab_3.html#multiple-linear-regression-modeling",
    "title": "Lab 3 - Multiple Linear Regression",
    "section": "Multiple Linear Regression Modeling",
    "text": "Multiple Linear Regression Modeling\nLet us start by creating a full model (we can name it m_full) that predicts professor score based on the following predictors: rank, gender, ethnicity, language (language of the university where they got their degree), age, cls_perc_eval (proportion of students that filled out evaluations), cls_students (number of students in class), cls_level (course level), cls_profs (number of professors), cls_credits (number of credits), and bty_avg (average beauty rating). Note that some of these variables are categorical and others are numerical. You can learn more about these variables including how they were measured by checking the documentation (you can run ?evals in the console).\nBefore running the code below, answer the following question:\nHow many variables do you expect to be in the multiple regression model? How do you know?\n\nm_full &lt;- lm(score ~ rank\n                  + gender \n                  + ethnicity \n                  + language \n                  + age \n                  + cls_perc_eval \n                  + cls_students \n                  + cls_level \n                  + cls_profs \n                  + cls_credits \n                  + bty_avg, data = evals)\ntidy(m_full)\n\nQuestions:\n\nHow many variables are in the model? Is this what you expected?\nInterpret the coefficient associated with the ethnicity variable.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#model-prunning",
    "href": "labs/Lab_3.html#model-prunning",
    "title": "Lab 3 - Multiple Linear Regression",
    "section": "Model Prunning",
    "text": "Model Prunning\nThe above model is called a full model because it contains all predictors. The full model is not always the best. We can improve on this model by dropping certain predictors that are not adding value to the model. We are going to use one of the step-wise selection methods (backward elimination) to prune the above model in order to improve it. We are trying to increase the adjusted \\(R^2\\).\nStep 1\nWe will start with the full model (m_full) and its adjusted \\(R^2\\). Run the glance function to check the adjusted \\(R^2\\) of the model above (m_full). Interpret the value in context.\n\nglance(m_full)\n\nThe output shows that the adjusted \\(R^2\\) is \\(14.12\\%\\). We use this number as our baseline and drop predictors one at a time, each time checking on the improvement in the adjusted \\(R^2\\). Our new model will be one that leads to the highest improvement in the adjusted \\(R^2\\).\nLet us start by dropping rank (we can call the model without rank rm_rank (meaning remove rank). We also tidy and glance the model. See the code below:\n\nrm_rank &lt;- lm(score ~ gender \n                    + ethnicity \n                    + language \n                    + age \n                    + cls_perc_eval \n                    + cls_students \n                    + cls_level \n                    + cls_profs \n                    + cls_credits \n                    + bty_avg, data = evals)\n#tidy(rm_rank)\nglance(rm_rank)\n\nDoes dropping rank lead to a significant improvement in the adjusted \\(R^2\\)?\nNext, we want to drop gender. See below:\n\nrm_gender &lt;- lm(score ~ rank \n                      + ethnicity \n                      + language \n                      + age \n                      + cls_perc_eval \n                      + cls_students \n                      + cls_level \n                      + cls_profs \n                      + cls_credits \n                      + bty_avg, data = evals)\n#tidy(rm_gender)\nglance(rm_gender)\n\nComment about the effect of dropping gender from the model? Is it a good idea?\nNext, let us drop ethnicity:\n\nrm_ethnicity &lt;- lm(score ~ rank\n                          + gender \n                          + language \n                          + age \n                          + cls_perc_eval \n                          + cls_students \n                          + cls_level \n                          + cls_profs \n                          + cls_credits \n                          + bty_avg, data = evals)\n#tidy(rm_ethnicity)\nglance(rm_ethnicity)\n\nWhat can you comment about the effect of dropping ethnicity?\nNext, drop language:\n\nrm_language &lt;- lm(score ~ rank\n                        + ethnicity\n                        + gender \n                        + age \n                        + cls_perc_eval \n                        + cls_students \n                        + cls_level \n                        + cls_profs \n                        + cls_credits \n                        + bty_avg, data = evals)\n#tidy(rm_language)\nglance(rm_language)\n\nWhat is the effect of dropping language?\nNext, drop age:\n\nrm_age &lt;- lm(score ~ rank\n                  + ethnicity\n                  + gender\n                  + language\n                  + cls_perc_eval \n                  + cls_students \n                  + cls_level \n                  + cls_profs \n                  + cls_credits \n                  + bty_avg, data = evals)\nglance(rm_age)\n\nComment on the effect of dropping age:\nNext, drop cls_perc_eval (i.e., proportion of students that filled out evaluations):\n\nrm_cls_perc_eval &lt;- lm(score ~ rank\n                              + ethnicity\n                              + age\n                              + gender\n                              + language\n                              + cls_students \n                              + cls_level \n                              + cls_profs \n                              + cls_credits \n                              + bty_avg, data = evals)\nglance(rm_cls_perc_eval)\n\nComment appropriately.\nNext, drop cls_students (class size):\n\nrm_cls_students &lt;- lm(score ~ rank\n                            + ethnicity\n                            + age\n                            + gender\n                            + cls_perc_eval\n                            + language\n                            + cls_level \n                            + cls_profs \n                            + cls_credits \n                            + bty_avg, data = evals)\nglance(rm_cls_students)\n\nComment on the effect of dropping class size.\nNext, drop cls_level (course level):\n\nrm_cls_level &lt;- lm(score ~ rank\n                          + ethnicity\n                          + age\n                          + gender\n                          + cls_students\n                          + cls_perc_eval\n                          + language\n                          + cls_profs \n                          + cls_credits \n                          + bty_avg, data = evals)\nglance(rm_cls_level)\n\nDoes dropping course level lead to an increase in the adjusted \\(R^2\\). if so, by how much?\n\nWe are almost done. Next, drop cls_profs (number of professors):\n\nrm_cls_profs &lt;- lm(score ~ rank\n                          + ethnicity\n                          + age\n                          + gender\n                          + cls_students\n                          + cls_level\n                          + cls_perc_eval\n                          + language\n                          + cls_credits \n                          + bty_avg, data = evals)\nglance(rm_cls_profs)\n\nDoes dropping cls_profs lead to an increase in adjusted \\(R^2\\)? By how much?\n\nNext, drop cls_credits (i.e., number of credits):\n\nrm_cls_credits &lt;- lm(score ~ rank\n                          + ethnicity\n                          + age\n                          + gender\n                          + cls_students\n                          + cls_level\n                          + cls_perc_eval\n                          + language\n                          + cls_profs\n                          + bty_avg, data = evals)\nglance(rm_cls_credits)\n\nComment on the effect of dropping cls_credits.\nFinally, drop bty_avg (i.e., average beauty score):\n\nrm_bty_avg &lt;- lm(score ~ rank\n                        + ethnicity\n                        + age\n                        + gender\n                        + cls_students\n                        + cls_level\n                        + cls_perc_eval\n                        +cls_credits\n                        + language\n                        + cls_profs, data = evals)\nglance(rm_bty_avg)\n\nAs you have seen, dropping class_profs leads to the most improvement to the model. So, we create a new model (call it m_prunned_1) without cls_profs and check its adjusted \\(R^2\\). Below is our new model:\n\nm_prunned_1 &lt;- lm(score ~ rank \n                        + gender \n                        + ethnicity \n                        + language \n                        + age \n                        + cls_perc_eval \n                        + cls_students \n                        + cls_level \n                        + cls_credits \n                        + bty_avg, data = evals)\ntidy(m_prunned_1)\nglance(m_prunned_1)\n\nThe new adjusted \\(R^2\\) is \\(14.3\\%\\) and we will use it in step two:\nStep 2\nWe repeat the process above using m_prunned_1 as the baseline. Remember, its adjusted \\(R^2\\) is \\(14.3\\%\\).\nFirst, drop gender:\n\nrm_gender &lt;- lm(score ~ rank \n                      + ethnicity \n                      + language \n                      + age \n                      + cls_perc_eval \n                      + cls_students \n                      + cls_level \n                      + cls_credits \n                      + bty_avg, data = evals)\n#tidy(rm_gender)\nglance(rm_gender)\n\nNext, remove rank:\n\nrm_rank &lt;- lm(score ~ gender \n                    + ethnicity \n                    + language \n                    + age \n                    + cls_perc_eval \n                    + cls_students \n                    + cls_level \n                    + cls_credits \n                    + bty_avg, data = evals)\n#tidy(rm_rank)\nglance(rm_rank)\n\n\nNext, remove ethnicity,\n\nrm_ethnicity &lt;- lm(score ~ rank \n                        + gender \n                        + language \n                        + age \n                        + cls_perc_eval \n                        + cls_students \n                        + cls_level \n                        + cls_credits \n                        + bty_avg, data = evals)\n#tidy(rm_ethnicity)\nglance(rm_ethnicity)\n\nNext, remove language:\n\nrm_language &lt;- lm(score ~ rank \n                        + gender \n                        + ethnicity \n                        + age \n                        + cls_perc_eval \n                        + cls_students \n                        + cls_level \n                        + cls_credits \n                        + bty_avg, data = evals)\n#tidy(rm_language)\nglance(rm_language)\n\nNo improvement.\nNext, remove age:\n\nrm_age &lt;- lm(score ~ rank \n                  + gender \n                  + ethnicity \n                  + language \n                  + cls_perc_eval \n                  + cls_students \n                  + cls_level \n                  + cls_credits \n                  + bty_avg, data = evals)\n#tidy(rm_age)\nglance(rm_age)\n\nNo improvement.\nNext, remove cls_perc_eval:\n\nrm_cls_per_eval &lt;- lm(score ~ rank \n                            + gender \n                            + ethnicity \n                            + language \n                            + age \n                            + cls_students \n                            + cls_level \n                            + cls_credits \n                            + bty_avg, data = evals)\n#tidy(rm_cls_per_eval)\nglance(rm_cls_per_eval)\n\nNo improvement.\nNext, remove cls_students\n\nrm_cls_st &lt;- lm(score ~ rank \n                      + gender \n                      + ethnicity \n                      + language \n                      + age \n                      + cls_perc_eval \n                      + cls_level \n                      + cls_credits \n                      + bty_avg, data = evals)\n#tidy(rm_cls_st)\nglance(rm_cls_st)\n\nNo improvement.\nNext, remove cls_level\n\nrm_cls_level &lt;- lm(score ~ rank \n                          + gender \n                          + ethnicity \n                          + language \n                          + age \n                          + cls_perc_eval \n                          + cls_students \n                          + cls_credits \n                          + bty_avg, data = evals)\n#tidy(rm_cls_level)\nglance(rm_cls_level)\n\nModel improves:\n\nNext, remove cls_credits:\n\nrm_cls_credits &lt;- lm(score ~ rank \n                          + gender \n                          + ethnicity \n                          + language \n                          + age \n                          + cls_perc_eval \n                          + cls_students \n                          + cls_level \n                          + bty_avg, data = evals)\n#tidy(rm_cls_credits)\nglance(rm_cls_credits)\n\nNo improvement.\nNext, we remove bty_avg:\n\nrm_bty_avg &lt;- lm(score ~ rank \n                      + gender \n                      + ethnicity \n                      + language \n                      + age \n                      + cls_perc_eval \n                      + cls_students \n                      + cls_level \n                      + cls_credits, data = evals)\n#tidy(rm_bty_avg)\nglance(rm_bty_avg)\n\nWe notice that removing cls_level leads to the most improvement in the adjusted \\(R^2\\) of \\(14.47-14.3=.17\\). So, our new improved model (call it m_prunned_2) is:\n\nm_prunned_2 &lt;- lm(score ~ rank \n                        + gender \n                        + ethnicity \n                        + language \n                        + age \n                        + cls_perc_eval \n                        + cls_students \n                        + cls_credits \n                        + bty_avg, data = evals)\n#tidy(rm_prunned_2)\nglance(m_prunned_2)\n\nStep 3\nWe use the model from step 2 above along with its adjusted \\(R^2\\) as the baseline for step 3. The rest of the work is left as an exercise.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#exercises",
    "href": "labs/Lab_3.html#exercises",
    "title": "Lab 3 - Multiple Linear Regression",
    "section": "Exercises",
    "text": "Exercises\n\n(8 pts) In class, we began the backward elimination method (based on adjusted R-squared) for model selection. Complete the steps to come up with the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on.\n(4 pts) Based on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a high evaluation score.\n(4 pts) Pick one slope for a numerical variable in your model and one for a categorical variable and interpret them in context.\n(4 pts) Would you be comfortable generalizing your conclusions to apply to professors generally (at any university)? Why or why not?",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_3.html#submission",
    "href": "labs/Lab_3.html#submission",
    "title": "Lab 3 - Multiple Linear Regression",
    "section": "Submission",
    "text": "Submission\nOnce you are finished with the lab, you will submit your final PDF document to Canvas.\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all questions are numbered appropriately. Delete any code chunks that were not used.\nYou must turn in a PDF file to the Canvas page by the submission deadline to be considered “on time”.\n\n\n\n\n\n\n\n\nChecklist\n\n\n\nMake sure you have:\n\nattempted all questions\nrendered your Quarto document to PDF\nuploaded your PDF to Canvas",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/Lab_0.html",
    "href": "labs/Lab_0.html",
    "title": "Hello, World!",
    "section": "",
    "text": "It is possible that some of you have heard/seen the phrase, Hello, World!, before. That is usually the first thing you learn in programming, i.e., to learn to write a computer program to print this sentence to screen. In this lab, we will not print Hello, World! to the screen. Instead, we will learn how to use R (via RStudio) to perform statistical analyses and to write reports in order to communicate the insights learned from data. While the analysis involves writing computer code, it is in no way going to involve writing computer programs. The goal of today’s lab is to introduce you to R and RStudio. You will learn to access and use RStudio, as well as perform the basic functions regarding statistical analysis. To clarify which is which: R is the name of the programming language itself and RStudio is a convenient interface (Integrated Development Environment or IDE) for working with R. I like to think about R as the car engine and RStudio as a nice driver dashboard. The engine drives the car, but the dashboard makes it easier for the driver to control the car. Most R users work with RStudio.\n\n\nCar Engine and Dashboard\n\nWe will work with the cloud (online) version of RStudio. To access RStudio online, click on the link https://posit.cloud to create an account or to sign in if you already have an account.\n\nAfter signing in to R studio, our next step is to create a new project. You can think of a project as a folder or simply a collection of files. Our project will be called “MATH 246 Fall 2024”. Each lab that you complete will be saved in this project. To create the project, you start by clicking on “New Project” and then change the default name (UNTITLED PROJECT) to “MATH 246 Fall 2024”.\nYay! You now have your project ready. In the next section, we explain the meaning of the various panels on your screen.\n\nYour new R studio project interface will look as follows:\n\n\n\nLeft Panel: The panel on the left is where the action happens. This panel is called the console. Every time you launch RStudio, it will have the same text at the top of the console telling you the version of R that you’re running. Below that information is the symbol ” &gt; “. This is where you enter your commands. When you enter and execute a command, the output will come right below it. These commands and their syntax have evolved over decades (literally) and now provide what many users feel is a fairly natural way to access data, organize, describe, and invoke statistical computations. Try typing 1 + 1 in the console and hit enter.\n\nCode1+1\n\n[1] 2\n\n\n\nUpper Right Panel: The panel in the upper right is called “environment”. It contains, among other things, the history of the actions or commands that you’ve previously entered.\nBottom Right Panel: The panel in the lower right contains tabs for browsing the files in your project folder, access help files for R functions, install and manage R packages, and inspecting visualizations through the viewer tab. By default, all data visualizations you make will appear directly below the code you used to create them. If you would rather your plots appear in the plots tab, you will need to change your global options.\n\nR is an open-source programming language, meaning that users can contribute packages that make our lives easier, and we can use them for free. Packages are simply pre-written code meant to serve specific purposes and may contain other packages inside them. Packages may also contain data sets. Packages are stored in a directory called “Library”. For this lab, and many others in the future, we will use the following two packages:\n\nThe tidyverse package is a very popular “umbrella” package which houses a suite of many different R packages: for data wrangling (including tidying) and data visualization.\nThe openintro package for data and custom functions with the OpenIntro resources. You will notice that the readings frequently refer to data contained in the OpenIntro Package. This is the package.\n\n\nThe command to install a package in R takes the following format:\ninstall.packages(\"package name\")\nTo install tidyverse and openintro, run the following commands:\ninstall.packages(\"tidyverse\")\ninstall.packages(\"openintro\")\nNote: You only need to install packages once, but you need to load them each time you relaunch RStudio. To call load (activate) the above installed packages, you use the following command:\nlibrary(tidyverse)\nlibrary(openintro)\nWhy Tidyverse? We are choosing to use the tidyverse package collection because it consists of a set of packages necessary for different aspects of working with data, anything from loading data to wrangling data to visualizing data to analyzing data. Additionally, these packages share common philosophies and are designed to work together. You can find more about the packages in the tidyverse at tidyverse.org.\n\nSuppose we want to find the mean of the numbers 23,24,26,19,18,25,21, and 39. The first thing you want to do is to get these data into R. We can achieve this by running the code below:\n\nCodex &lt;- c(23,24,26,19,18,25,21, 39) \n# We use the symbol &lt;- for assigning elements to an object. \n\n\nHere, we are creating a vector (a series of numbers) and storing it in an object called x. The symbols &lt;- is used for assignment. In R, we use a # to designate a comment (text that should not be evaluated as code). In above chunk, the text after # is a comment. Comments are a good way to document your code.\nTo find the mean of those numbers, we simply run the command mean(x) as shown below.\n\nCodemean(x)\n\n[1] 24.375\n\n\nThe general format is do_this(on_this). Here, do_this is the function while x is the thing on which we want an action taken.\nTo find the median of the numbers, we run the command median(x) as shown below.\n\nCodemedian(x)\n\n[1] 23.5\n\n\nWe can also create a string object (i.e., a series of non-numerical elements or characters). We use quotes for string characters. See below:\n\nCodey &lt;- c( \"Jane\", \"John\", \"Jess\", \"Jeff\", \"Joe\", \"Holli\", \"Henry\", \"Han\") \n# We use quotes for strings.\n\n\nNow, try to run the command mean(y):\n\nCodemean(y)\n\nWarning in mean.default(y): argument is not numeric or logical: returning NA\n\n\n[1] NA\n\n\nWhat do you get? Programming languages generally produce error messages when you try to perform an inappropriate operation or if there is a mistake in the code. Error messages are a good way to learn what you did wrong. In this case, the mean/average of the object y does not make sense because the entries of y are not numerical.\nYou can, however, perform other operations on y. For example, you may want to know how many Jane entries are in y. To do this, you may simply tabulate the entries in y as shown below:\n\nCodetable(y)\n\ny\n  Han Henry Holli  Jane  Jeff  Jess   Joe  John \n    1     1     1     1     1     1     1     1 \n\n\nWe see that there is only one Jane entry in y. While you could easily count the number of Jane entries in y, the table function is useful when you have many entries and you want to know how many times each entry appears.\nBefore we proceed, delete (or comment out) the mean(y) that you had written earlier.\n\nYou can create a data frame by combining vectors of equal length. Before we do that, let us create two more vectors (a numeric one and a character one).\n\nCodea&lt;-c(3,3,2,2,3,4,1,2)\n\nb&lt;-c(\"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\")\n\n\nYou can combine the vectors a, b, c, x, and y into a data frame as follows. We store this in an object called practice_data and then print it. Note that we use an underscore to separate the words practice and data in the name. Do not use a blank space for object or variable names.\n\nCodepractice_data &lt;- data.frame(name=y, age=x, sex=b, year=a)\nprint (practice_data)\n\n   name age sex year\n1  Jane  23   F    3\n2  John  24   M    3\n3  Jess  26   F    2\n4  Jeff  19   M    2\n5   Joe  18   M    3\n6 Holli  25   F    4\n7 Henry  21   M    1\n8   Han  39   M    2\n\n\nNotice that the data frame looks like a more natural way that you are likely to encounter data. Most of the time, data is collected and stored in excel and can be imported into R for use. Throughout the labs, we will learn how to import data from various sources into R.\n\nWe can run various statistics from data frames. Because the data frame combines many vectors (variables) we need to specify the data frame name and the variable we are targeting. For example, to find the mean of age, we write the data frame name and variable name separated by a dollar sign as follows mean(practice_data$age).\n\nCodemean(practice_data$age)\n\n[1] 24.375\n\n\nYou can also find other statistics. For median, the command is median(data$variable). For standard deviation, the command is sd(data$variable).\nYou can also run multiple summary statistics at once using the command summary(median(data$variable)). See below:\n\nCodesummary(practice_data$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   20.50   23.50   24.38   25.25   39.00 \n\n\nThe summary command give you the minimum value, first quartile, median, mean, third quartile and maximum value.\n\nYou can create basic plots in base R:\n\nTo create a scatter plot to visualize the relationship between age and year, you can use the code below:\n\nCodeplot(practice_data$age, practice_data$year)\n\n\n\n\n\n\n\n\nTo create a bar plot to visualize the distribution of M and F, you can use the command below:\n\nCodebarplot(table((practice_data$sex)))\n\n\n\n\n\n\n\n\n\nCreate an object (name it income) containing the following numerical elements: 750, 810, 680, 1200, 1500, 1399,1525.\nFind the mean of the values in #1 above.\nCreate another object with names of your choosing and then combine it with the first object to make a data frame called my_data.\nRun the summary statistics for the income variable using my_data data frame that you created in #3 above.\nR comes with many pre-loaded data frames. One such data frame is called mtcars. Run the command ?mtcars to learn more about this data frame. Next, load this data frame into your work space by running the command data(mtcars).\nUse the mtcars data frame to find the median horsepower of the cars.\nCreate a histogram to visualize the distribution of the variable hp. What can you say about the distribution of hp based on the histogram?",
    "crumbs": [
      "Labs",
      "Lab 0"
    ]
  },
  {
    "objectID": "labs/Lab_0.html#creating-a-new-project",
    "href": "labs/Lab_0.html#creating-a-new-project",
    "title": "Hello, World!",
    "section": "",
    "text": "After signing in to R studio, our next step is to create a new project. You can think of a project as a folder or simply a collection of files. Our project will be called “MATH 246 Fall 2024”. Each lab that you complete will be saved in this project. To create the project, you start by clicking on “New Project” and then change the default name (UNTITLED PROJECT) to “MATH 246 Fall 2024”.\nYay! You now have your project ready. In the next section, we explain the meaning of the various panels on your screen.",
    "crumbs": [
      "Labs",
      "Lab 0"
    ]
  },
  {
    "objectID": "labs/Lab_0.html#the-r-studio-interface",
    "href": "labs/Lab_0.html#the-r-studio-interface",
    "title": "Hello, World!",
    "section": "",
    "text": "Your new R studio project interface will look as follows:\n\n\n\nLeft Panel: The panel on the left is where the action happens. This panel is called the console. Every time you launch RStudio, it will have the same text at the top of the console telling you the version of R that you’re running. Below that information is the symbol ” &gt; “. This is where you enter your commands. When you enter and execute a command, the output will come right below it. These commands and their syntax have evolved over decades (literally) and now provide what many users feel is a fairly natural way to access data, organize, describe, and invoke statistical computations. Try typing 1 + 1 in the console and hit enter.\n\nCode1+1\n\n[1] 2\n\n\n\nUpper Right Panel: The panel in the upper right is called “environment”. It contains, among other things, the history of the actions or commands that you’ve previously entered.\nBottom Right Panel: The panel in the lower right contains tabs for browsing the files in your project folder, access help files for R functions, install and manage R packages, and inspecting visualizations through the viewer tab. By default, all data visualizations you make will appear directly below the code you used to create them. If you would rather your plots appear in the plots tab, you will need to change your global options.",
    "crumbs": [
      "Labs",
      "Lab 0"
    ]
  },
  {
    "objectID": "labs/Lab_0.html#r-packages",
    "href": "labs/Lab_0.html#r-packages",
    "title": "Hello, World!",
    "section": "",
    "text": "R is an open-source programming language, meaning that users can contribute packages that make our lives easier, and we can use them for free. Packages are simply pre-written code meant to serve specific purposes and may contain other packages inside them. Packages may also contain data sets. Packages are stored in a directory called “Library”. For this lab, and many others in the future, we will use the following two packages:\n\nThe tidyverse package is a very popular “umbrella” package which houses a suite of many different R packages: for data wrangling (including tidying) and data visualization.\nThe openintro package for data and custom functions with the OpenIntro resources. You will notice that the readings frequently refer to data contained in the OpenIntro Package. This is the package.\n\n\nThe command to install a package in R takes the following format:\ninstall.packages(\"package name\")\nTo install tidyverse and openintro, run the following commands:\ninstall.packages(\"tidyverse\")\ninstall.packages(\"openintro\")\nNote: You only need to install packages once, but you need to load them each time you relaunch RStudio. To call load (activate) the above installed packages, you use the following command:\nlibrary(tidyverse)\nlibrary(openintro)\nWhy Tidyverse? We are choosing to use the tidyverse package collection because it consists of a set of packages necessary for different aspects of working with data, anything from loading data to wrangling data to visualizing data to analyzing data. Additionally, these packages share common philosophies and are designed to work together. You can find more about the packages in the tidyverse at tidyverse.org.",
    "crumbs": [
      "Labs",
      "Lab 0"
    ]
  },
  {
    "objectID": "labs/Lab_0.html#creating-vectors-in-r",
    "href": "labs/Lab_0.html#creating-vectors-in-r",
    "title": "Hello, World!",
    "section": "",
    "text": "Suppose we want to find the mean of the numbers 23,24,26,19,18,25,21, and 39. The first thing you want to do is to get these data into R. We can achieve this by running the code below:\n\nCodex &lt;- c(23,24,26,19,18,25,21, 39) \n# We use the symbol &lt;- for assigning elements to an object. \n\n\nHere, we are creating a vector (a series of numbers) and storing it in an object called x. The symbols &lt;- is used for assignment. In R, we use a # to designate a comment (text that should not be evaluated as code). In above chunk, the text after # is a comment. Comments are a good way to document your code.\nTo find the mean of those numbers, we simply run the command mean(x) as shown below.\n\nCodemean(x)\n\n[1] 24.375\n\n\nThe general format is do_this(on_this). Here, do_this is the function while x is the thing on which we want an action taken.\nTo find the median of the numbers, we run the command median(x) as shown below.\n\nCodemedian(x)\n\n[1] 23.5\n\n\nWe can also create a string object (i.e., a series of non-numerical elements or characters). We use quotes for string characters. See below:\n\nCodey &lt;- c( \"Jane\", \"John\", \"Jess\", \"Jeff\", \"Joe\", \"Holli\", \"Henry\", \"Han\") \n# We use quotes for strings.\n\n\nNow, try to run the command mean(y):\n\nCodemean(y)\n\nWarning in mean.default(y): argument is not numeric or logical: returning NA\n\n\n[1] NA\n\n\nWhat do you get? Programming languages generally produce error messages when you try to perform an inappropriate operation or if there is a mistake in the code. Error messages are a good way to learn what you did wrong. In this case, the mean/average of the object y does not make sense because the entries of y are not numerical.\nYou can, however, perform other operations on y. For example, you may want to know how many Jane entries are in y. To do this, you may simply tabulate the entries in y as shown below:\n\nCodetable(y)\n\ny\n  Han Henry Holli  Jane  Jeff  Jess   Joe  John \n    1     1     1     1     1     1     1     1 \n\n\nWe see that there is only one Jane entry in y. While you could easily count the number of Jane entries in y, the table function is useful when you have many entries and you want to know how many times each entry appears.\nBefore we proceed, delete (or comment out) the mean(y) that you had written earlier.",
    "crumbs": [
      "Labs",
      "Lab 0"
    ]
  },
  {
    "objectID": "labs/Lab_0.html#creating-data-frames",
    "href": "labs/Lab_0.html#creating-data-frames",
    "title": "Hello, World!",
    "section": "",
    "text": "You can create a data frame by combining vectors of equal length. Before we do that, let us create two more vectors (a numeric one and a character one).\n\nCodea&lt;-c(3,3,2,2,3,4,1,2)\n\nb&lt;-c(\"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\")\n\n\nYou can combine the vectors a, b, c, x, and y into a data frame as follows. We store this in an object called practice_data and then print it. Note that we use an underscore to separate the words practice and data in the name. Do not use a blank space for object or variable names.\n\nCodepractice_data &lt;- data.frame(name=y, age=x, sex=b, year=a)\nprint (practice_data)\n\n   name age sex year\n1  Jane  23   F    3\n2  John  24   M    3\n3  Jess  26   F    2\n4  Jeff  19   M    2\n5   Joe  18   M    3\n6 Holli  25   F    4\n7 Henry  21   M    1\n8   Han  39   M    2\n\n\nNotice that the data frame looks like a more natural way that you are likely to encounter data. Most of the time, data is collected and stored in excel and can be imported into R for use. Throughout the labs, we will learn how to import data from various sources into R.",
    "crumbs": [
      "Labs",
      "Lab 0"
    ]
  },
  {
    "objectID": "labs/Lab_0.html#using-data-frames",
    "href": "labs/Lab_0.html#using-data-frames",
    "title": "Hello, World!",
    "section": "",
    "text": "We can run various statistics from data frames. Because the data frame combines many vectors (variables) we need to specify the data frame name and the variable we are targeting. For example, to find the mean of age, we write the data frame name and variable name separated by a dollar sign as follows mean(practice_data$age).\n\nCodemean(practice_data$age)\n\n[1] 24.375\n\n\nYou can also find other statistics. For median, the command is median(data$variable). For standard deviation, the command is sd(data$variable).\nYou can also run multiple summary statistics at once using the command summary(median(data$variable)). See below:\n\nCodesummary(practice_data$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   20.50   23.50   24.38   25.25   39.00 \n\n\nThe summary command give you the minimum value, first quartile, median, mean, third quartile and maximum value.",
    "crumbs": [
      "Labs",
      "Lab 0"
    ]
  },
  {
    "objectID": "labs/Lab_0.html#basic-visualizations",
    "href": "labs/Lab_0.html#basic-visualizations",
    "title": "Hello, World!",
    "section": "",
    "text": "You can create basic plots in base R:\n\nTo create a scatter plot to visualize the relationship between age and year, you can use the code below:\n\nCodeplot(practice_data$age, practice_data$year)\n\n\n\n\n\n\n\n\nTo create a bar plot to visualize the distribution of M and F, you can use the command below:\n\nCodebarplot(table((practice_data$sex)))",
    "crumbs": [
      "Labs",
      "Lab 0"
    ]
  },
  {
    "objectID": "labs/Lab_0.html#exercises",
    "href": "labs/Lab_0.html#exercises",
    "title": "Hello, World!",
    "section": "",
    "text": "Create an object (name it income) containing the following numerical elements: 750, 810, 680, 1200, 1500, 1399,1525.\nFind the mean of the values in #1 above.\nCreate another object with names of your choosing and then combine it with the first object to make a data frame called my_data.\nRun the summary statistics for the income variable using my_data data frame that you created in #3 above.\nR comes with many pre-loaded data frames. One such data frame is called mtcars. Run the command ?mtcars to learn more about this data frame. Next, load this data frame into your work space by running the command data(mtcars).\nUse the mtcars data frame to find the median horsepower of the cars.\nCreate a histogram to visualize the distribution of the variable hp. What can you say about the distribution of hp based on the histogram?",
    "crumbs": [
      "Labs",
      "Lab 0"
    ]
  },
  {
    "objectID": "labs/lab-3.html",
    "href": "labs/lab-3.html",
    "title": "Lab 3 - Data tidying and joining",
    "section": "",
    "text": "In this lab you’ll build the data wrangling and visualization skills you’ve developed so far and data tidying and joining to your repertoire.\n\n\n\n\n\n\nNote\n\n\n\nThis lab assumes you’ve completed the labs so far and doesn’t repeat setup and overview content from those labs. If you have not yet done those, you should go back and review the previous labs before starting on this one.\n\n\n\nBy the end of the lab, you will…\n\nBe able to pivot/reshape data using tidyr\n\nContinue developing your data wrangling skills using dplyr\n\nBuild on your mastery of data visualizations using ggplot2\n\nGet more experience with data science workflow using R, RStudio, Git, and GitHub\nFurther your reproducible authoring skills with Quarto\nImprove your familiarity with version control using Git and GitHub\n\nLog in to RStudio, clone your lab-3 repo from GitHub, open your lab-3.qmd document, and get started!\n\n\n\n\n\n\nClick here if you prefer to see step-by-step instructions\n\n\n\n\n\n\n\nGo to https://cmgr.oit.duke.edu/containers and log in with your Duke NetID and Password.\nClick STA198-199 under My reservations to log into your container. You should now see the RStudio environment.\n\n\nGo to the course organization at github.com/sta199-s24 organization on GitHub. Click on the repo with the prefix lab-3. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ➛ New Project ➛Version Control ➛ Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-3.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab.\n\nIn lab-3.qmd, update the author field to your name, render your document and examine the changes. Then, in the Git pane, click on Diff to view your changes, add a commit message (e.g., “Added author name”), and click Commit. Then, push the changes to your GitHub repository, and in your browser confirm that these changes have indeed propagated to your repository.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you run into any issues with the first steps outlined above, flag a TA for help before proceeding.\n\n\n\nIn this lab we will work with the tidyverse package, which is a collection of packages for doing data analysis in a “tidy” way.\n\nlibrary(tidyverse)\n\nRender the document which loads this package with the library() function.\n\nAs we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\nIn addition, the code should all the code should be be able to be read (not run off the page) when you render to PDF. Make sure that is the case, and add line breaks where the code is running off the page.1\n\n\n\n\n\n\nNote\n\n\n\nContinuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to render, commit, and push your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment."
  },
  {
    "objectID": "labs/lab-3.html#learning-objectives",
    "href": "labs/lab-3.html#learning-objectives",
    "title": "Lab 3 - Data tidying and joining",
    "section": "",
    "text": "By the end of the lab, you will…\n\nBe able to pivot/reshape data using tidyr\n\nContinue developing your data wrangling skills using dplyr\n\nBuild on your mastery of data visualizations using ggplot2\n\nGet more experience with data science workflow using R, RStudio, Git, and GitHub\nFurther your reproducible authoring skills with Quarto\nImprove your familiarity with version control using Git and GitHub"
  },
  {
    "objectID": "labs/lab-3.html#getting-started",
    "href": "labs/lab-3.html#getting-started",
    "title": "Lab 3 - Data tidying and joining",
    "section": "",
    "text": "Log in to RStudio, clone your lab-3 repo from GitHub, open your lab-3.qmd document, and get started!\n\n\n\n\n\n\nClick here if you prefer to see step-by-step instructions\n\n\n\n\n\n\n\nGo to https://cmgr.oit.duke.edu/containers and log in with your Duke NetID and Password.\nClick STA198-199 under My reservations to log into your container. You should now see the RStudio environment.\n\n\nGo to the course organization at github.com/sta199-s24 organization on GitHub. Click on the repo with the prefix lab-3. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ➛ New Project ➛Version Control ➛ Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-3.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab.\n\nIn lab-3.qmd, update the author field to your name, render your document and examine the changes. Then, in the Git pane, click on Diff to view your changes, add a commit message (e.g., “Added author name”), and click Commit. Then, push the changes to your GitHub repository, and in your browser confirm that these changes have indeed propagated to your repository.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you run into any issues with the first steps outlined above, flag a TA for help before proceeding."
  },
  {
    "objectID": "labs/lab-3.html#packages",
    "href": "labs/lab-3.html#packages",
    "title": "Lab 3 - Data tidying and joining",
    "section": "",
    "text": "In this lab we will work with the tidyverse package, which is a collection of packages for doing data analysis in a “tidy” way.\n\nlibrary(tidyverse)\n\nRender the document which loads this package with the library() function."
  },
  {
    "objectID": "labs/lab-3.html#guidelines",
    "href": "labs/lab-3.html#guidelines",
    "title": "Lab 3 - Data tidying and joining",
    "section": "",
    "text": "As we’ve discussed in lecture, your plots should include an informative title, axes should be labeled, and careful consideration should be given to aesthetic choices.\nIn addition, the code should all the code should be be able to be read (not run off the page) when you render to PDF. Make sure that is the case, and add line breaks where the code is running off the page.1\n\n\n\n\n\n\nNote\n\n\n\nContinuing to develop a sound workflow for reproducible data analysis is important as you complete the lab and other assignments in this course. There will be periodic reminders in this assignment to remind you to render, commit, and push your changes to GitHub. You should have at least 3 commits with meaningful commit messages by the end of the assignment."
  },
  {
    "objectID": "labs/lab-3.html#part-1",
    "href": "labs/lab-3.html#part-1",
    "title": "Lab 3 - Data tidying and joining",
    "section": "Part 1",
    "text": "Part 1\nInflation across the world\nFor this part of the analysis you will work with inflation data from various countries in the world over the last 30 years.\n\ncountry_inflation &lt;- read_csv(\"data/country-inflation.csv\")\n\nQuestion 1\nGet to know the data.\n\nglimpse() at the country_inflation data frame and answer the following questions based on the output. How many rows does country_inflation have and what does each row represent? How many columns does country_inflation have and what does each column represent?\nDisplay a list of the countries included in the dataset.\n\n\n\n\n\n\n\nTip\n\n\n\nA function that can be useful for part (b) is pull(). Check out its documentation for examples of usage.\n\n\nQuestion 2\nWhich countries had the top three highest inflation rates in 2021? Your output should be a data frame with two columns, country and 2021, with inflation rates in descending order, and three rows for the top three countries. Briefly comment on how the inflation rates for these countries compare to the inflation rate for United States in that year.\n\n\n\n\n\n\nTip\n\n\n\nColumn names that are numbers are not considered “proper” in R, therefore to select them you’ll need to surround them with backticks, e.g. select( ` 1993 ` ).\n\n\nQuestion 3\nIn a single pipeline,\n\ncalculate the ratio of the inflation in 2021 and inflation in 1993 for each country and store this information in a new column called inf_ratio,\narrange the data frame in decreasing order of inf_ratio, and\nselect the variables country and inf_ratio to display as the result of the pipeline.\n\nDo not save this new variable in inf_ratio, only calculate and display it so you can answer the following question based on the output of the pipeline.\nWhich country’s inflation change is the largest over this time period? Did inflation increase of decrease between 1993 and 2021 in this country?\n\n\n\n\n\n\nTip\n\n\n\nFor this question you’ll once again need to use variables whose names are numbers (years) in your pipeline. Make sure to surround the names of such variables with backticks (`).\n\n\nQuestion 4\nReshape (pivot) country_inflation such that each row represents a country/year combination, with columns country, year, and annual_inflation. Then, display the resulting data frame and state how many rows and columns it has.\nRequirements:\n\nYour code must use one of pivot_longer() or pivot_wider(). There are other ways you can do this reshaping move in R, but this question requires solving this problem by pivoting.\nIn your pivot_*() function, you must use names_transform = as.numeric as an argument to transform the variable type to numeric as you pivot the data so that in the resulting data frame the year variable is numeric.\nThe resulting data frame must be saved as something other than country_inflation so you (1) can refer to this data frame later in your analysis and (2) do not overwrite country_inflation. Use a short but informative name.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe remaining questions in Part 1 require the use of the pivoted data frame from Question 4.\n\n\nQuestion 5\nUse a separate, single pipeline to answer each of the following questions.\nRequirement: Your code must use the filter() function for each part, not arrange().\n\nWhat is the highest inflation rate observed between 1993 and 2021? The output of the pipeline should be a data frame with one row and three columns. In addition to code and output, your response should include a single sentence stating the country and year.\nWhat is the lowest inflation rate observed between 1993 and 2021? The output of the pipeline should be a data frame with one row and three columns. In addition to code and output, your response should include a single sentence stating the country and year.\nPutting (a) and (b) together: What are the highest and the lowest inflation rates observed between 1993 and 2021? The output of the pipeline should be a data frame with two rows and three columns.\nQuestion 6\na. Create a vector called countries_of_interest which contains the names of up tp five countries you want to visualize the inflation rates for over the years. For example, if these countries are Türkiye and United States, you can express this as follows:\n\ncountries_of_interest &lt;- c(\"Türkiye\", \"United States\")\n\nIf they are Türkiye, United States, and China, you can express this as follows:\n\ncountries_of_interest &lt;- c(\n  \"Türkiye\", \"United States\", \"China (People's Republic of)\"\n)\n\nSo on and so forth… Then, in 1-2 sentences, state why you chose these countries.\n\n\n\n\n\n\nNote\n\n\n\nYour countries_of_interest should consist of no more than five countries. Make sure that the spelling of your countries matches how they appear in the dataset.\n\n\nb. In a single pipeline, filter your reshaped dataset to include only the countries_of_interest from part (a), and save the resulting data frame with a new name so you (1) can refer to this data frame later in your analysis and (2) do not overwrite the data frame you’re starting with. Use a short but informative name. Then, in a new pipeline, find the distinct() countries in the data frame you created.\n\n\n\n\n\n\nTip\n\n\n\nThe number of distinct countries in the filtered data frame you created in part (b) should equal the number of countries you chose in part (a). If it doesn’t, you might have misspelled a country name or made a mistake in how to filter for these countries. Go back and check your code.\n\n\nQuestion 7\nUsing your data frame from the previous question, create a plot of annual inflation vs. year for these countries. Then, in a few sentences, describe the patterns you observe in the plot, particularly focusing on anything you find surprising or not surprising, based on your knowledge (or lack thereof) of these countries economies.\nRequirements for the plot:\n\nData should be represented with points as well as lines connecting the points for each country.\nEach country should be represented by a different color line and different color and shape points.\nAxes and legend should be properly labeled.\nThe plot should have an appropriate title (and optionally a subtitle).\nPlot should be customized in at least one way – you could use a different than default color scale, or different than default theme, or some other customization.\n\n\nIf you haven’t yet done so, now is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "labs/lab-3.html#part-2",
    "href": "labs/lab-3.html#part-2",
    "title": "Lab 3 - Data tidying and joining",
    "section": "Part 2",
    "text": "Part 2\nInflation in the US\nThe OECD defines inflation as follows:\n\nInflation is a rise in the general level of prices of goods and services that households acquire for the purpose of consumption in an economy over a period of time.\nThe main measure of inflation is the annual inflation rate which is the movement of the Consumer Price Index (CPI) from one month/period to the same month/period of the previous year expressed as percentage over time.\nSource: OECD CPI FAQ\n\nCPI is broken down into 12 divisions such as food, housing, health, etc. Your goal in this part is to create another time series plot of annual inflation, this time for US only.\nThe data you will need to create this visualization is spread across two files:\n\n\nus-inflation.csv: Annual inflation rate for the US for 12 CPI divisions. Each division is identified by an ID number.\n\ncpi-divisions.csv: A “lookup table” of CPI division ID numbers and their descriptions.\n\nLet’s load both of these files.\n\nus_inflation &lt;- read_csv(\"data/us-inflation.csv\")\ncpi_divisions &lt;- read_csv(\"data/cpi-divisions.csv\")\n\nQuestion 8\na. How many columns and how many rows does the us_inflation dataset have? What are the variables in it? Add a brief (1-2 sentences) narrative summarizing this information.\nb. How many columns and how many rows does the cpi_divisions dataset have? What are the variables in it? Add a brief (1-2 sentences) narrative summarizing this information.\nc. Create a new dataset by joining the us_inflation dataset with the cpi_division_id dataset.\n\nDetermine which type of join is the most appropriate one and use that.\nNote that the two datasets don’t have a common variable. Review the help for the join functions to determine how to use the by argument when the names of the variables that the datasets should be joined by are different.\nUse a short but informative name for the joined dataset, and do not overwrite either of the datasets that go into creating it.\n\nThen, find the number of rows and columns of the resulting dataset and report the names of its columns. Add a brief (1-2 sentences) narrative summarizing this information.\nQuestion 9\na. Create a vector called divisions_of_interest which contains the descriptions or IDs of CPI divisions you want to visualize. Your divisions_of_interest should consist of no more than five divisions. If you’re using descriptions, make sure that the spelling of your divisions matches how they appear in the dataset. Then, in 1-2 sentences, state why you chose these divisions.\n\n\n\n\n\n\nTip\n\n\n\nRefer back to the guidance provided in Question 6 if you’re not sure how to create this vector.\n\n\nb. In a single pipeline, filter your reshaped dataset to include only the divisions_of_interest from part (a), and save the resulting data frame with a new name so you (1) can refer to this data frame later in your analysis and (2) do not overwrite the data frame you’re starting with. Use a short but informative name. Then, in a new pipeline, find the distinct() divisions in the data frame you created.\nQuestion 10\nUsing your data frame from the previous question, create a plot of annual inflation vs. year for these divisions. Then, in a few sentences, describe the patterns you observe in the plot, particularly focusing on anything you find surprising or not surprising, based on your knowledge (or lack thereof) of inflation rates in the US over the last decade.\n\nData should be represented with points as well as lines connecting the points for each division.\nEach division should be represented by a different color line and different color and shape points.\nAxes and legend should be properly labeled.\nThe plot should have an appropriate title (and optionally a subtitle).\nPlot should be customized in at least one way – you could use a different than default color scale, or different than default theme, or some other customization.\nIf your legend has labels that are too long, you can try moving the legend to the bottom and stack the labels vertically. Hint: The legend.position and legend.direction arguments of the theme() functions will be useful.\n\n\nggplot(...) +\n  ... +\n  theme(\n    legend.position = \"bottom\", \n    legend.direction = \"vertical\"\n  )\n\n\nIf you haven’t yet done so since Part 1, now is a good time to render, commit, and push. Make sure that you commit and push all changed documents and your Git pane is completely empty before proceeding."
  },
  {
    "objectID": "labs/lab-3.html#submission",
    "href": "labs/lab-3.html#submission",
    "title": "Lab 3 - Data tidying and joining",
    "section": "Submission",
    "text": "Submission\nOnce you are finished with the lab, you will submit your final PDF document to Gradescope.\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all of your documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nYou must turn in a PDF file to the Gradescope page by the submission deadline to be considered “on time”.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials \\(\\rightarrow\\) Duke NetID and log in using your NetID credentials.\nClick on your STA 199 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark all the pages associated with question. All the pages of your lab should be associated with at least one question (i.e., should be “checked”).\n\n\n\n\n\n\n\nChecklist\n\n\n\nMake sure you have:\n\nattempted all questions\nrendered your Quarto document\ncommitted and pushed everything to your GitHub repository such that the Git pane in RStudio is empty\nuploaded your PDF to Gradescope\nselected pages associated with each question on Gradescope"
  },
  {
    "objectID": "labs/lab-3.html#grading",
    "href": "labs/lab-3.html#grading",
    "title": "Lab 3 - Data tidying and joining",
    "section": "Grading",
    "text": "Grading\nThe lab is graded out of a total of 50 points.\nYou can earn up to 5 points on each question:\n\n5: Response shows excellent understanding and addresses all or almost all of the rubric items.\n4: Response shows good understanding and addresses most of the rubric items.\n3: Response shows understanding and addresses a majority of the rubric items.\n2: Response shows effort and misses many of the rubric items.\n1: Response does not show sufficient effort or understanding and/or is largely incomplete.\n0: No attempt."
  },
  {
    "objectID": "labs/lab-3.html#footnotes",
    "href": "labs/lab-3.html#footnotes",
    "title": "Lab 3 - Data tidying and joining",
    "section": "Footnotes",
    "text": "Footnotes\n\nRemember, haikus not novellas when writing code!↩︎"
  },
  {
    "objectID": "labs/Lab_1.html",
    "href": "labs/Lab_1.html",
    "title": "Reproducible Reports",
    "section": "",
    "text": "In today’s lab, we introduce an interface known as quarto that is helpful in creating reproducible reports. One of the most powerful features of quarto is the fact that you can write code (in code chunks) and plain text in the same document. You can generate (render) your work into formats such as pdf, word, html, etc that can be shared easily. If you are using the desktop version of quarto, you need to download quarto from https://quarto.org/docs/get-started/ before you proceed.\n\nWe are going to need a package called palmerpenguins. Install it before you proceed. To install the package, run the following command in the console\ninstall.packages(\"palmerpenguins)\nAn alternative way to install a package in RStudio is to use the install button in the packages tab.\n\nTo create your First Quarto file, follow the following steps:\nGo to File&gt;New File &gt; Quarto document. See below:\n\nAfter clicking Quarto document,a pop up window will appear with fields for the title and author. Enter the title of the document as Introducing Quarto and Tidyverse because that is what we are doing today. Write your name under author. The output format can stay as HTML. You can always change these options even after creating the document. The popup window looks as follows:\n\nClick create to create the document. Note that the document appears with the name Untitled. Click on file then navigate to save then change the name from untitled to Lab_01. Remember, we do not want to use a space for the document name. To tell whether your document saved properly, you will see the document under files with a .qmd extension. With this document saved here, you can always return to it any time and continue working.\nNow, click on Render to see the output of the document you just created. You will note that the document has both plain text and code chunks. We shall use the code chunks for writing code and plain text for interpreting our analyses and writing reports. To add your own code chunk, just click on code the go to add new chunk or use the keyboard shortcut cmd+opt+I on a mac and control+opt+I on windows. When doing this, make sure your cursor is at the place where you want to create the new code chunk.\nBy default, the code chunk that comes is for R code. If you want to write python code, just change the r to python.\nTo create a new code chunk, you can copy an already existing code chunk and paste it elsewhere on the document then edit it appropriately. You may also create new code chunks by clicking on Code and navigating to Insert code chunk.\n\nIn a code chunk, you write code that you want to reproduce in your report. There are other operations such as installing packages that should be done only in the console. When you render a quarto document, quarto runs all code chunks sequentially from top to bottom in order to output your report (in pdf, html, or word). If any code chunks has code for package installation, it means R will try to re-install the package every time you render the document (we said packages are installed one). Operations such as activating packages, i.e., library(package name), should be included in the first code chunk.\nAs a start, let us load the openintro, tidyverse, and palmerpenguins packages. Copy and paste the following code in the first code chunk:\nlibrary(openintro)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nRun the code chunk with the packages to ensure that they are all working. If any of them is missing, R will prompt you to install them.\n\nRemember that the openintro package contains the data that comes with the openintro text. To view a complete list of the data frames, visit https://www.openintro.org/data/. Today, we will use a data frame called penguins contained in the palmerpenguins package. To learn more about this data frame, run the following code in the console:\n?penguins\nTo load the data into your document, run the following command:\n\nCodedata(\"penguins\")\n\n\nWhen you run the above code, a new object should appear in the environment area. Click on penguins to view and study the data.\n\nNow that you have “imported” the data into R, we want to perform a few analyses. We are going to use the tidyverse package to do this. The tidyverse is a collection of R packages for data analysis that are developed with common ideas and norms. According to @Wickham2019,\n“At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next.”\n\nIn lab_00, we performed the analyses using base R code. We saw that to compute the mean of a given variable in a data frame, you use data_frame_name$variable_name. The tidyverse workflow is a bit different and is what we shall use for the most part.\nFor example, to compute the mean of bill_length_mm from the penguins data frame, base R code would be\nmean(penguins$bill_length_mm)\nTry the code to see if it works. If not, what is the problem and how would you fix it?\nIn the tidyverse, the code for the mean of the bill length would be\npenguins %&gt;% summarize(length_mean = mean(bill_length_mm))\nThe symbol %&gt;% is called a pipe and is very common in tidyverse. It takes anything on its left and sends it (pipes it) to the function on the right. Here, we are taking the mtcars data frame and piping it into the summarize function (the function for summary statistics). Inside the function, we specify the variable and the statistic (in this case the mean). We have chosen to name the result as length_mean but this could be changed.\nYou can compute other summary statistics in a similar manner as above and if there is an NA one way to deal with would be to remove it. In some cases, one would replace NA with the average of the other values.\n\nIn this section, we will start by creating scatter plots and then proceed to bar plots. You will learn about more visualization tools in later labs.\n\nSuppose we want to answer the following question: Do penguins with longer flippers weigh more or less than penguins with shorter flippers?\nWe will use a package called ggplot2 (part of tidyverse) to create a scatter plot to visualize the association between flipper length and and body weight.\nggplot2 is a package (part of the tidyverse umbrella) that is used to create nice-looking graphics. It adopts the grammar of graphics, which is a coherent system for describing and building graphs.\nYou provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to sequentially until you get the desired plot. We will do this in steps:\n\nStep 1: Add the data:\n\n\nCode  ggplot(data = penguins)\n\n\n\n\n\n\n\nThis creates an empty canvas\n\nStep 2: Provide the information about the x-axis and the y-axis (i.e., what variable do you want where?). We call this a mapping.\n\n\nCodeggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)  \n\n\n\n\n\n\n\nThe above code adds to our empty canvas the variables specified for the y axis and the x-axis. We still do not have a scatter plot.\n\nStep 3: Define the geometry (geom) that you want ggplot2 to use. In our case, we want to use points and so we use the function geom_point(). Notice that you must have the parentheses because this is a function that can take arguments (you will learn more about this). Use teh code below:\n\n\nCodeggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nWe finally have our scatter plot. Based on this scatter plot, what can you say about the question we sought to answer?\nRemember the steps:\n\nGive ggplot the data,\nprovide a mapping, and\ndefine a geometry using geom_.\n\nQuestion: Try to change the geometry above to a histogram, i.e.,\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_histogram()\nDoes it work? If not, why?\n\nThe scatter plot above is a great tool to visualize the relationship between the two variables (flipper length and weight). Suppose we want to add a third (categorical) variable such as species to the scatter plot. We can achieve this by adding a third argument such as color to the aesthetics). See code below:\n\nCodeggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color=species )\n) +\n  geom_point()\n\n\n\n\n\n\n\nWhat do you learn from this new scatter plot that you do not from the first?\nYou can also visualize how the different species are scattered on the same scale by using the face_wrap function. See code below\n\nCodeggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()+\n  facet_wrap(~species)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nYou can also add a numerical variable such as bill_depth_mm. See code below:\n\nCodeggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color=bill_depth_mm )\n) +\n  geom_point()\n\n\n\n\n\n\n\nWhat do you learn from this scatter plot that you do not from the first?\n\nYou can also create bar plots in a similar manner as above. Remember that bar plots are for categorical variables. For example, we can use a bar plot to visualize the species of the penguins. See code below:\n\nCodeggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\n\n\n\n\n\nThe above bar plot simply gives us the count for each species. If you wanted to color the bars by species, you can\n\nCodeggplot(penguins, aes(x = species, color=species)) +\n  geom_bar()\n\n\n\n\n\n\n\nYou can make the colors to fill the bars by using fill instead of color. See below:\n\nCodeggplot(penguins, aes(x = species, fill=species)) +\n  geom_bar()\n\n\n\n\n\n\n\nIf you wanted to add another categorical variable such as island to visualize the relationship, you can fill by island nstead of by species. See below:\n\nCodeggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar()\n\n\n\n\n\n\n\nThe problem with this bar plot is that it is difficult to interpret. We often prefer to have side-by-side bar plots. To achieve this, we add an argument called position inside the geom_bar function and set it to dodge.\n\nCodeggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n(2 pts) There is a data frame called loan50 that is contained in the openintro package. load the data frame into your work space. \n\n\n(2 pts) Give a brief description of this data frame. How many variables does the data frame have? How many cases/observations does it have? \n\n\n(4 pts) Do people that have been employed for long tend to get lower interest rates? To answer this question, create a scatter plot for the variables emp_length and interest_rate. \n\n\n(4 pts) Recreate the following plot using the loan50 data frame.\n\n\n\n\n\n\n\n\n\n\n(4 pts) Create a bar plot (with differently colored bars) to visualize the distribution of the loans by homeownership. What insights can you draw from this bar plot? \n(4 pts) Recreate the following plot using the loan50 data frame.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#introduction",
    "href": "labs/Lab_1.html#introduction",
    "title": "Reproducible Reports",
    "section": "",
    "text": "In today’s lab, we introduce an interface known as quarto that is helpful in creating reproducible reports. One of the most powerful features of quarto is the fact that you can write code (in code chunks) and plain text in the same document. You can generate (render) your work into formats such as pdf, word, html, etc that can be shared easily. If you are using the desktop version of quarto, you need to download quarto from https://quarto.org/docs/get-started/ before you proceed.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#installing-a-new-package",
    "href": "labs/Lab_1.html#installing-a-new-package",
    "title": "Reproducible Reports",
    "section": "",
    "text": "We are going to need a package called palmerpenguins. Install it before you proceed. To install the package, run the following command in the console\ninstall.packages(\"palmerpenguins)\nAn alternative way to install a package in RStudio is to use the install button in the packages tab.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#creating-a-quarto-file",
    "href": "labs/Lab_1.html#creating-a-quarto-file",
    "title": "Reproducible Reports",
    "section": "",
    "text": "To create your First Quarto file, follow the following steps:\nGo to File&gt;New File &gt; Quarto document. See below:\n\nAfter clicking Quarto document,a pop up window will appear with fields for the title and author. Enter the title of the document as Introducing Quarto and Tidyverse because that is what we are doing today. Write your name under author. The output format can stay as HTML. You can always change these options even after creating the document. The popup window looks as follows:\n\nClick create to create the document. Note that the document appears with the name Untitled. Click on file then navigate to save then change the name from untitled to Lab_01. Remember, we do not want to use a space for the document name. To tell whether your document saved properly, you will see the document under files with a .qmd extension. With this document saved here, you can always return to it any time and continue working.\nNow, click on Render to see the output of the document you just created. You will note that the document has both plain text and code chunks. We shall use the code chunks for writing code and plain text for interpreting our analyses and writing reports. To add your own code chunk, just click on code the go to add new chunk or use the keyboard shortcut cmd+opt+I on a mac and control+opt+I on windows. When doing this, make sure your cursor is at the place where you want to create the new code chunk.\nBy default, the code chunk that comes is for R code. If you want to write python code, just change the r to python.\nTo create a new code chunk, you can copy an already existing code chunk and paste it elsewhere on the document then edit it appropriately. You may also create new code chunks by clicking on Code and navigating to Insert code chunk.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#using-code-chunks",
    "href": "labs/Lab_1.html#using-code-chunks",
    "title": "Reproducible Reports",
    "section": "",
    "text": "In a code chunk, you write code that you want to reproduce in your report. There are other operations such as installing packages that should be done only in the console. When you render a quarto document, quarto runs all code chunks sequentially from top to bottom in order to output your report (in pdf, html, or word). If any code chunks has code for package installation, it means R will try to re-install the package every time you render the document (we said packages are installed one). Operations such as activating packages, i.e., library(package name), should be included in the first code chunk.\nAs a start, let us load the openintro, tidyverse, and palmerpenguins packages. Copy and paste the following code in the first code chunk:\nlibrary(openintro)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nRun the code chunk with the packages to ensure that they are all working. If any of them is missing, R will prompt you to install them.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#loading-and-viewing-data",
    "href": "labs/Lab_1.html#loading-and-viewing-data",
    "title": "Reproducible Reports",
    "section": "",
    "text": "Remember that the openintro package contains the data that comes with the openintro text. To view a complete list of the data frames, visit https://www.openintro.org/data/. Today, we will use a data frame called penguins contained in the palmerpenguins package. To learn more about this data frame, run the following code in the console:\n?penguins\nTo load the data into your document, run the following command:\n\nCodedata(\"penguins\")\n\n\nWhen you run the above code, a new object should appear in the environment area. Click on penguins to view and study the data.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#some-analyses-with-tidyverse",
    "href": "labs/Lab_1.html#some-analyses-with-tidyverse",
    "title": "Reproducible Reports",
    "section": "",
    "text": "Now that you have “imported” the data into R, we want to perform a few analyses. We are going to use the tidyverse package to do this. The tidyverse is a collection of R packages for data analysis that are developed with common ideas and norms. According to @Wickham2019,\n“At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next.”\n\nIn lab_00, we performed the analyses using base R code. We saw that to compute the mean of a given variable in a data frame, you use data_frame_name$variable_name. The tidyverse workflow is a bit different and is what we shall use for the most part.\nFor example, to compute the mean of bill_length_mm from the penguins data frame, base R code would be\nmean(penguins$bill_length_mm)\nTry the code to see if it works. If not, what is the problem and how would you fix it?\nIn the tidyverse, the code for the mean of the bill length would be\npenguins %&gt;% summarize(length_mean = mean(bill_length_mm))\nThe symbol %&gt;% is called a pipe and is very common in tidyverse. It takes anything on its left and sends it (pipes it) to the function on the right. Here, we are taking the mtcars data frame and piping it into the summarize function (the function for summary statistics). Inside the function, we specify the variable and the statistic (in this case the mean). We have chosen to name the result as length_mean but this could be changed.\nYou can compute other summary statistics in a similar manner as above and if there is an NA one way to deal with would be to remove it. In some cases, one would replace NA with the average of the other values.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#visualizing-with-ggplot2",
    "href": "labs/Lab_1.html#visualizing-with-ggplot2",
    "title": "Reproducible Reports",
    "section": "",
    "text": "In this section, we will start by creating scatter plots and then proceed to bar plots. You will learn about more visualization tools in later labs.\n\nSuppose we want to answer the following question: Do penguins with longer flippers weigh more or less than penguins with shorter flippers?\nWe will use a package called ggplot2 (part of tidyverse) to create a scatter plot to visualize the association between flipper length and and body weight.\nggplot2 is a package (part of the tidyverse umbrella) that is used to create nice-looking graphics. It adopts the grammar of graphics, which is a coherent system for describing and building graphs.\nYou provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to sequentially until you get the desired plot. We will do this in steps:\n\nStep 1: Add the data:\n\n\nCode  ggplot(data = penguins)\n\n\n\n\n\n\n\nThis creates an empty canvas\n\nStep 2: Provide the information about the x-axis and the y-axis (i.e., what variable do you want where?). We call this a mapping.\n\n\nCodeggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)  \n\n\n\n\n\n\n\nThe above code adds to our empty canvas the variables specified for the y axis and the x-axis. We still do not have a scatter plot.\n\nStep 3: Define the geometry (geom) that you want ggplot2 to use. In our case, we want to use points and so we use the function geom_point(). Notice that you must have the parentheses because this is a function that can take arguments (you will learn more about this). Use teh code below:\n\n\nCodeggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nWe finally have our scatter plot. Based on this scatter plot, what can you say about the question we sought to answer?\nRemember the steps:\n\nGive ggplot the data,\nprovide a mapping, and\ndefine a geometry using geom_.\n\nQuestion: Try to change the geometry above to a histogram, i.e.,\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_histogram()\nDoes it work? If not, why?\n\nThe scatter plot above is a great tool to visualize the relationship between the two variables (flipper length and weight). Suppose we want to add a third (categorical) variable such as species to the scatter plot. We can achieve this by adding a third argument such as color to the aesthetics). See code below:\n\nCodeggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color=species )\n) +\n  geom_point()\n\n\n\n\n\n\n\nWhat do you learn from this new scatter plot that you do not from the first?\nYou can also visualize how the different species are scattered on the same scale by using the face_wrap function. See code below\n\nCodeggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()+\n  facet_wrap(~species)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nYou can also add a numerical variable such as bill_depth_mm. See code below:\n\nCodeggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color=bill_depth_mm )\n) +\n  geom_point()\n\n\n\n\n\n\n\nWhat do you learn from this scatter plot that you do not from the first?\n\nYou can also create bar plots in a similar manner as above. Remember that bar plots are for categorical variables. For example, we can use a bar plot to visualize the species of the penguins. See code below:\n\nCodeggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\n\n\n\n\n\nThe above bar plot simply gives us the count for each species. If you wanted to color the bars by species, you can\n\nCodeggplot(penguins, aes(x = species, color=species)) +\n  geom_bar()\n\n\n\n\n\n\n\nYou can make the colors to fill the bars by using fill instead of color. See below:\n\nCodeggplot(penguins, aes(x = species, fill=species)) +\n  geom_bar()\n\n\n\n\n\n\n\nIf you wanted to add another categorical variable such as island to visualize the relationship, you can fill by island nstead of by species. See below:\n\nCodeggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar()\n\n\n\n\n\n\n\nThe problem with this bar plot is that it is difficult to interpret. We often prefer to have side-by-side bar plots. To achieve this, we add an argument called position inside the geom_bar function and set it to dodge.\n\nCodeggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar(position = \"dodge\")",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_1.html#exercises",
    "href": "labs/Lab_1.html#exercises",
    "title": "Reproducible Reports",
    "section": "",
    "text": "(2 pts) There is a data frame called loan50 that is contained in the openintro package. load the data frame into your work space. \n\n\n(2 pts) Give a brief description of this data frame. How many variables does the data frame have? How many cases/observations does it have? \n\n\n(4 pts) Do people that have been employed for long tend to get lower interest rates? To answer this question, create a scatter plot for the variables emp_length and interest_rate. \n\n\n(4 pts) Recreate the following plot using the loan50 data frame.\n\n\n\n\n\n\n\n\n\n\n(4 pts) Create a bar plot (with differently colored bars) to visualize the distribution of the loans by homeownership. What insights can you draw from this bar plot? \n(4 pts) Recreate the following plot using the loan50 data frame.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/Lab_2.html",
    "href": "labs/Lab_2.html",
    "title": "Data Wrangling & Simple Regression",
    "section": "",
    "text": "In this lab, we’ll be analyzing data from the Human Freedom Index (HFI) reports. The data frame is contained within the openintro package that we have already installed. We will start by doing some basic data transformations (data wrangling), then proceed to summarize a few of the relationships within the data both graphically and numerically in order to find which variables can help tell a story about freedom.\n\n\nCreate a new Quarto document with the title Data Transformations and Linear Regression. Change the output format to pdf (note that it is set to html by default). Refer to lab_00_Guide if you don’t remember how to create a new quarto file. Save the file as lab_02.\nNote that if you created the file correctly, it should appear under files with a .qmd extension(i.e., lab_02.qmd). If you do not have this file exactly as described, please stop and make sure you have it done correctly before you proceed.\nAfter correctly creating the file, click on Render to see the output in pdf format. Note that it may pop up in a new window. You may want to delete the content that comes with the quarto template before you proceed.\n\nIn this lab, you will need four packages: openintro, tidyverse, statsr, and broom. Recall that we have installed the first two but not the last two. So, we start by installing statsr and broom. Run the following code in the console. Note that we are installing multiple packages on one step:\ninstall.packages(\"statsr\", \"broom\")\nAfter installing the two packages, open a code chunk within your quarto document and load all packages needed using the following code:\nlibrary(openintro)\nlibrary(tidyverse)\nlibrary(statsr)\nlibrary(broom)\nRun the code chunk with the packages to ensure that they are all working correctly. Remember to disable the library code output in your rendered pdf from quarto. You may use include=FALSE option.\n\nAs indicated earlier, the data we will work with is called hfi (human freedom index) and is part of the openintro package. All we need to do is create a code chunk and load the data into our quarto file using the following command:\n\nCodedata(hfi)\n\n\nOnce you run the above command, you should be able to see a new object called hfi in the environment area. Click on it to examine the data.\nYou can learn more about the data by running the following command in the console:\n?hfi\nQuestion: How many variables and how many observations are the hfi dataframe?\n\nIt is often claimed that data scientists spend nearly 80 percent of their time getting their data into an easily usable format. This process is often referred to as data wrangling and is quite broad. We are going to learn about a few basic functions that you can use to perform basic data transformations. These include:\n\nselect\nmutate\nfilter\narrange\ngroup_by\ndrop_na\n\n\nThe select function is used to select specific variables (columns) from a larger data set.\nAs you should have noticed, the hfi data frame is very big. Suppose you are only interested in the variables region, pf_expression_control and pf_score. We can use the select function to create a new data frame that has only these three variables. It if often advisable to have a new name for you new data frame. If you use the same name as the original, it will overwrite the original data frame. I have chosen to use the name hfi_new. Run the code below:\n\nCodehfi_new &lt;- hfi %&gt;% \nselect (region,pf_expression_control , pf_score)\n# Pipe hfi into the select function and specify the variables of interest.\n\n\nNote:\n\nBe sure to take a moment to make sure you understand what the code is doing.\nAfter running the above code, a new data frame (hfi_new) should appear in the environment area. Click to make sure it is what you expect to see.\n\nThe mutate function is used for creating a new variable by altering (mutating) existing variable(s) in some way. Suppose, for example, let us say you wanted to convert the expression control variable (pf_expression_control) to percentages. We know that the expression control is measured out of 10. So we can divide each score by 10 and multiply by 100. Use the code below. We will name the new data frame as hfi_new_1:\n\nCodehfi_new_2 &lt;- hfi_new %&gt;% \nmutate(pf_exp_cont_percent= (pf_expression_control/10)*100)\n# The new data frame will now have 4 variables.\n\n\nNote - The new data frame (hfi_new_2) now has 4 variables. - If you wanted to delete the original variable, you can use transmute instead of mutate. - The above example assumes that variable being mutated is numerical, otherwise, the operation may not be possible. How do you mutate a categorical variable?\n\nThe operations we have considered so far focus on columns (variables). We can also work on cases (rows). The filter function is used to choose rows that meet certain characteristics that you may be interested in. Suppose, for example, that we are interested in countries from Eastern Europe only. Let us create a new data frame (call it hfi_new_3) from hfi_new_2. Use the code below:\n\nCodehfi_new_3 &lt;- hfi_new_2 %&gt;% \nfilter (region == \"Eastern Europe\")\n\n\nNote - As expected, the new data frame has only 198 cases (rows). - The number of columns (variables) stays the same as in hfi_new2.\nIf you wanted countries from Eastern Europe OR from Western Europe. To do this, you separate the regions using a | as shown below:\n\nCodehfi_new_4 &lt;- hfi_new_2 %&gt;% \nfilter (region == \"Eastern Europe\"| region== \"Western Europe\")\n\n\n\nGroup_by is a special kind of filtering that is commonly used alongside a summarize function. Suppose you want to compute the average freedom score (pf_score) by region. You can achieve this by piping the data into a group_by function that will group the data according to your wish and then call the summarize function. Run the code below:\n\nCodehfi_new_4 %&gt;% \n  group_by (region) %&gt;%\n  summarize(mean(pf_score))\n\n\nYou may have noticed that the code did not return a value for Eastern Europe. This is because there are some missing values in the data for Eastern Europe. To fix this, you can add one more step in the workflow that will drop the NA values from the pf_score variable. See below:\n\nCodehfi_new_4 %&gt;%\n  drop_na(pf_score)%&gt;% # This is the new step\n  group_by (region) %&gt;%\n  summarize(mean(pf_score))\n\n\n\n\nAs can be seen from the hfi documentation (obtained by running ?hfi in the console), the variable pf_score measures the personal freedom score of a country. The higher the score a country has on pf_score, the more the personal freedoms enjoyed by its people. On the other hand, the variable pf_expression_control measures the extent to which countries place political pressure and controls on the media content. A low score on pf_expression_control means higher controls and political pressure.\nWhat is the relationship between personal freedom score (pf_score) and the personal freedom expression control (pf_expression_contro l) for Western and Eastern European countries? To answer this question, we can create a scatter plot to visualize the relationship. We will use the hfi_new_4 data frame. Use the code below:\n\nCodeggplot(data=hfi_new_4, mapping=aes(x=pf_expression_control, y=pf_score))+\n  geom_point()\n\n\n\n\n\n\n\nNote:\n\nFrom the above scatter plot, we see an increasing (positive) linear trend where countries with low pressure on media (i.e., high score on pf_expression_control) tend to have higher personal freedoms (high scores on pf_score) and vice versa. Furthermore, the relationship appears to be fairly strong with a few suspected outliers on the lower side.\nIn your code above, change the geom_point to geom_jitter. What do you notice?\n\nThe correlation coefficient gives us a numerical measure of the association between two (numerical) variables. It tells us the strength and direction of the association. See the Tidyverse workflow below for doing this:\n\nCodehfi_new_4 %&gt;%\n  drop_na(pf_score, pf_expression_control)%&gt;%\n  summarize(cor(pf_score,pf_expression_control))\n# Notice we first dropped missing values from the two variables\n\n\nYou can also use base R (non-Tidyverse) code as follows but this is generally difficult to read/decode.\n\nCodecor(hfi_new_4$pf_score, hfi_new_4$pf_expression_control, use=\"complete.obs\")\n# Notice we first dropped missing values from the two variables\n\n\nIt is often helpful to generate both scatter plots and correlation coefficient in order to get a complete picture of the relationship.\n\nSuppose we want to use the pf_expression_control variable (predictor) to estimate the values of personal freedom score (pf_score). We can do this by fitting a least squares regression line. We use a function called lm (for linear model) as follows:\n\nCodemodel &lt;- lm(pf_score ~ pf_expression_control, data =hfi_new_4)\n\n\nNote:\n\nThe variable that comes after the ~ is ALWAYS the predictor variable (or x-variable).\nAfter setting the relationship, we must specify the data set. Here we use hfi_new_4.\nWe saved this model as an object called model because we will need it later. This means we won’t have to write the code all over again when we need it.\nTo see the model output, you can click on the object named model in the environment area and identify the slope and intercept for your model.\n\nIt is often useful to visualize the line of best fit. You can achieve this by running the following ggplot commands:\n\nCodeggplot(data =hfi_new_4, aes(x =pf_expression_control, y =pf_score))+ \n  geom_jitter() + \n  geom_smooth(method =\"lm\", se=FALSE, color=\"red\")\n\n\n\n\n\n\n\n\nThe model output seen earlier contains too much information which makes it hard to find what you need. In most cases, we are interested in the intercept, the slope, and perhaps the p-values (for statistical inference). To get a cleaner looking model output, we use the tidy function (contained in broom package. Create a code chunk and run the code below:\n\nCodetidy(model)\n\n# A tibble: 2 × 5\n  term                  estimate std.error statistic   p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)              6.13     0.0791      77.5 1.85e-222\n2 pf_expression_control    0.354    0.0108      32.9 3.44e-109\n\n\nNote:\n\nIf you had not saved the object model you would have to write the code all over again and use that code in place of model in the tidy function above.\nThe formula for the model is \\[\\widehat{ef\\_score}=6.131+0.354 \\times pf\\_expression\\_control\\]\n\n\nInterpretation:\n\nIntercept: On average, the estimated freedom score (ef_score) for a countries with absolute political pressure and control on the media (i.e., pf_expression_control of 0) would be 6.131.\nSlope: Every time we reduce the political control on the media (pf_expression_control) by one unit, the personal freedom score (pf_score) increases by 0.354 on average.\n\nFinally, we can assess model fit using a statistic called coefficient of determination (aka R-squared). This statistic gives the proportion of variability in the response variable that is explained by the explanatory variable. We use the glance() function for this. Use the code below:\n\nCodeglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.756         0.755 0.359     1083. 3.44e-109     1  -138.  283.  294.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nNote:\n\nThe \\(R^2\\) value is .7558. This means that up to 75.58% of variability in the personal freedom scores for the countries can be accounted for by the model with pf_expression_control as the predictor.\n\nWe can also run a regression analysis using a categorical variable. Suppose we want to pf_score by country. We want to focus on Western vs Eastern Europe so we will use hfi_new_4 data frame. Before we create anything, we may want to visualize the relationship by way of a scatter plot. You can use the geom_point or the geom_jitter to shake them up a little.\n\nCodeggplot(data=hfi_new_4, aes(x=region, y=pf_score))+\n  geom_point()\n\n\n\n\n\n\nCode# Try switching geom_point to geom_jitter and see what happens.\n\n\n\nBased on the scatter plot, how would you describe the relationship between region and pf_score?\n\nWhen you have a categorical variable, you may create side-by-side box plots. These are great in comparing the distribution of the numerical variable variable by category. Use the code below:\n\nCodeggplot(data=hfi_new_4, aes(x=region, y=pf_score))+\n  geom_boxplot()\n\n\n\n\n\n\n\n\nDescribe the relationship based on the side-by-side box plots.\n\nTo fit a linear model, we use the same code as in the case of two numerical variables. We know that region is the predictor and `pf_score is the outcome. Go region comes after the ~ sign. See code below:\n\nCodemodel_2 &lt;- lm(pf_score ~ region, data =hfi_new_4)\n\n\nYou can then tidy the model using the code:\n\nCodetidy(model_2)\n\n# A tibble: 2 × 5\n  term                 estimate std.error statistic  p.value\n  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)             8.24     0.0412     200.  0       \n2 regionWestern Europe    0.910    0.0607      15.0 1.50e-39\n\n\nThe formula for the model can be written as: \\[\\widehat{ef\\_score}=8.24+0.91 \\times region\\_WesternEurope\\]\nQuestion:\n\nWhy do we have region_WesternEurope in the model instead of the expected variable name (i.e., region)?\nInterpret the slope and the intercept in context.\n\nYou may want to create a new quarto file for the exercises. This way, it will be easy to keep track of what you are doing and avoid confusion with the work done during class. For this lab you need the same packages used in class (i.e., openintro, tidyverse, statsr, and broom).\n\n(2 pts) Create a new data frame (name it new_hfi) with the variables region, ef_regulation, ef_regulation_business and ef_score. You will need this data frame in the next exercises.\n(2 pts) Create a new data frame (name it new_hfi_1) from new_hfi. The data frame new_hfi_1 should have countries from South Asia or East Asia only. How many cases are in this new data frame?\n(2 pts) Use the group_by function to compute the average ef_score for counties in East Asia and those in South Asia.\n(4 pts) Reproduce the following scatter plot using the new_hfi_1 data frame. What can you say about the relationship between ef_score and ef_regulation for countries in East Asia South Asia?\n\n\n\n\n\n\n\n\n\n\n(2 pts) Compute the correlation coefficient for the variables in exercise 4 above. Comment about the strength of the relationship.\n(6 pts) Suppose we want to create a model for estimating the economic freedom score (ef_score) of countries in East or South Asia based on their business regulation (ef_regulation_business). Create a scatter plot to assess the relationship between the variables. Given the scatter plot, do you think it is appropriate to proceed with linear regression modeling? If yes, create the model and interpret its parameters(i.e., the slope and the intercept) in context. Use your model to estimate the economic freedom score of a country with a business regulation score of 3.9.\n(2 pts) Find the R-squared value for the model in exercise 6 and explain its meaning in context.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#introduction",
    "href": "labs/Lab_2.html#introduction",
    "title": "Data Wrangling & Simple Regression",
    "section": "",
    "text": "In this lab, we’ll be analyzing data from the Human Freedom Index (HFI) reports. The data frame is contained within the openintro package that we have already installed. We will start by doing some basic data transformations (data wrangling), then proceed to summarize a few of the relationships within the data both graphically and numerically in order to find which variables can help tell a story about freedom.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#creating-a-quarto-file",
    "href": "labs/Lab_2.html#creating-a-quarto-file",
    "title": "Data Wrangling & Simple Regression",
    "section": "",
    "text": "Create a new Quarto document with the title Data Transformations and Linear Regression. Change the output format to pdf (note that it is set to html by default). Refer to lab_00_Guide if you don’t remember how to create a new quarto file. Save the file as lab_02.\nNote that if you created the file correctly, it should appear under files with a .qmd extension(i.e., lab_02.qmd). If you do not have this file exactly as described, please stop and make sure you have it done correctly before you proceed.\nAfter correctly creating the file, click on Render to see the output in pdf format. Note that it may pop up in a new window. You may want to delete the content that comes with the quarto template before you proceed.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#packages",
    "href": "labs/Lab_2.html#packages",
    "title": "Data Wrangling & Simple Regression",
    "section": "",
    "text": "In this lab, you will need four packages: openintro, tidyverse, statsr, and broom. Recall that we have installed the first two but not the last two. So, we start by installing statsr and broom. Run the following code in the console. Note that we are installing multiple packages on one step:\ninstall.packages(\"statsr\", \"broom\")\nAfter installing the two packages, open a code chunk within your quarto document and load all packages needed using the following code:\nlibrary(openintro)\nlibrary(tidyverse)\nlibrary(statsr)\nlibrary(broom)\nRun the code chunk with the packages to ensure that they are all working correctly. Remember to disable the library code output in your rendered pdf from quarto. You may use include=FALSE option.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#loading-and-viewing-data",
    "href": "labs/Lab_2.html#loading-and-viewing-data",
    "title": "Data Wrangling & Simple Regression",
    "section": "",
    "text": "As indicated earlier, the data we will work with is called hfi (human freedom index) and is part of the openintro package. All we need to do is create a code chunk and load the data into our quarto file using the following command:\n\nCodedata(hfi)\n\n\nOnce you run the above command, you should be able to see a new object called hfi in the environment area. Click on it to examine the data.\nYou can learn more about the data by running the following command in the console:\n?hfi\nQuestion: How many variables and how many observations are the hfi dataframe?",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#data-transfromations",
    "href": "labs/Lab_2.html#data-transfromations",
    "title": "Data Wrangling & Simple Regression",
    "section": "",
    "text": "It is often claimed that data scientists spend nearly 80 percent of their time getting their data into an easily usable format. This process is often referred to as data wrangling and is quite broad. We are going to learn about a few basic functions that you can use to perform basic data transformations. These include:\n\nselect\nmutate\nfilter\narrange\ngroup_by\ndrop_na\n\n\nThe select function is used to select specific variables (columns) from a larger data set.\nAs you should have noticed, the hfi data frame is very big. Suppose you are only interested in the variables region, pf_expression_control and pf_score. We can use the select function to create a new data frame that has only these three variables. It if often advisable to have a new name for you new data frame. If you use the same name as the original, it will overwrite the original data frame. I have chosen to use the name hfi_new. Run the code below:\n\nCodehfi_new &lt;- hfi %&gt;% \nselect (region,pf_expression_control , pf_score)\n# Pipe hfi into the select function and specify the variables of interest.\n\n\nNote:\n\nBe sure to take a moment to make sure you understand what the code is doing.\nAfter running the above code, a new data frame (hfi_new) should appear in the environment area. Click to make sure it is what you expect to see.\n\nThe mutate function is used for creating a new variable by altering (mutating) existing variable(s) in some way. Suppose, for example, let us say you wanted to convert the expression control variable (pf_expression_control) to percentages. We know that the expression control is measured out of 10. So we can divide each score by 10 and multiply by 100. Use the code below. We will name the new data frame as hfi_new_1:\n\nCodehfi_new_2 &lt;- hfi_new %&gt;% \nmutate(pf_exp_cont_percent= (pf_expression_control/10)*100)\n# The new data frame will now have 4 variables.\n\n\nNote - The new data frame (hfi_new_2) now has 4 variables. - If you wanted to delete the original variable, you can use transmute instead of mutate. - The above example assumes that variable being mutated is numerical, otherwise, the operation may not be possible. How do you mutate a categorical variable?\n\nThe operations we have considered so far focus on columns (variables). We can also work on cases (rows). The filter function is used to choose rows that meet certain characteristics that you may be interested in. Suppose, for example, that we are interested in countries from Eastern Europe only. Let us create a new data frame (call it hfi_new_3) from hfi_new_2. Use the code below:\n\nCodehfi_new_3 &lt;- hfi_new_2 %&gt;% \nfilter (region == \"Eastern Europe\")\n\n\nNote - As expected, the new data frame has only 198 cases (rows). - The number of columns (variables) stays the same as in hfi_new2.\nIf you wanted countries from Eastern Europe OR from Western Europe. To do this, you separate the regions using a | as shown below:\n\nCodehfi_new_4 &lt;- hfi_new_2 %&gt;% \nfilter (region == \"Eastern Europe\"| region== \"Western Europe\")\n\n\n\nGroup_by is a special kind of filtering that is commonly used alongside a summarize function. Suppose you want to compute the average freedom score (pf_score) by region. You can achieve this by piping the data into a group_by function that will group the data according to your wish and then call the summarize function. Run the code below:\n\nCodehfi_new_4 %&gt;% \n  group_by (region) %&gt;%\n  summarize(mean(pf_score))\n\n\nYou may have noticed that the code did not return a value for Eastern Europe. This is because there are some missing values in the data for Eastern Europe. To fix this, you can add one more step in the workflow that will drop the NA values from the pf_score variable. See below:\n\nCodehfi_new_4 %&gt;%\n  drop_na(pf_score)%&gt;% # This is the new step\n  group_by (region) %&gt;%\n  summarize(mean(pf_score))",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#regression-modeling",
    "href": "labs/Lab_2.html#regression-modeling",
    "title": "Data Wrangling & Simple Regression",
    "section": "",
    "text": "As can be seen from the hfi documentation (obtained by running ?hfi in the console), the variable pf_score measures the personal freedom score of a country. The higher the score a country has on pf_score, the more the personal freedoms enjoyed by its people. On the other hand, the variable pf_expression_control measures the extent to which countries place political pressure and controls on the media content. A low score on pf_expression_control means higher controls and political pressure.\nWhat is the relationship between personal freedom score (pf_score) and the personal freedom expression control (pf_expression_contro l) for Western and Eastern European countries? To answer this question, we can create a scatter plot to visualize the relationship. We will use the hfi_new_4 data frame. Use the code below:\n\nCodeggplot(data=hfi_new_4, mapping=aes(x=pf_expression_control, y=pf_score))+\n  geom_point()\n\n\n\n\n\n\n\nNote:\n\nFrom the above scatter plot, we see an increasing (positive) linear trend where countries with low pressure on media (i.e., high score on pf_expression_control) tend to have higher personal freedoms (high scores on pf_score) and vice versa. Furthermore, the relationship appears to be fairly strong with a few suspected outliers on the lower side.\nIn your code above, change the geom_point to geom_jitter. What do you notice?\n\nThe correlation coefficient gives us a numerical measure of the association between two (numerical) variables. It tells us the strength and direction of the association. See the Tidyverse workflow below for doing this:\n\nCodehfi_new_4 %&gt;%\n  drop_na(pf_score, pf_expression_control)%&gt;%\n  summarize(cor(pf_score,pf_expression_control))\n# Notice we first dropped missing values from the two variables\n\n\nYou can also use base R (non-Tidyverse) code as follows but this is generally difficult to read/decode.\n\nCodecor(hfi_new_4$pf_score, hfi_new_4$pf_expression_control, use=\"complete.obs\")\n# Notice we first dropped missing values from the two variables\n\n\nIt is often helpful to generate both scatter plots and correlation coefficient in order to get a complete picture of the relationship.\n\nSuppose we want to use the pf_expression_control variable (predictor) to estimate the values of personal freedom score (pf_score). We can do this by fitting a least squares regression line. We use a function called lm (for linear model) as follows:\n\nCodemodel &lt;- lm(pf_score ~ pf_expression_control, data =hfi_new_4)\n\n\nNote:\n\nThe variable that comes after the ~ is ALWAYS the predictor variable (or x-variable).\nAfter setting the relationship, we must specify the data set. Here we use hfi_new_4.\nWe saved this model as an object called model because we will need it later. This means we won’t have to write the code all over again when we need it.\nTo see the model output, you can click on the object named model in the environment area and identify the slope and intercept for your model.\n\nIt is often useful to visualize the line of best fit. You can achieve this by running the following ggplot commands:\n\nCodeggplot(data =hfi_new_4, aes(x =pf_expression_control, y =pf_score))+ \n  geom_jitter() + \n  geom_smooth(method =\"lm\", se=FALSE, color=\"red\")\n\n\n\n\n\n\n\n\nThe model output seen earlier contains too much information which makes it hard to find what you need. In most cases, we are interested in the intercept, the slope, and perhaps the p-values (for statistical inference). To get a cleaner looking model output, we use the tidy function (contained in broom package. Create a code chunk and run the code below:\n\nCodetidy(model)\n\n# A tibble: 2 × 5\n  term                  estimate std.error statistic   p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)              6.13     0.0791      77.5 1.85e-222\n2 pf_expression_control    0.354    0.0108      32.9 3.44e-109\n\n\nNote:\n\nIf you had not saved the object model you would have to write the code all over again and use that code in place of model in the tidy function above.\nThe formula for the model is \\[\\widehat{ef\\_score}=6.131+0.354 \\times pf\\_expression\\_control\\]\n\n\nInterpretation:\n\nIntercept: On average, the estimated freedom score (ef_score) for a countries with absolute political pressure and control on the media (i.e., pf_expression_control of 0) would be 6.131.\nSlope: Every time we reduce the political control on the media (pf_expression_control) by one unit, the personal freedom score (pf_score) increases by 0.354 on average.\n\nFinally, we can assess model fit using a statistic called coefficient of determination (aka R-squared). This statistic gives the proportion of variability in the response variable that is explained by the explanatory variable. We use the glance() function for this. Use the code below:\n\nCodeglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.756         0.755 0.359     1083. 3.44e-109     1  -138.  283.  294.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nNote:\n\nThe \\(R^2\\) value is .7558. This means that up to 75.58% of variability in the personal freedom scores for the countries can be accounted for by the model with pf_expression_control as the predictor.\n\nWe can also run a regression analysis using a categorical variable. Suppose we want to pf_score by country. We want to focus on Western vs Eastern Europe so we will use hfi_new_4 data frame. Before we create anything, we may want to visualize the relationship by way of a scatter plot. You can use the geom_point or the geom_jitter to shake them up a little.\n\nCodeggplot(data=hfi_new_4, aes(x=region, y=pf_score))+\n  geom_point()\n\n\n\n\n\n\nCode# Try switching geom_point to geom_jitter and see what happens.\n\n\n\nBased on the scatter plot, how would you describe the relationship between region and pf_score?\n\nWhen you have a categorical variable, you may create side-by-side box plots. These are great in comparing the distribution of the numerical variable variable by category. Use the code below:\n\nCodeggplot(data=hfi_new_4, aes(x=region, y=pf_score))+\n  geom_boxplot()\n\n\n\n\n\n\n\n\nDescribe the relationship based on the side-by-side box plots.\n\nTo fit a linear model, we use the same code as in the case of two numerical variables. We know that region is the predictor and `pf_score is the outcome. Go region comes after the ~ sign. See code below:\n\nCodemodel_2 &lt;- lm(pf_score ~ region, data =hfi_new_4)\n\n\nYou can then tidy the model using the code:\n\nCodetidy(model_2)\n\n# A tibble: 2 × 5\n  term                 estimate std.error statistic  p.value\n  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)             8.24     0.0412     200.  0       \n2 regionWestern Europe    0.910    0.0607      15.0 1.50e-39\n\n\nThe formula for the model can be written as: \\[\\widehat{ef\\_score}=8.24+0.91 \\times region\\_WesternEurope\\]\nQuestion:\n\nWhy do we have region_WesternEurope in the model instead of the expected variable name (i.e., region)?\nInterpret the slope and the intercept in context.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/Lab_2.html#exercises",
    "href": "labs/Lab_2.html#exercises",
    "title": "Data Wrangling & Simple Regression",
    "section": "",
    "text": "You may want to create a new quarto file for the exercises. This way, it will be easy to keep track of what you are doing and avoid confusion with the work done during class. For this lab you need the same packages used in class (i.e., openintro, tidyverse, statsr, and broom).\n\n(2 pts) Create a new data frame (name it new_hfi) with the variables region, ef_regulation, ef_regulation_business and ef_score. You will need this data frame in the next exercises.\n(2 pts) Create a new data frame (name it new_hfi_1) from new_hfi. The data frame new_hfi_1 should have countries from South Asia or East Asia only. How many cases are in this new data frame?\n(2 pts) Use the group_by function to compute the average ef_score for counties in East Asia and those in South Asia.\n(4 pts) Reproduce the following scatter plot using the new_hfi_1 data frame. What can you say about the relationship between ef_score and ef_regulation for countries in East Asia South Asia?\n\n\n\n\n\n\n\n\n\n\n(2 pts) Compute the correlation coefficient for the variables in exercise 4 above. Comment about the strength of the relationship.\n(6 pts) Suppose we want to create a model for estimating the economic freedom score (ef_score) of countries in East or South Asia based on their business regulation (ef_regulation_business). Create a scatter plot to assess the relationship between the variables. Given the scatter plot, do you think it is appropriate to proceed with linear regression modeling? If yes, create the model and interpret its parameters(i.e., the slope and the intercept) in context. Use your model to estimate the economic freedom score of a country with a business regulation score of 3.9.\n(2 pts) Find the R-squared value for the model in exercise 6 and explain its meaning in context.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio\n🔗 on Posit Cloud\n\n\n\nGitHub\n🔗 on github.com\n\n\n\nOpen hours\n🔗 M & F: 12-1 (walk in to Will 311E)\n🔗 More on Calendly (by appt.)\n\n\n\nDiscussion forum\n🔗 on Canvas\n\n\n\nGradebook\n🔗 on Canvas\n\n\n\nTexbooks\n🔗 Introduction to Modern Statistics\n🔗 R for Data Science\n🔗 Intermediate Statistics\n\n\n\nPackage documentation\n🔗 openintro: openintro.org\n🔗 ggplot2: ggplot2.tidyverse.org\n🔗 dplyr: dplyr.tidyverse.org\n🔗 tidyr: tidyr.tidyverse.org\n🔗 stringr: stringr.tidyverse.org\n🔗 lubridate: lubridate.tidyverse.org\n🔗 readr: readr.tidyverse.org\n🔗 readxl: readxl.tidyverse.org",
    "crumbs": [
      "Course information",
      "Useful links"
    ]
  },
  {
    "objectID": "ae/ae-10-modeling-fish-A.html",
    "href": "ae/ae-10-modeling-fish-A.html",
    "title": "AE 10: Modelling fish",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key.\n\n\nFor this application exercise, we will work with data on fish. The dataset we will use, called fish, is on two common fish species in fish market sales.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nfish &lt;- read_csv(\"data/fish.csv\")\n\nThe data dictionary is below:\n\n\n\nvariable\ndescription\n\n\n\n\nspecies\nSpecies name of fish\n\n\nweight\nWeight, in grams\n\n\nlength_vertical\nVertical length, in cm\n\n\nlength_diagonal\nDiagonal length, in cm\n\n\nlength_cross\nCross length, in cm\n\n\nheight\nHeight, in cm\n\n\nwidth\nDiagonal width, in cm\n\n\n\n\nVisualizing the model\nWe’re going to investigate the relationship between the weights and heights of fish.\n\nDemo: Create an appropriate plot to investigate this relationship. Add appropriate labels to the plot.\n\n\nggplot(fish, aes(x = height, y = weight)) +\n  geom_point() +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n\n\n\n\n\n\n\n\nYour turn (5 minutes):\n\nIf you were to draw a a straight line to best represent the relationship between the heights and weights of fish, where would it go? Why?\nStart from the bottom and go up Identify the first and last point and draw a line through most the others.\nNow, let R draw the line for you. Refer to the documentation at https://ggplot2.tidyverse.org/reference/geom_smooth.html. Specifically, refer to the method section.\n\n\nggplot(fish, aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Weights vs. lengths of fish\",\n    x = \"Head-to-tail lentgh (cm)\",\n    y = \"Weight of fish (grams)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nWhat types of questions can this plot help answer?\nIs there a relationship between fish heights and weights of fish?\n\nYour turn (3 minutes):\n\nWe can use this line to make predictions. Predict what you think the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm. Which prediction is considered extrapolation?\nAt 10 cm, we estimate a weight of 375 grams. At 15 cm, we estimate a weight of 600 grams At 20 cm, we estimate a weight of 975 grams. 20 cm would be considered extrapolation.\nWhat is a residual?\nDifference between predicted and observed.\n\n\n\n\nModel fitting\n\nDemo: Fit a model to predict fish weights from their heights.\n\n\nfish_hw_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ height, data = fish)\n\nfish_hw_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = weight ~ height, data = data)\n\nCoefficients:\n(Intercept)       height  \n    -288.42        60.92  \n\n\n\nYour turn (3 minutes): Predict what the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm using this model.\n\n\nx &lt;- c(10, 15, 20)\n-288 + 60.92 * x\n\n[1] 321.2 625.8 930.4\n\n\n\nDemo: Calculate predicted weights for all fish in the data and visualize the residuals under this model.\n\n\nfish_hw_aug &lt;- augment(fish_hw_fit, new_data = fish)\n\nggplot(fish_hw_aug, aes(x = height, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +  \n  geom_segment(aes(xend = height, yend = .pred), color = \"gray\") +  \n  geom_point(aes(y = .pred), shape = \"circle open\") + \n  theme_minimal() +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    subtitle = \"Residuals\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nModel summary\n\nDemo: Display the model summary including estimates for the slope and intercept along with measurements of uncertainty around them. Show how you can extract these values from the model output.\n\n\nfish_hw_tidy &lt;- tidy(fish_hw_fit)\nfish_hw_tidy\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -288.      34.0      -8.49 1.83e-11\n2 height          60.9      2.64     23.1  2.40e-29\n\n\n\nDemo: Write out your model using mathematical notation.\n\n\\(\\widehat{weight} = -288 + 60.9 \\times height\\)\n\n\nCorrelation\nWe can also assess correlation between two quantitative variables.\n\nYour turn (5 minutes):\n\nWhat is correlation? What are values correlation can take?\nStrength and direction of a linear relationship. It’s bounded by -1 and 1.\nAre you good at guessing correlation? Give it a try! https://www.rossmanchance.com/applets/2021/guesscorrelation/GuessCorrelation.html\n\nDemo: What is the correlation between heights and weights of fish?\n\n\nfish |&gt;\n  summarize(r = cor(height, weight))\n\n# A tibble: 1 × 1\n      r\n  &lt;dbl&gt;\n1 0.954\n\n\n\n\nAdding a third variable\n\nDemo: Does the relationship between heights and weights of fish change if we take into consideration species? Plot two separate straight lines for the Bream and Roach species.\n\n\nggplot(fish, \n       aes(x = height, y = weight, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nFitting other models\n\nDemo: We can fit more models than just a straight line. Change the following code below to read method = \"loess\". What is different from the plot created before?\n\n\nggplot(fish, \n       aes(x = height, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Weights vs. heights of fish\",\n    x = \"Height (cm)\",\n    y = \"Weight (gr)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html",
    "href": "ae/ae-17-effective-dataviz.html",
    "title": "Trends instructional staff employees in universities",
    "section": "",
    "text": "The American Association of University Professors (AAUP) is a nonprofit membership association of faculty and other academic professionals. This report by the AAUP shows trends in instructional staff employees between 1975 and 2011, and contains the following image. What trends are apparent in this visualization?"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#packages",
    "href": "ae/ae-17-effective-dataviz.html#packages",
    "title": "Trends instructional staff employees in universities",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggthemes)"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#data",
    "href": "ae/ae-17-effective-dataviz.html#data",
    "title": "Trends instructional staff employees in universities",
    "section": "Data",
    "text": "Data\nEach row in this dataset represents a faculty type, and the columns are the years for which we have data. The values are percentage of hires of that type of faculty for each year.\n\nstaff &lt;- read_csv(\"https://sta199-s24.github.io/data/instructional-staff.csv\")\nstaff\n\n# A tibble: 5 × 12\n  faculty_type    `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Full-Time Tenu…   29     27.6   25     24.8   21.8   20.3   19.3   17.8   17.2\n2 Full-Time Tenu…   16.1   11.4   10.2    9.6    8.9    9.2    8.8    8.2    8  \n3 Full-Time Non-…   10.3   14.1   13.6   13.6   15.2   15.5   15     14.8   14.9\n4 Part-Time Facu…   24     30.4   33.1   33.2   35.5   36     37     39.3   40.5\n5 Graduate Stude…   20.5   16.5   18.1   18.8   18.7   19     20     19.9   19.5\n# ℹ 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt;"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#recreate",
    "href": "ae/ae-17-effective-dataviz.html#recreate",
    "title": "Trends instructional staff employees in universities",
    "section": "Recreate",
    "text": "Recreate\n\nYour turn (10 minutes): Recreate the visualization above. Try to match as many of the elements as possible. Hint: You might need to reshape your data first.\n\n\n# add code here\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#represent-percentages-as-parts-of-a-whole",
    "href": "ae/ae-17-effective-dataviz.html#represent-percentages-as-parts-of-a-whole",
    "title": "Trends instructional staff employees in universities",
    "section": "Represent percentages as parts of a whole",
    "text": "Represent percentages as parts of a whole\n\nDemo: Recreate the previous visualization where the percentages are represented as parts of a whole.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#place-time-on-x-axis",
    "href": "ae/ae-17-effective-dataviz.html#place-time-on-x-axis",
    "title": "Trends instructional staff employees in universities",
    "section": "Place time on x-axis",
    "text": "Place time on x-axis\n\nDemo: Convert the visualization to a line plot with time on the x-axis.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#pay-attention-to-variable-types",
    "href": "ae/ae-17-effective-dataviz.html#pay-attention-to-variable-types",
    "title": "Trends instructional staff employees in universities",
    "section": "Pay attention to variable types",
    "text": "Pay attention to variable types\n\nQuestion: What is wrong with the x-axis of the plot above? How can you fix it?\n\nAdd response here.\n\nYour turn: Implement the fix for the x-axis of the plot.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#use-an-accessible-color-scale",
    "href": "ae/ae-17-effective-dataviz.html#use-an-accessible-color-scale",
    "title": "Trends instructional staff employees in universities",
    "section": "Use an accessible color scale",
    "text": "Use an accessible color scale\n\nQuestion: What do we mean by an accessible color scale? What types of color vision deficiencies are there?\n\nAdd response here.\n\nDemo: What does the plot look like to people with various color vision deficiencies?\nDemo: Remake the plot with an accessible color scale.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#use-direct-labeling",
    "href": "ae/ae-17-effective-dataviz.html#use-direct-labeling",
    "title": "Trends instructional staff employees in universities",
    "section": "Use direct labeling",
    "text": "Use direct labeling\n\nDemo: Remove the legend and add labels for each line at the end of the line (where x is the max(x) recorded).\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#use-color-to-draw-attention",
    "href": "ae/ae-17-effective-dataviz.html#use-color-to-draw-attention",
    "title": "Trends instructional staff employees in universities",
    "section": "Use color to draw attention",
    "text": "Use color to draw attention\n\nDemo: Redo the line plot where Part-time Faculty is red and the rest are gray.\n\n\n# label: recode\n# add code here\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#pick-a-purpose",
    "href": "ae/ae-17-effective-dataviz.html#pick-a-purpose",
    "title": "Trends instructional staff employees in universities",
    "section": "Pick a purpose",
    "text": "Pick a purpose\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#use-labels-to-communicate-the-message",
    "href": "ae/ae-17-effective-dataviz.html#use-labels-to-communicate-the-message",
    "title": "Trends instructional staff employees in universities",
    "section": "Use labels to communicate the message",
    "text": "Use labels to communicate the message\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#simplify",
    "href": "ae/ae-17-effective-dataviz.html#simplify",
    "title": "Trends instructional staff employees in universities",
    "section": "Simplify",
    "text": "Simplify\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-effective-dataviz.html#summary",
    "href": "ae/ae-17-effective-dataviz.html#summary",
    "title": "Trends instructional staff employees in universities",
    "section": "Summary",
    "text": "Summary\n\nRepresent percentages as parts of a whole\nPlace variables representing time on the x-axis when possible\nPay attention to data types, e.g., represent time as time on a continuous scale, not years as levels of a categorical variable\nPrefer direct labeling over legends\nUse accessible colors\nUse color to draw attention\nPick a purpose and label, color, annotate for that purpose\nCommunicate your main message directly in the plot labels\nSimplify before you call it done (a.k.a. “Before you leave the house, look in the mirror and take one thing off”)"
  },
  {
    "objectID": "ae/ae-14-spam-filter.html",
    "href": "ae/ae-14-spam-filter.html",
    "title": "Building a spam filter",
    "section": "",
    "text": "In this application exercise, we will\nTo illustrate logistic regression, we will build a spam filter from email data.\nThe data come from incoming emails in David Diez’s (one of the authors of OpenIntro textbooks) Gmail account for the first three months of 2012. All personally identifiable information has been removed.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nglimpse(email)\n\nRows: 3,921\nColumns: 21\n$ spam         &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ from         &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, …\n$ sent_email   &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n$ time         &lt;dttm&gt; 2012-01-01 01:16:41, 2012-01-01 02:03:59, 2012-01-01 11:…\n$ image        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       &lt;dbl&gt; 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, …\n$ winner       &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     &lt;dbl&gt; 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ num_char     &lt;dbl&gt; 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421…\n$ line_breaks  &lt;int&gt; 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191…\n$ format       &lt;fct&gt; 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ re_subj      &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, …\n$ exclaim_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ urgent_subj  &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess &lt;dbl&gt; 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0…\n$ number       &lt;fct&gt; big, small, small, small, none, none, big, small, small, …\nThe variables we’ll use in this analysis are\nGoal: Use the number of exclamation points in an email to predict whether or not it is spam."
  },
  {
    "objectID": "ae/ae-14-spam-filter.html#exercise-1",
    "href": "ae/ae-14-spam-filter.html#exercise-1",
    "title": "Building a spam filter",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nDemo: Fit the logistic regression model using the number of exclamation points to predict the probability an email is spam.\n\n\nlog_fit &lt;- logistic_reg() |&gt;\n  fit(spam ~ exclaim_mess, data = email)\n\ntidy(log_fit)\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -2.27      0.0553     -41.1     0    \n2 exclaim_mess  0.000272  0.000949     0.287   0.774\n\n\n\nYour turn: How does the code above differ from previous code we’ve used to fit regression models? Compare your summary output to the estimated model below.\n\n\\[\\log\\Big(\\frac{p}{1-p}\\Big) = -2.27 - 0.000272 \\times exclaim\\_mess\\]\nAdd response here."
  },
  {
    "objectID": "ae/ae-14-spam-filter.html#exercise-2",
    "href": "ae/ae-14-spam-filter.html#exercise-2",
    "title": "Building a spam filter",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat is the probability the email is spam if it contains 10 exclamation points? Answer the question using the predict() function.\nWe can use the predict function in R to produce the probability as well.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-14-spam-filter.html#exercise-3",
    "href": "ae/ae-14-spam-filter.html#exercise-3",
    "title": "Building a spam filter",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe have the probability an email is spam, but ultimately we want to use the probability to classify an email as spam or not spam. Therefore, we need to set a decision-making threshold, such that an email is classified as spam if the predicted probability is greater than the threshold and not spam otherwise.\nSuppose you are a data scientist working on a spam filter. You must determine how high the predicted probability must be before you think it would be reasonable to call it spam and put it in the junk folder (which the user is unlikely to check).\nYour turn: What are some trade offs you would consider as you set the decision-making threshold? Discuss with your neighbor.\nAdd response here.\n\nlog_aug &lt;- augment(log_fit, email)\n\nggplot(log_aug, aes(x = exclaim_mess, y = spam, color = .pred_class)) +\n  geom_jitter(alpha = 0.5)"
  },
  {
    "objectID": "ae/ae-14-spam-filter.html#exercise-4",
    "href": "ae/ae-14-spam-filter.html#exercise-4",
    "title": "Building a spam filter",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit a model with all variables in the dataset as predictors and receate the visualization above for this model.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-03-duke-forest-A.html",
    "href": "ae/ae-03-duke-forest-A.html",
    "title": "AE 03: Duke Forest",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key.\n\n\n\nPackages\n\nlibrary(tidyverse)\nlibrary(openintro)\n\n\n\nExercise 1\nThe distributions of prices of houses with and without garages are fairly similar, with centers around $600,000. Each distribution has potential outliers on the higher end. Based on this visualization, having a garage does not appear to “make a difference”.\n\nduke_forest |&gt;\n  mutate(garage = if_else(str_detect(parking, \"Garage\"), \"Garage\", \"No garage\")) |&gt;\n  ggplot(aes(x = price, fill = garage)) +\n  geom_histogram() +\n  facet_wrap(~garage, ncol = 1) +\n  labs(\n    x = \"Price in $\",\n    y = \"\",\n    title = \"Histogram of Price of Homes by Garage or not\",\n    fill = \"Garage or not\"\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\nClaim 1: Yes, there seems to be evidence of a positive relationship between the price of the home and the area of the home.\nClaim 2: No, there does not seem to be a relationship between the price and age of the home.\nClaim 3: No, there does not seem to be any evidence to suggest that larger more expensive homes are newer than those houses that are cheaper and smaller. Points that are lighter colored (newer homes) are not concentrated on the top right of the plot.\n\n\nggplot(\n  duke_forest,\n  aes(x = area, y = price, color = year_built)\n) +\n  geom_point(size = 3) +\n  geom_smooth(se = FALSE) +\n  labs(\n    x = \"Area of Home\",\n    y = \"Price of Home\",\n    title = \"Relationship between Price and Area by Year Built\",\n    color = \"Year Built\"\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: colour\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?"
  },
  {
    "objectID": "ae/ae-07-population-types-A.html",
    "href": "ae/ae-07-population-types-A.html",
    "title": "AE 07: Types and classes and populations",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key."
  },
  {
    "objectID": "ae/ae-07-population-types-A.html#packages",
    "href": "ae/ae-07-population-types-A.html#packages",
    "title": "AE 07: Types and classes and populations",
    "section": "Packages",
    "text": "Packages\nWe will use the following two packages in this application exercise.\n\ntidyverse: For data import, wrangling, and visualization.\nscales: For better axis labels.\n\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(scales)"
  },
  {
    "objectID": "ae/ae-07-population-types-A.html#type-coercion",
    "href": "ae/ae-07-population-types-A.html#type-coercion",
    "title": "AE 07: Types and classes and populations",
    "section": "Type coercion",
    "text": "Type coercion\n\nDemo: Determine the type of the following vector. And then, change the type to numeric.\n\nx &lt;- c(\"1\", \"2\", \"3\")\ntypeof(x)\n\n[1] \"character\"\n\nas.numeric(x)\n\n[1] 1 2 3\n\n\nDemo: Once again, determine the type of the following vector. And then, change the type to numeric. What’s different than the previous exercise?\n\ny &lt;- c(\"a\", \"b\", \"c\")\ntypeof(y)\n\n[1] \"character\"\n\nas.numeric(y)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA\n\n\nDemo: Once again, determine the type of the following vector. And then, change the type to numeric. What’s different than the previous exercise?\n\nz &lt;- c(\"1\", \"2\", \"three\")\ntypeof(z)\n\n[1] \"character\"\n\nas.numeric(z)\n\nWarning: NAs introduced by coercion\n\n\n[1]  1  2 NA\n\n\nDemo: Suppose you conducted a survey where you asked people how many cars their household owns collectively. And the answers are as follows:\n\nsurvey_results &lt;- tibble(cars = c(1, 2, \"three\"))\nsurvey_results\n\n# A tibble: 3 × 1\n  cars \n  &lt;chr&gt;\n1 1    \n2 2    \n3 three\n\n\nThis is annoying because of that third survey taker who just had to go and type out the number instead of providing as a numeric value. So now you need to update the cars variable to be numeric. You do the following\n\nsurvey_results |&gt;\n  mutate(cars = as.numeric(cars))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `cars = as.numeric(cars)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n# A tibble: 3 × 1\n   cars\n  &lt;dbl&gt;\n1     1\n2     2\n3    NA\n\n\nAnd now things are even more annoying because you get a warning NAs introduced by coercion that happened while computing cars = as.numeric(cars) and the response from the third survey taker is now an NA (you lost their data). Fix your mutate() call to avoid this warning.\n\nsurvey_results |&gt;\n  mutate(\n    cars = if_else(cars == \"three\", \"3\", cars),\n    cars = as.numeric(cars)\n  )\n\n# A tibble: 3 × 1\n   cars\n  &lt;dbl&gt;\n1     1\n2     2\n3     3\n\n\nYour turn: First, guess the type of the vector. Then, check if you guessed right. I’ve done the first one for you, you’ll see that it’s helpful to check the type of each element of the vector first.\n\nc(1, 1L, \"C\")\n\nv1 &lt;- c(1, 1L, \"C\")\n\n# to help you guess\ntypeof(1)\n\n[1] \"double\"\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(\"C\")\n\n[1] \"character\"\n\n# to check after you guess\ntypeof(v1)\n\n[1] \"character\"\n\n\nc(1L / 0, \"A\")\n\nv2 &lt;- c(1L / 0, \"A\")\n\n# to help you guess\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(0)\n\n[1] \"double\"\n\ntypeof(1L / 0)\n\n[1] \"double\"\n\ntypeof(\"A\")\n\n[1] \"character\"\n\n# to check after you guess\ntypeof(v2)\n\n[1] \"character\"\n\n\nc(1:3, 5)\n\nv3 &lt;- c(1:3, 5)\n\n# to help you guess\ntypeof(1:3)\n\n[1] \"integer\"\n\ntypeof(5)\n\n[1] \"double\"\n\n# to check after you guess\ntypeof(v3)\n\n[1] \"double\"\n\n\nc(3, \"3+\")\n\nv4 &lt;- c(3, \"3+\")\n\n# to help you guess\ntypeof(3)\n\n[1] \"double\"\n\ntypeof(\"3+\")\n\n[1] \"character\"\n\n# to check after you guess\ntypeof(v4)\n\n[1] \"character\"\n\n\nc(NA, TRUE)\n\nv5 &lt;- c(NA, TRUE)\n\n# to help you guess\ntypeof(NA)\n\n[1] \"logical\"\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\n# to check after you guess\ntypeof(v5)\n\n[1] \"logical\""
  },
  {
    "objectID": "ae/ae-07-population-types-A.html#populations-in-continents",
    "href": "ae/ae-07-population-types-A.html#populations-in-continents",
    "title": "AE 07: Types and classes and populations",
    "section": "Populations in continents",
    "text": "Populations in continents\nIn the previous application exercise you joined two datasets (after a bit of data cleaning), and calculated total population in each continent and visualized it.\n\nFirst, you loaded the data:\n\n\ncontinents &lt;- read_csv(\"https://sta199-s24.github.io/data/continents.csv\")\n\nRows: 285 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): entity, code, continent\ndbl (1): year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npopulation &lt;- read_csv(\"https://sta199-s24.github.io/data/world-pop-2022.csv\")\n\nRows: 217 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): year, population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nThen you cleaned the country names where the spelling in one data frame didn’t match the other, and joined the data sets:\n\n\npopulation_continent &lt;- population |&gt;\n  mutate(country = case_when(\n    country == \"Congo, Dem. Rep.\" ~ \"Democratic Republic of Congo\",\n    country == \"Congo, Rep.\" ~ \"Congo\",\n    country == \"Hong Kong SAR, China\" ~ \"Hong Kong\",\n    country == \"Korea, Dem. People's Rep.\" ~ \"North Korea\",\n    country == \"Korea, Rep.\" ~ \"South Korea\",\n    country == \"Kyrgyz Republic\" ~ \"Kyrgyzstan\",\n    .default = country\n    )\n  ) |&gt;\n  left_join(continents, by = join_by(country == entity))\n\n\nThen, you calculated total population for each continent.\n\n\npopulation_summary &lt;- population_continent |&gt;\n  group_by(continent) |&gt;\n  summarize(total_pop = sum(population)) |&gt;\n  arrange(desc(total_pop))\n\n\nAnd finally, you visualized these data.\n\n\nggplot(population_summary) +\n  geom_point(aes(x = total_pop, y = continent)) +\n  geom_segment(aes(y = continent, yend = continent, x = 0, xend = total_pop)) +\n  scale_x_continuous(labels = label_number(scale = 1/1000000, suffix = \" bil\")) +\n  theme_minimal() +\n  labs(\n    x = \"Total population\",\n    y = \"Continent\",\n    title = \"World population\",\n    subtitle = \"As of 2022\",\n    caption = \"Data sources: The World Bank and Our World in Data\"\n  )\n\n\n\n\n\n\n\n\n\nQuestion: Take a look at the visualization. How are the continents ordered? What would be a better order?\n\nOrdering the continents by the value of total population would be better.\n\nDemo: Reorder the continents on the y-axis (levels of continent) in order of value of total population. You will want to use a function from the forcats package, see https://forcats.tidyverse.org/reference/index.html for inspiration and help.\n\n\npopulation_summary |&gt;\n  mutate(continent = fct_reorder(continent, total_pop)) |&gt;\n  ggplot() +\n  geom_point(aes(x = total_pop, y = continent)) +\n  geom_segment(aes(y = continent, yend = continent, x = 0, xend = total_pop)) +\n  geom_segment(aes(y = continent, yend = continent, x = 0, xend = total_pop)) +\n  scale_x_continuous(labels = label_number(scale = 1/1000000, suffix = \" bil\")) +\n  theme_minimal() +\n  labs(\n    x = \"Total population\",\n    y = \"Continent\",\n    title = \"World population\",\n    subtitle = \"As of 2022\",\n    caption = \"Data sources: The World Bank and Our World in Data\"\n  )\n\n\n\n\n\n\n\n\n\nThink out loud: Describe what is happening in the each step of the code chunk above.\n\nAnswers may vary."
  },
  {
    "objectID": "ae/ae-06-population-joining-A.html",
    "href": "ae/ae-06-population-joining-A.html",
    "title": "AE 06: Joining country populations with continents",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key."
  },
  {
    "objectID": "ae/ae-06-population-joining-A.html#goal",
    "href": "ae/ae-06-population-joining-A.html#goal",
    "title": "AE 06: Joining country populations with continents",
    "section": "Goal",
    "text": "Goal\nOur ultimate goal in this application exercise is to create a bar plot of total populations of continents, where the input data are:\n\nCountries and populations\nCountries and continents\n\n\nlibrary(tidyverse) # for data wrangling and visualization\nlibrary(scales)    # for pretty axis breaks"
  },
  {
    "objectID": "ae/ae-06-population-joining-A.html#data",
    "href": "ae/ae-06-population-joining-A.html#data",
    "title": "AE 06: Joining country populations with continents",
    "section": "Data",
    "text": "Data\n\nCountries and populations\nThese data come from The World Bank and reflect population counts as of 2022.\n\npopulation &lt;- read_csv(\"https://sta199-s24.github.io/data/world-pop-2022.csv\")\n\nLet’s take a look at the data.\n\npopulation\n\n# A tibble: 217 × 3\n   country              year population\n   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;\n 1 Afghanistan          2022    41129. \n 2 Albania              2022     2778. \n 3 Algeria              2022    44903. \n 4 American Samoa       2022       44.3\n 5 Andorra              2022       79.8\n 6 Angola               2022    35589. \n 7 Antigua and Barbuda  2022       93.8\n 8 Argentina            2022    46235. \n 9 Armenia              2022     2780. \n10 Aruba                2022      106. \n# ℹ 207 more rows\n\n\n\n\nContinents\nThese data come from Our World in Data.\n\ncontinents &lt;- read_csv(\"https://sta199-s24.github.io/data/continents.csv\")\n\nLet’s take a look at the data.\n\ncontinents\n\n# A tibble: 285 × 4\n   entity                code      year continent    \n   &lt;chr&gt;                 &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;        \n 1 Abkhazia              OWID_ABK  2015 Asia         \n 2 Afghanistan           AFG       2015 Asia         \n 3 Akrotiri and Dhekelia OWID_AKD  2015 Asia         \n 4 Aland Islands         ALA       2015 Europe       \n 5 Albania               ALB       2015 Europe       \n 6 Algeria               DZA       2015 Africa       \n 7 American Samoa        ASM       2015 Oceania      \n 8 Andorra               AND       2015 Europe       \n 9 Angola                AGO       2015 Africa       \n10 Anguilla              AIA       2015 North America\n# ℹ 275 more rows"
  },
  {
    "objectID": "ae/ae-06-population-joining-A.html#exercises",
    "href": "ae/ae-06-population-joining-A.html#exercises",
    "title": "AE 06: Joining country populations with continents",
    "section": "Exercises",
    "text": "Exercises\n\nThink out loud:\n\nWhich variable(s) will we use to join the population and continents data frames?\n\ncountry from population and entity from continents\n\nWe want to create a new data frame that keeps all rows and columns from population and brings in the corresponding information from continents. Which join function should we use?\n\nleft_join() with population on the left.\nDemo: Join the two data frames and name assign the joined data frame to a new data frame population_continents.\n\n\npopulation_continent &lt;- population |&gt;\n  left_join(continents, by = join_by(country == entity))\n\n\nDemo: Take a look at the newly created population_continent data frame. There are some countries that were not in continents. First, identify which countries these are (they will have NA values for continent).\n\n\npopulation_continent |&gt;\n  filter(is.na(continent))\n\n# A tibble: 6 × 6\n  country                   year.x population code  year.y continent\n  &lt;chr&gt;                      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    \n1 Congo, Dem. Rep.            2022     99010. &lt;NA&gt;      NA &lt;NA&gt;     \n2 Congo, Rep.                 2022      5970. &lt;NA&gt;      NA &lt;NA&gt;     \n3 Hong Kong SAR, China        2022      7346. &lt;NA&gt;      NA &lt;NA&gt;     \n4 Korea, Dem. People's Rep.   2022     26069. &lt;NA&gt;      NA &lt;NA&gt;     \n5 Korea, Rep.                 2022     51628. &lt;NA&gt;      NA &lt;NA&gt;     \n6 Kyrgyz Republic             2022      6975. &lt;NA&gt;      NA &lt;NA&gt;     \n\n\n\nDemo: All of these countries are actually in the continents data frame, but under different names. So, let’s clean that data first by updating the country names in the population data frame in a way they will match the continents data frame, and then joining them, using a case_when() statement in mutate(). At the end, check that all countries now have continent information.\n\n\npopulation_continent &lt;- population |&gt;\n  mutate(country = case_when(\n    country == \"Congo, Dem. Rep.\" ~ \"Democratic Republic of Congo\",\n    country == \"Congo, Rep.\" ~ \"Congo\",\n    country == \"Hong Kong SAR, China\" ~ \"Hong Kong\",\n    country == \"Korea, Dem. People's Rep.\" ~ \"North Korea\",\n    country == \"Korea, Rep.\" ~ \"South Korea\",\n    country == \"Kyrgyz Republic\" ~ \"Kyrgyzstan\",\n    .default = country\n    )\n  ) |&gt;\n  left_join(continents, by = join_by(country == entity))\n\npopulation_continent |&gt;\n  filter(is.na(continent))\n\n# A tibble: 0 × 6\n# ℹ 6 variables: country &lt;chr&gt;, year.x &lt;dbl&gt;, population &lt;dbl&gt;, code &lt;chr&gt;,\n#   year.y &lt;dbl&gt;, continent &lt;chr&gt;\n\n\n\nThink out loud: Which continent do you think has the highest population? Which do you think has the second highest? The lowest?\n\nAdd your response here.\n\nDemo: Create a new data frame called population_summary that contains a row for each continent and a column for the total population for that continent, in descending order of population. Note that the function for calculating totals in R is sum().\n\n\npopulation_summary &lt;- population_continent |&gt;\n  group_by(continent) |&gt;\n  summarize(total_pop = sum(population)) |&gt;\n  arrange(desc(total_pop))\n\n\nYour turn: Make a bar plot with total population on the y-axis and continent on the x-axis, where the height of each bar represents the total population in that continent.\n\n\nggplot(population_summary, aes(x = continent, y = total_pop)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nYour turn: Recreate the following plot, which is commonly referred to as a lollipop plot. Hint: Start with the points, then try adding the segments, then add axis labels and caption, and finally, as a stretch goal, update the x scale (which will require a function we haven’t introduced in lectures or labs yet!).\n\n\n\nggplot(population_summary, aes()) +\n  geom_point(aes(x = total_pop, y = continent)) +\n  geom_segment(aes(y = continent, yend = continent, x = 0, xend = total_pop)) +\n  scale_x_continuous(labels = label_number(scale = 1/1000000, suffix = \" bil\")) +\n  theme_minimal() +\n  labs(\n    x = \"Total population\",\n    y = \"Continent\",\n    title = \"World population\",\n    subtitle = \"As of 2022\",\n    caption = \"Data sources: The World Bank and Our World in Data\"\n  )\n\n\n\n\n\n\n\n\n\nThink out loud: What additional improvements would you like to make to this plot.\n\nAnswers may vary. Ordering the continents in decreasing order of population."
  },
  {
    "objectID": "ae/ae-08-data-import-A.html",
    "href": "ae/ae-08-data-import-A.html",
    "title": "AE 08: Data import",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers. This document should be used as reference only, it’s not designed to be an exhaustive key."
  },
  {
    "objectID": "ae/ae-08-data-import-A.html#packages",
    "href": "ae/ae-08-data-import-A.html#packages",
    "title": "AE 08: Data import",
    "section": "Packages",
    "text": "Packages\nWe will use the following two packages in this application exercise.\n\ntidyverse: For data import, wrangling, and visualization.\nreadxl: For importing data from Excel.\n\n\nlibrary(tidyverse)\nlibrary(readxl)"
  },
  {
    "objectID": "ae/ae-13-modeling-loans.html",
    "href": "ae/ae-13-modeling-loans.html",
    "title": "Modelling loan interest rates",
    "section": "",
    "text": "In this application exercise we will be studying loan interest rates. The dataset is one you’ve come across before in your reading – the dataset about loans from the peer-to-peer lender, Lending Club, from the openintro package. We will use tidyverse and tidymodels for data exploration and modeling, respectively.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nBefore we use the dataset, we’ll make a few transformations to it.\nAdd response here.\nloans &lt;- loans_full_schema |&gt;\n  mutate(\n    credit_util = total_credit_utilized / total_credit_limit,\n    bankruptcy = as.factor(if_else(public_record_bankrupt == 0, 0, 1)),\n    verified_income = droplevels(verified_income),\n    homeownership = str_to_title(homeownership),\n    homeownership = fct_relevel(homeownership, \"Rent\", \"Mortgage\", \"Own\")\n  ) |&gt;\n  rename(credit_checks = inquiries_last_12m) |&gt;\n  select(\n    interest_rate, loan_amount, verified_income, \n    debt_to_income, credit_util, bankruptcy, term, \n    credit_checks, issue_month, homeownership\n  )\nHere is a glimpse at the data:\nglimpse(loans)\n\nRows: 10,000\nColumns: 10\n$ interest_rate   &lt;dbl&gt; 14.07, 12.61, 17.09, 6.72, 14.07, 6.72, 13.59, 11.99, …\n$ loan_amount     &lt;int&gt; 28000, 5000, 2000, 21600, 23000, 5000, 24000, 20000, 2…\n$ verified_income &lt;fct&gt; Verified, Not Verified, Source Verified, Not Verified,…\n$ debt_to_income  &lt;dbl&gt; 18.01, 5.04, 21.15, 10.16, 57.96, 6.46, 23.66, 16.19, …\n$ credit_util     &lt;dbl&gt; 0.54759517, 0.15003472, 0.66134832, 0.19673228, 0.7549…\n$ bankruptcy      &lt;fct&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, …\n$ term            &lt;dbl&gt; 60, 36, 36, 36, 36, 36, 60, 60, 36, 36, 60, 60, 36, 60…\n$ credit_checks   &lt;int&gt; 6, 1, 4, 0, 7, 6, 1, 1, 3, 0, 4, 4, 8, 6, 0, 0, 4, 6, …\n$ issue_month     &lt;fct&gt; Mar-2018, Feb-2018, Feb-2018, Jan-2018, Mar-2018, Jan-…\n$ homeownership   &lt;fct&gt; Mortgage, Rent, Rent, Rent, Rent, Own, Mortgage, Mortg…"
  },
  {
    "objectID": "ae/ae-13-modeling-loans.html#main-effects-model",
    "href": "ae/ae-13-modeling-loans.html#main-effects-model",
    "title": "Modelling loan interest rates",
    "section": "Main effects model",
    "text": "Main effects model\n\nDemo: Fit a model to predict interest rate from credit utilization and homeownership, without an interaction effect between the two predictors. Display the summary output and write out the estimated regression equation.\n\n\n# add code here\n\nAdd response here.\n\nDemo: Write the estimated regression equation for loan applications from each of the homeownership groups separately.\n\nRent: Add response here.\nMortgage: Add response here.\nOwn: Add response here.\n\nQuestion: How does the model predict the interest rate to vary as credit utilization varies for loan applicants with different homeownership status. Are the rates the same or different?\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-13-modeling-loans.html#interaction-effects-model",
    "href": "ae/ae-13-modeling-loans.html#interaction-effects-model",
    "title": "Modelling loan interest rates",
    "section": "Interaction effects model",
    "text": "Interaction effects model\n\nDemo: Fit a model to predict interest rate from credit utilization and homeownership, with an interaction effect between the two predictors. Display the summary output and write out the estimated regression equation.\n\n\n# add code here\n\nAdd response here.\n\nDemo: Write the estimated regression equation for loan applications from each of the homeownership groups separately.\n\nRent: Add response here.\nMortgage: Add response here.\nOwn: Add response here.\n\nQuestion: How does the model predict the interest rate to vary as credit utilization varies for loan applicants with different homeownership status. Are the rates the same or different?\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-13-modeling-loans.html#choosing-a-model",
    "href": "ae/ae-13-modeling-loans.html#choosing-a-model",
    "title": "Modelling loan interest rates",
    "section": "Choosing a model",
    "text": "Choosing a model\nRule of thumb: Occam’s Razor - Don’t overcomplicate the situation! We prefer the simplest best model.\n\n# add code here\n\n\nReview: What is R-squared? What is adjusted R-squared?\n\nAdd response here.\n\nQuestion: Based on the adjusted \\(R^2\\)s of these two models, which one do we prefer?\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-11-modeling-penguins.html",
    "href": "ae/ae-11-modeling-penguins.html",
    "title": "AE 11: Modelling penguins",
    "section": "",
    "text": "In this application exercise we will be studying penguins. The data can be found in the palmerpenguins package and we will use tidyverse and tidymodels for data exploration and modeling, respectively.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\nPlease read the following context and take a glimpse at the data set before we get started.\n\nThis data set comprising various measurements of three different penguin species, namely Adelie, Gentoo, and Chinstrap. The rigorous study was conducted in the islands of the Palmer Archipelago, Antarctica. These data were collected from 2007 to 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. The data set is called penguins.\n\n\n# add code here\n\nOur goal is to understand better how various body measurements and attributes of penguins relate to their body mass. First, we are going to investigate the relationship between a penguins’ flipper lengths and their body masses.\n\nQuestion: Based on our research focus, which variable is the response variable?\n\nAdd response here\n\nDemo: Visualize the relationship between flipper length and body mass of penguins.\n\n\n# add code here \n\n\nCorrelation\n\nYour turn (5 minutes):\n\nWhat is correlation? What values can correlation take?\nStrength and direction of a linear relationship. It’s bounded by -1 and 1.\nAre you good at guessing correlation? Give it a try! https://www.rossmanchance.com/applets/2021/guesscorrelation/GuessCorrelation.html\n\nDemo: What is the correlation between flipper length and body mass of penguins?\n\n\n# add code here\n\n\n\nDefining, fitting, and summarizing a model\n\nDemo: Write the population model below that explains the relationship between body mass and flipper length.\n\n\\[\nadd~math~text~here\n\\]\n\nDemo: Fit the linear regression model and display the results. Write the estimated model output below.\n\n\n# add code here\n\n\\[\nadd~math~text~here\n\\]\n\nYour turn: Interpret the slope and the intercept in the context of the data.\n\nIntercept: Add your response here\nSlopes: Add your response here\n\nYour turn: Recreate the visualization from above, this time adding a regression line to the visualization geom_smooth(method = \"lm\").\n\n\n# add code here\n\n\nWhat is the estimated body mass for a penguin with a flipper length of 210?\n\n\n# add code here\n\n\nWhat is the estimated body mass for a penguin with a flipper length of 100?\n\n\n# add code here\n\n\n\nAnother model\n\nDemo: A different researcher wants to look at body weight of penguins based on the island they were recorded on. How are the variables involved in this analysis different?\n\nAdd response here\n\nDemo: Make an appropriate visualization to investigate this relationship below. Additionally, calculate the mean body mass by island.\n\n\n# add code here\n\n\n# add code here\n\n\nDemo: Change the geom of your previous plot to geom_point(). Use this plot to think about how R models these data.\n\n\n# add code here\n\n\nYour turn: Fit the linear regression model and display the results. Write the estimated model output below.\n\n\n# add code here\n\nAdd math text here\n\nDemo: Interpret each coefficient in context of the problem.\n\nIntercept: Add your response here\nSlopes: Add your response here\n\nDemo: What is the estimated body weight of a penguin on Biscoe island? What are the estimated body weights of penguins on Dream and Torgersen islands?\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-06-population-joining.html",
    "href": "ae/ae-06-population-joining.html",
    "title": "AE 06: Joining country populations with continents",
    "section": "",
    "text": "Our ultimate goal in this application exercise is to create a bar plot of total populations of continents, where the input data are:\n\nCountries and populations\nCountries and continents\n\n\nlibrary(tidyverse) # for data wrangling and visualization\nlibrary(scales)    # for pretty axis breaks"
  },
  {
    "objectID": "ae/ae-06-population-joining.html#goal",
    "href": "ae/ae-06-population-joining.html#goal",
    "title": "AE 06: Joining country populations with continents",
    "section": "",
    "text": "Our ultimate goal in this application exercise is to create a bar plot of total populations of continents, where the input data are:\n\nCountries and populations\nCountries and continents\n\n\nlibrary(tidyverse) # for data wrangling and visualization\nlibrary(scales)    # for pretty axis breaks"
  },
  {
    "objectID": "ae/ae-06-population-joining.html#data",
    "href": "ae/ae-06-population-joining.html#data",
    "title": "AE 06: Joining country populations with continents",
    "section": "Data",
    "text": "Data\n\nCountries and populations\nThese data come from The World Bank and reflect population counts as of 2022.\n\npopulation &lt;- read_csv(\"https://sta199-s24.github.io/data/world-pop-2022.csv\")\n\nLet’s take a look at the data.\n\npopulation\n\n# A tibble: 217 × 3\n   country              year population\n   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;\n 1 Afghanistan          2022    41129. \n 2 Albania              2022     2778. \n 3 Algeria              2022    44903. \n 4 American Samoa       2022       44.3\n 5 Andorra              2022       79.8\n 6 Angola               2022    35589. \n 7 Antigua and Barbuda  2022       93.8\n 8 Argentina            2022    46235. \n 9 Armenia              2022     2780. \n10 Aruba                2022      106. \n# ℹ 207 more rows\n\n\n\n\nContinents\nThese data come from Our World in Data.\n\ncontinents &lt;- read_csv(\"https://sta199-s24.github.io/data/continents.csv\")\n\nLet’s take a look at the data.\n\ncontinents\n\n# A tibble: 285 × 4\n   entity                code      year continent    \n   &lt;chr&gt;                 &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;        \n 1 Abkhazia              OWID_ABK  2015 Asia         \n 2 Afghanistan           AFG       2015 Asia         \n 3 Akrotiri and Dhekelia OWID_AKD  2015 Asia         \n 4 Aland Islands         ALA       2015 Europe       \n 5 Albania               ALB       2015 Europe       \n 6 Algeria               DZA       2015 Africa       \n 7 American Samoa        ASM       2015 Oceania      \n 8 Andorra               AND       2015 Europe       \n 9 Angola                AGO       2015 Africa       \n10 Anguilla              AIA       2015 North America\n# ℹ 275 more rows"
  },
  {
    "objectID": "ae/ae-06-population-joining.html#exercises",
    "href": "ae/ae-06-population-joining.html#exercises",
    "title": "AE 06: Joining country populations with continents",
    "section": "Exercises",
    "text": "Exercises\n\nThink out loud:\n\nWhich variable(s) will we use to join the population and continents data frames?\n\nAdd response here.\n\nWe want to create a new data frame that keeps all rows and columns from population and brings in the corresponding information from continents. Which join function should we use?\n\nAdd response here.\nDemo: Join the two data frames and name assign the joined data frame to a new data frame population_continents.\n\n\n# add code here\n\n\nDemo: Take a look at the newly created population_continent data frame. There are some countries that were not in continents. First, identify which countries these are (they will have NA values for continent).\n\n\n# add code here\n\n\nDemo: All of these countries are actually in the continents data frame, but under different names. So, let’s clean that data first by updating the country names in the population data frame in a way they will match the continents data frame, and then joining them, using a case_when() statement in mutate(). At the end, check that all countries now have continent information.\n\n\n# add code here\n\n\nThink out loud: Which continent do you think has the highest population? Which do you think has the second highest? The lowest?\n\nAdd your response here.\n\nDemo: Create a new data frame called population_summary that contains a row for each continent and a column for the total population for that continent, in descending order of population. Note that the function for calculating totals in R is sum().\n\n\n# add code here\n\n\nYour turn: Make a bar plot with total population on the y-axis and continent on the x-axis, where the height of each bar represents the total population in that continent.\n\n\n# add code here\n\n\nYour turn: Recreate the following plot, which is commonly referred to as a lollipop plot. Hint: Start with the points, then try adding the segments, then add axis labels and caption, and finally, as a stretch goal, update the x scale (which will require a function we haven’t introduced in lectures or labs yet!).\n\n\n\n# add code here\n\n\nThink out loud: What additional improvements would you like to make to this plot.\n\nAdd your response here."
  },
  {
    "objectID": "ae/ae-01-meet-the-penguins.html",
    "href": "ae/ae-01-meet-the-penguins.html",
    "title": "AE 01: Meet the penguins",
    "section": "",
    "text": "For this application exercise, we’ll use the tidyverse and palmerpenguins packages.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\n\nThe dataset we will visualize is called penguins. Let’s glimpse() at it.\n\nYour turn: Replace #add code here with the code for “glimpse”ing at the data penguins data frame – glimpse(penguins). Render the document and view the output.\n\n\n# add code here\n\n\nDemo: First, replace the blank below with the number of rows in the penguins data frame based on the output of the chunk below. Then, replace it with “inline code” and render again.\n\n\nnrow(penguins)\n\n[1] 344\n\n\nThere are ___ penguins in the penguins data frame.\n\nx &lt;- 2\nx * 3\n\n[1] 6"
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html",
    "href": "ae/ae-16-equality-randomization-A.html",
    "title": "AE 16: Equality",
    "section": "",
    "text": "In this application exercise, we’ll do inference on two population proportions."
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-1",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-1",
    "title": "AE 16: Equality",
    "section": "Exercise 1",
    "text": "Exercise 1\nThe two populations of interest in this survey are 18-24 year olds and 25+ year olds. State the hypotheses for evaluating whether there is a discernible difference between the proportions of those who think “The country needs to continue to make changes to give women equal rights to men.” (need more changes) in the two age groups.\nLet \\(p\\) = the true proportion of those who think “The country needs to continue to make changes to give women equal rights to men” among 18-24 year old NC voters and 25+ year old NC voters.\n\\(H_0: p_{18-24} = p_{25+}\\)\n\\(H_A: p_{18-24} \\ne p_{25+}\\)"
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-2",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-2",
    "title": "AE 16: Equality",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat proportion of 18-24 year olds think “The country needs to continue to make changes to give women equal rights to men”? What proportion of 25+ year olds? Calculate and visualize these proportions.\n\nequality |&gt;\n  count(age, response) |&gt;\n  group_by(age) |&gt;\n  mutate(p_hat = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   age [2]\n  age   response              n p_hat\n  &lt;chr&gt; &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt;\n1 18-24 Most changes done    32 0.478\n2 18-24 Need more changes    35 0.522\n3 25+   Most changes done   211 0.319\n4 25+   Need more changes   450 0.681\n\nggplot(equality, aes(y = age, fill = response)) +\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-3",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-3",
    "title": "AE 16: Equality",
    "section": "Exercise 3",
    "text": "Exercise 3\nCalculate the observed sample statistic, i.e., the difference between the proportions of respondents who think “The country needs to continue to make changes to give women equal rights to men” between the two age groups.\n\nobs_stat &lt;- equality |&gt;\n  specify(response = response, explanatory = age, success = \"Need more changes\") |&gt;\n  calculate(stat = \"diff in props\", order = c(\"18-24\", \"25+\"))\n\nobs_stat\n\nResponse: response (factor)\nExplanatory: age (factor)\n# A tibble: 1 × 1\n    stat\n   &lt;dbl&gt;\n1 -0.158"
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-4",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-4",
    "title": "AE 16: Equality",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat is the parameter of interest?\nDifference between the proportions of those who think “The country needs to continue to make changes to give women equal rights to men” between 18-24 and 25+ year old NC voters."
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-5",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-5",
    "title": "AE 16: Equality",
    "section": "Exercise 5",
    "text": "Exercise 5\nExplain how you can set up a simulation for this hypothesis test.\nAdd response here."
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-6",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-6",
    "title": "AE 16: Equality",
    "section": "Exercise 6",
    "text": "Exercise 6\nConduct the hypothesis test using randomization and visualize and report the p-value.\n\nset.seed(1234)\n\nnull_dist &lt;- equality |&gt;\n  specify(response = response, explanatory = age, success = \"Need more changes\") |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in props\", order = c(\"18-24\", \"25+\"))\n\nnull_dist |&gt;\n  get_p_value(obs_stat = obs_stat, direction = \"two sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.012\n\nnull_dist |&gt;\n  visualize() +\n  shade_p_value(obs_stat = obs_stat, direction = \"two sided\")"
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-7",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-7",
    "title": "AE 16: Equality",
    "section": "Exercise 7",
    "text": "Exercise 7\nWhat is the conclusion of the hypothesis test?\nWith a p-value of 0.018, which is smaller than the discernability level of 0.05, we reject the null hypothesis. The data provide convincing evidence that there is a difference between the proportions of those who think “The country needs to continue to make changes to give women equal rights to men”."
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-8",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-8",
    "title": "AE 16: Equality",
    "section": "Exercise 8",
    "text": "Exercise 8\nInterpret the p-value in the context of the data and the hypotheses.\nThe probability of observing a difference in sample proportions of those who think “The country needs to continue to make changes to give women equal rights to men” between a sample of 67 18-24 year olds and 661 25+ year olds of 0.158 or more (in either direction) is 0.018 if in fact the two population proportions are equal."
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-9",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-9",
    "title": "AE 16: Equality",
    "section": "Exercise 9",
    "text": "Exercise 9\nEstimate the difference in population proportions of 18-24 year old NC voters and 25+ year old NC voters using a 95% bootstrap interval.\n\nset.seed(1234)\n\nboot_dist &lt;- equality |&gt;\n  specify(response = response, explanatory = age, success = \"Need more changes\") |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"diff in props\", order = c(\"18-24\", \"25+\"))\n\nci &lt;- boot_dist |&gt;\n  get_ci()\nci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1   -0.288  -0.0279\n\nvisualize(boot_dist) +\n  shade_ci(ci)"
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-10",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-10",
    "title": "AE 16: Equality",
    "section": "Exercise 10",
    "text": "Exercise 10\nInterpret the confidence interval in context of the data.\nWe are 95% confident that the proportion of 18-24 year old NC voters who think “The country needs to continue to make changes to give women equal rights to men” is 28.4% to 3.4% lower than 25+ year old NC voters who share this opinion."
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-11",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-11",
    "title": "AE 16: Equality",
    "section": "Exercise 11",
    "text": "Exercise 11\nDescribe how the simulation scheme for bootstrapping is different than that for the hypothesis test.\nFor bootstrapping we resample with replacement. For testing, we shuffle under the assumption of independence."
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-12",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-12",
    "title": "AE 16: Equality",
    "section": "Exercise 12",
    "text": "Exercise 12\nWhat is \\(p\\) vs. \\(\\hat{p}\\) vs. p-value. Explain generically as well as in the context of these data and research question.\nAdd response here."
  },
  {
    "objectID": "ae/ae-16-equality-randomization-A.html#exercise-13",
    "href": "ae/ae-16-equality-randomization-A.html#exercise-13",
    "title": "AE 16: Equality",
    "section": "Exercise 13",
    "text": "Exercise 13\nWhat is bootstrap distribution vs. null distribution? Explain generically as well as in the context of these data and research question.\nAdd response here."
  },
  {
    "objectID": "ae/ae-12-modeling-penguins-multi.html",
    "href": "ae/ae-12-modeling-penguins-multi.html",
    "title": "AE 12: Modeling penguins with multiple predictors",
    "section": "",
    "text": "In this application exercise we will be studying penguins. The data can be found in the palmerpenguins package and we will use tidyverse and tidymodels for data exploration and modeling, respectively.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nPlease read the following context and take a glimpse at the data set before we get started.\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\nOur goal is to understand better how various body measurements and attributes of penguins relate to their body mass."
  },
  {
    "objectID": "ae/ae-12-modeling-penguins-multi.html#additive-vs.-interaction-models",
    "href": "ae/ae-12-modeling-penguins-multi.html#additive-vs.-interaction-models",
    "title": "AE 12: Modeling penguins with multiple predictors",
    "section": "Additive vs. interaction models",
    "text": "Additive vs. interaction models\n\nYour turn: Run the two chunks of code below and create two separate plots. How are the two plots different than each other? Which plot does the model we fit above represent?\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\nRemoved 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nAdd response here.\n\nYour turn: Interpret the slope coefficient for flipper length in the context of the data and the research question.\n\nAdd response here.\n\nDemo: Predict the body mass of a Dream island penguin with a flipper length of 200 mm.\n\n\n# add code here\n\n\nReview: Look back at Plot B. What assumption does the additive model make about the slopes between flipper length and body mass for each of the three islands?\n\nThe additive model assumes the same slope between body mass and flipper length for all three islands.\n\nDemo: Now fit the interaction model represented in Plot A and write the estimated regression model.\n\n\n# add code here\n\n\\[\nadd~math~text~here\n\\]\n\nReview: What does modeling body mass with an interaction effect get us that without doing so does not?\n\nThe interaction effect allows us to model the rate of change in estimated body mass as flipper length increases as different in the three islands.\n\nYour turn: Predict the body mass of a Dream island penguin with a flipper length of 200 mm.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-12-modeling-penguins-multi.html#choosing-a-model",
    "href": "ae/ae-12-modeling-penguins-multi.html#choosing-a-model",
    "title": "AE 12: Modeling penguins with multiple predictors",
    "section": "Choosing a model",
    "text": "Choosing a model\nRule of thumb: Occam’s Razor - Don’t overcomplicate the situation! We prefer the simplest best model.\n\n# add code here\n\n\nReview: What is R-squared? What is adjusted R-squared?\n\nR-squared is the percent variability in the response that is explained by our model. (Can use when models have same number of variables for model selection)\nAdjusted R-squared is similar, but has a penalty for the number of variables in the model. (Should use for model selection when models have different numbers of variables)."
  },
  {
    "objectID": "ae/ae-10-modeling-fish.html",
    "href": "ae/ae-10-modeling-fish.html",
    "title": "Modelling fish",
    "section": "",
    "text": "For this application exercise, we will work with data on fish. The dataset we will use, called fish, is on two common fish species in fish market sales.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nfish &lt;- read_csv(\"data/fish.csv\")\n\nThe data dictionary is below:\n\n\n\nvariable\ndescription\n\n\n\n\nspecies\nSpecies name of fish\n\n\nweight\nWeight, in grams\n\n\nlength_vertical\nVertical length, in cm\n\n\nlength_diagonal\nDiagonal length, in cm\n\n\nlength_cross\nCross length, in cm\n\n\nheight\nHeight, in cm\n\n\nwidth\nDiagonal width, in cm\n\n\n\n\nVisualizing the model\nWe’re going to investigate the relationship between the weights and heights of fish.\n\nDemo: Create an appropriate plot to investigate this relationship. Add appropriate labels to the plot.\n\n\n# add code here\n\n\nYour turn (5 minutes):\n\nIf you were to draw a a straight line to best represent the relationship between the heights and weights of fish, where would it go? Why?\nAdd response here.\nNow, let R draw the line for you. Refer to the documentation at https://ggplot2.tidyverse.org/reference/geom_smooth.html. Specifically, refer to the method section.\n\n\n# add code here\n\n\nWhat types of questions can this plot help answer?\n\nAdd response here.\nYour turn (3 minutes):\n\nWe can use this line to make predictions. Predict what you think the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm. Which prediction is considered extrapolation?\n\nAdd response here.\n\nWhat is a residual?\n\nAdd response here.\n\n\n\nModel fitting\n\nDemo: Fit a model to predict fish weights from their heights.\n\n\n# add code here\n\n\nYour turn (3 minutes): Predict what the weight of a fish would be with a height of 10 cm, 15 cm, and 20 cm using this model.\n\n\n# add code here\n\n\nDemo: Calculate predicted weights for all fish in the data and visualize the residuals under this model.\n\n\n# add code here\n\n\n\nModel summary\n\nDemo: Display the model summary including estimates for the slope and intercept along with measurements of uncertainty around them. Show how you can extract these values from the model output.\n\n\n# add code here\n\n\nDemo: Write out your model using mathematical notation.\n\nAdd response here.\n\n\nCorrelation\nWe can also assess correlation between two quantitative variables.\n\nYour turn (5 minutes):\n\nWhat is correlation? What are values correlation can take?\n\nAdd response here.\n\nAre you good at guessing correlation? Give it a try! https://www.rossmanchance.com/applets/2021/guesscorrelation/GuessCorrelation.html\n\nDemo: What is the correlation between heights and weights of fish?\n\n\n# add code here\n\n\n\nAdding a third variable\n\nDemo: Does the relationship between heights and weights of fish change if we take into consideration species? Plot two separate straight lines for the Bream and Roach species.\n\n\n# add code here\n\n\n\nFitting other models\n\nDemo: We can fit more models than just a straight line. Change the following code below to read method = \"loess\". What is different from the plot created before?\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-05-majors-tidying.html",
    "href": "ae/ae-05-majors-tidying.html",
    "title": "AE 05: Tidying StatSci Majors",
    "section": "",
    "text": "Our ultimate goal in this application exercise is to make the following data visualization."
  },
  {
    "objectID": "ae/ae-05-majors-tidying.html#goal",
    "href": "ae/ae-05-majors-tidying.html#goal",
    "title": "AE 05: Tidying StatSci Majors",
    "section": "",
    "text": "Our ultimate goal in this application exercise is to make the following data visualization."
  },
  {
    "objectID": "ae/ae-05-majors-tidying.html#data",
    "href": "ae/ae-05-majors-tidying.html#data",
    "title": "AE 05: Tidying StatSci Majors",
    "section": "Data",
    "text": "Data\nThe data come from the Office of the University Registrar. They make the data available as a table that you can download as a PDF, but I’ve put the data exported in a CSV file for you. Let’s load that in.\n\nlibrary(tidyverse)\n\nstatsci &lt;- read_csv(\"https://sta199-s24.github.io/data/statsci.csv\")\n\nAnd let’s take a look at the data.\n\nstatsci\n\n# A tibble: 4 × 14\n  degree   `2011` `2012` `2013` `2014` `2015` `2016` `2017` `2018` `2019` `2020`\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Statist…     NA      1     NA     NA      4      4      1     NA     NA      1\n2 Statist…      2      2      4      1      3      6      3      4      4      1\n3 Statist…      2      6      1     NA      5      6      6      8      8     17\n4 Statist…      5      9      4     13     10     17     24     21     26     27\n# ℹ 3 more variables: `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;, `2023` &lt;dbl&gt;"
  },
  {
    "objectID": "ae/ae-05-majors-tidying.html#pivoting",
    "href": "ae/ae-05-majors-tidying.html#pivoting",
    "title": "AE 05: Tidying StatSci Majors",
    "section": "Pivoting",
    "text": "Pivoting\n\nDemo: Pivot the statsci data frame longer such that each row represents a degree type / year combination and year and number of graduates for that year are columns in the data frame.\n\n\n# add your code here\n\n\nQuestion: What is the type of the year variable? Why? What should it be?\n\nAdd your response here.\n\nDemo: Start over with pivoting, and this time also make sure year is a numerical variable in the resulting data frame.\n\n\n# add your code here\n\n\nQuestion: What does an NA mean in this context? Hint: The data come from the university registrar, and they have records on every single graduates, there shouldn’t be anything “unknown” to them about who graduated when.\n\nAdd your response here.\n\nDemo: Add on to your pipeline that you started with pivoting and convert NAs in n to 0s.\n\n\n# add your code here\n\n\nDemo: In our plot the degree types are BS, BS2, AB, and AB2. This information is in our dataset, in the degree column, but this column also has additional characters we don’t need. Create a new column called degree_type with levels BS, BS2, AB, and AB2 (in this order) based on degree. Do this by adding on to your pipeline from earlier.\n\n\n# add your code here\n\n\nYour turn: Now we start making our plot, but let’s not get too fancy right away. Create the following plot, which will serve as the “first draft” on the way to our Goal. Do this by adding on to your pipeline from earlier.\n\n\n\n\n\n\n\n# add your code here\n\n\nYour turn: What aspects of the plot need to be updated to go from the draft you created above to the Goal plot at the beginning of this application exercise.\n\nAdd your response here.\n\nDemo: Update x-axis scale such that the years displayed go from 2011 to 2023 in increments of 2 years. Do this by adding on to your pipeline from earlier.\n\n\n# add your code here\n\n\nDemo: Update line colors using the following level / color assignments. Once again, do this by adding on to your pipeline from earlier.\n\n“BS” = “cadetblue4”\n“BS2” = “cadetblue3”\n“AB” = “lightgoldenrod4”\n“AB2” = “lightgoldenrod3”\n\n\n\n# add your code here\n\n\nYour turn: Update the plot labels (title, subtitle, x, y, and caption) and use theme_minimal(). Once again, do this by adding on to your pipeline from earlier.\n\n\n# add your code here\n\n\nDemo: Finally, adding to your pipeline you’ve developed so far, move the legend into the plot, make its background white, and its border gray. Set fig-width: 7 and fig-height: 5 for your plot in the chunk options.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-19-last-ae.html",
    "href": "ae/ae-19-last-ae.html",
    "title": "Last AE",
    "section": "",
    "text": "Make a change, any change, to this document. Render, commit, and push by Sunday evening."
  },
  {
    "objectID": "ae/ae-09-chronicle-scrape.html",
    "href": "ae/ae-09-chronicle-scrape.html",
    "title": "AE 09: Opinion articles in The Chronicle",
    "section": "",
    "text": "This will be done in the chronicle-scrape.R R script. Save the resulting data frame in the data folder."
  },
  {
    "objectID": "ae/ae-09-chronicle-scrape.html#part-1---data-scraping",
    "href": "ae/ae-09-chronicle-scrape.html#part-1---data-scraping",
    "title": "AE 09: Opinion articles in The Chronicle",
    "section": "",
    "text": "This will be done in the chronicle-scrape.R R script. Save the resulting data frame in the data folder."
  },
  {
    "objectID": "ae/ae-09-chronicle-scrape.html#part-2---data-analysis",
    "href": "ae/ae-09-chronicle-scrape.html#part-2---data-analysis",
    "title": "AE 09: Opinion articles in The Chronicle",
    "section": "Part 2 - Data analysis",
    "text": "Part 2 - Data analysis\nLet’s start by loading the packages we will need:\n\nlibrary(tidyverse)\n\n\nYour turn (1 minute): Load the data you saved into the data folder and name it chronicle.\n\n\n# add code here\n\n\nYour turn (3 minutes): Who are the most prolific authors of the 100 most recent opinion articles in The Chronicle?\n\n\n# add code here\n\n\nDemo: Draw a line plot of the number of opinion articles published per day in The Chronicle.\n\n\n# add code here\n\n\nDemo: What percent of the most recent 100 opinion articles in The Chronicle mention “climate” in their title?\n\n\n# add code here\n\n\nYour turn (5 minutes): What percent of the most recent 100 opinion articles in The Chronicle mention “climate” in their title or abstract?\n\n\n# add code here\n\n\nTime permitting: Come up with another question and try to answer it using the data.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-07-population-types.html",
    "href": "ae/ae-07-population-types.html",
    "title": "AE 07: Types and classes and populations",
    "section": "",
    "text": "We will use the following two packages in this application exercise.\n\ntidyverse: For data import, wrangling, and visualization.\nscales: For better axis labels.\n\n\nlibrary(tidyverse)\nlibrary(scales)"
  },
  {
    "objectID": "ae/ae-07-population-types.html#packages",
    "href": "ae/ae-07-population-types.html#packages",
    "title": "AE 07: Types and classes and populations",
    "section": "",
    "text": "We will use the following two packages in this application exercise.\n\ntidyverse: For data import, wrangling, and visualization.\nscales: For better axis labels.\n\n\nlibrary(tidyverse)\nlibrary(scales)"
  },
  {
    "objectID": "ae/ae-07-population-types.html#type-coercion",
    "href": "ae/ae-07-population-types.html#type-coercion",
    "title": "AE 07: Types and classes and populations",
    "section": "Type coercion",
    "text": "Type coercion\n\nDemo: Determine the type of the following vector. And then, change the type to numeric.\n\n\nx &lt;- c(\"1\", \"2\", \"3\")\n\n# add code here\n\n\nDemo: Once again, determine the type of the following vector. And then, change the type to numeric. What’s different than the previous exercise?\n\n\ny &lt;- c(\"a\", \"b\", \"c\")\n\n# add code here\n\n\nDemo: Once again, determine the type of the following vector. And then, change the type to numeric. What’s different than the previous exercise?\n\n\nz &lt;- c(\"1\", \"2\", \"three\")\n\n# add code here\n\n\nDemo: Suppose you conducted a survey where you asked people how many cars their household owns collectively. And the answers are as follows:\n\n\nsurvey_results &lt;- tibble(cars = c(1, 2, \"three\"))\nsurvey_results\n\n# A tibble: 3 × 1\n  cars \n  &lt;chr&gt;\n1 1    \n2 2    \n3 three\n\n\nThis is annoying because of that third survey taker who just had to go and type out the number instead of providing as a numeric value. So now you need to update the cars variable to be numeric. You do the following:\n\n# add code here\n\nAnd now things are even more annoying because you get a warning NAs introduced by coercion that happened while computing cars = as.numeric(cars) and the response from the third survey taker is now an NA (you lost their data). Fix your mutate() call to avoid this warning.\n\n# add code here\n\n\nYour turn: First, guess the type of the vector. Then, check if you guessed right. I’ve done the first one for you, you’ll see that it’s helpful to check the type of each element of the vector first.\n\nc(1, 1L, \"C\")\n\n\n\nv1 &lt;- c(1, 1L, \"C\")\n\n# to help you guess\ntypeof(1)\n\n[1] \"double\"\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(\"C\")\n\n[1] \"character\"\n\n# to check after you guess\ntypeof(v1)\n\n[1] \"character\"\n\n\n-   `c(1L / 0, \"A\")`\n\nv2 &lt;- c(1L / 0, \"A\")\n\n# to help you guess\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(0)\n\n[1] \"double\"\n\ntypeof(1L / 0)\n\n[1] \"double\"\n\ntypeof(\"A\")\n\n[1] \"character\"\n\n# to check after you guess\ntypeof(v2)\n\n[1] \"character\"\n\n\n- `c(1:3, 5)`\n\nv3 &lt;- c(1:3, 5)\n\n# to help you guess\n\n# add code here\n\n# to check after you guess\n\n# add code here\n\n-   `c(3, \"3+\")`\n\nv4 &lt;- c(3, \"3+\")\n\n# to help you guess\n\n# add code here\n\n# to check after you guess\n\n# add code here\n\n-   `c(NA, TRUE)`\n\nv5 &lt;- c(NA, TRUE)\n\n# to help you guess\n\n# add code here\n\n# to check after you guess\n\n# add code here"
  },
  {
    "objectID": "ae/ae-07-population-types.html#populations-in-continents",
    "href": "ae/ae-07-population-types.html#populations-in-continents",
    "title": "AE 07: Types and classes and populations",
    "section": "Populations in continents",
    "text": "Populations in continents\nIn the previous application exercise you joined two datasets (after a bit of data cleaning), and calculated total population in each continent and visualized it.\n\nFirst, you loaded the data:\n\n\ncontinents &lt;- read_csv(\"https://sta199-s24.github.io/data/continents.csv\")\n\nRows: 285 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): entity, code, continent\ndbl (1): year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npopulation &lt;- read_csv(\"https://sta199-s24.github.io/data/world-pop-2022.csv\")\n\nRows: 217 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): year, population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nThen you cleaned the country names where the spelling in one data frame didn’t match the other, and joined the data sets:\n\n\npopulation_continent &lt;- population |&gt;\n  mutate(country = case_when(\n    country == \"Congo, Dem. Rep.\" ~ \"Democratic Republic of Congo\",\n    country == \"Congo, Rep.\" ~ \"Congo\",\n    country == \"Hong Kong SAR, China\" ~ \"Hong Kong\",\n    country == \"Korea, Dem. People's Rep.\" ~ \"North Korea\",\n    country == \"Korea, Rep.\" ~ \"South Korea\",\n    country == \"Kyrgyz Republic\" ~ \"Kyrgyzstan\",\n    .default = country\n    )\n  ) |&gt;\n  left_join(continents, by = join_by(country == entity))\n\n\nThen, you calculated total population for each continent.\n\n\npopulation_summary &lt;- population_continent |&gt;\n  group_by(continent) |&gt;\n  summarize(total_pop = sum(population)) |&gt;\n  arrange(desc(total_pop))\n\n\nAnd finally, you visualized these data.\n\n\nggplot(population_summary) +\n  geom_point(aes(x = total_pop, y = continent)) +\n  geom_segment(aes(y = continent, yend = continent, x = 0, xend = total_pop)) +\n  scale_x_continuous(labels = label_number(scale = 1/1000000, suffix = \" bil\")) +\n  theme_minimal() +\n  labs(\n    x = \"Total population\",\n    y = \"Continent\",\n    title = \"World population\",\n    subtitle = \"As of 2022\",\n    caption = \"Data sources: The World Bank and Our World in Data\"\n  )\n\n\n\n\n\n\n\n\n\nQuestion: Take a look at the visualization. How are the continents ordered? What would be a better order?\n\nAdd answer here.\n\nDemo: Reorder the continents on the y-axis (levels of continent) in order of value of total population. You will want to use a function from the forcats package, see https://forcats.tidyverse.org/reference/index.html for inspiration and help.\n\n\n# add code here\n\n\nThink out loud: Describe what is happening in the each step of the code chunk above.\n\nAdd answer here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 246: Intermediate Statistics",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\nWEEK\nDATE\nPREPARE\nTOPIC\nMATERIALS\nDUE OR ASSIGN\n\n\n\n\n1\nMon, Aug 26\n\n\n\nIntroductions & Syllabus\n\n🖥️ Welcome slides \n\nQuiz 1 (take-home syllabus quiz)\n\n\n\n\n\nWed, Aug 28\n\n\n\nR(e)-Introduction to Statistics:\nLab 0:  R & RStudio - Base R basics, R packages\n\n\n\n\nAssign Problem Set 1\n\n\n\n\n\nFri, Aug 30\n\n\n\nR(e)-Intro to stats cont.\nLab 1:  Reproducible reports: quarto  Tidyverse workflow & ggplot2 (data viz.)\n\n\n\n\nSyllabus Quiz due (11.59 pm)\n\n\n\n2\nMon, Sep 2\n\n\n\nNo class - Labor Day holiday\n\n\n\n\n\n\n\n\n\nWed, Sep 4\n📘 ims - chp 4  📘 ims - chp 5  📗 r4ds - chp 1  🎥 Visualising data with ggplot2 🎥 Visualizing numerical data  🎥 Visualizing categorical data\n\nR(e)-Intro to stats cont. Exploratory Data Analysis: Categorical and Numerical data\n\n\n\n\nLab 1 due: 11.59 pm\n\n\n\n\n\nFri, Sep 6\n📘 ims - chp 7.1 \n\nQuiz 2 \nRegression modelling (single predictor):  aka Simple Linear Regression (SLR)\n\n🖥️ SLR slides \n\nQuiz 2 in class\nProblem Set 1 due\n\n\n\n3\nMon, Sep 9\n📘 ims - chp 7.2-7.3 \n\nSLR Modeling\n\n\n\n\nAssign PS2\n\n\n\n\n\nWed, Sep 11\n\n\n\nSLR Modeling\n\n\n\n\n\n\n\n\n\nFri, Sep 13\n\n\n\nLab 2: SLR Modelling\nGitHub (Student developer pack) \nIntroducing GitHub Copilot**\n\n\n\n\nAssign Project 1\n\n\n\n4\nMon, Sep 16\n📘 ims - chp 8.1  📘 ims - chp 8.2 \n\nRegression Modeling with multiple predictors  Multiple Linear Regressions (MLR)\n\n🖥️ MLR slides \n\nLab 2 due at 11.59 pm\n\n\n\n\n\nWed, Sep 18\n📘 ims - chp 8.3  📘 ims - chp 8.4 \n\nMLR Modeling\n\n\n\n\nPS2 due at 11.59 pm\nAssign PS3\n\n\n\n\n\nFri, Sep 20\n\n\n\nShort Quiz 3 \nLab 3: MLR Modelling\n\n\n\n\nQuiz 3 in class\n\n\n\n5\nMon, Sep 23\n📘 ims - chp 9.1 & 9.2  🎥 Logistic regression \n\nRegression Modeling (Binary outcomes)  aka Logistic Regression\n\n\n\n\nLab 3 due at 11.59 pm\nPS2 due at 11.59 pm\nProject Proposal due at 11.59 pm\n\n\n\n\n\nWed, Sep 25\n📘 ims - chp 9.3 \n\nLogistic Regression Modeling\n\n\n\n\n\n\n\n\n\nFri, Sep 27\n\n\n\nQuiz 4 \nLab 4:  Regression Modelling (Binary Outcomes)\n\n\n\n\nQuiz 4 in class\nProject 1 draft due at 11.59 pm.\n\n\n\n6\nMon, Sep 30\n📘 ims - chp 11 \n\nIntro to Statistical Inference:  Hypothesis testing via randomization\n\n\n\n\nPS3 due at 11.59 pm\n\n\n\n\n\nWed, Oct 2\n📘 ims - chp 12  🎥 Bootstrapping \n\nIntro to Statistical Inference cont.: Confidence intervals via bootstrapping\n\n\n\n\n\n\n\n\n\nFri, Oct 4\n\n\n\nQuiz 5 \nLab 5: Statistical Inference: the infer package\n\n\n\n\nQuiz 5 in class\nProject peer review due at 11.59 pm\n\n\n\n7\nMon, Oct 7\n\n\n\nProject Time (in-class)\n\n\n\n\n\n\n\n\n\nWed, Oct 9\n\n\n\nProject Time (in-class)\n\n\n\n\n\n\n\n\n\nFri, Oct 11\n\n\n\nExam 1 Review\n\n\n\n\n\n\n\n8\nMon, Oct 14\n\n\n\nExam 1 (in-class) \nTake-home exam released\n\n\n\n\n\n\n\n\n\nWed, Oct 16\n\n\n\nProject Presentations (possibly online)\n\n\n\n\nProject presentation due at 11.59 pm\n\n\n\n\n\nFri, Oct 18\n\n\n\n🌴 No classes - Fall Break\n\n\n\n\nFinal project write-up due:  Submit both source file (qmd) and rendered pdf.\n\n\n\n9\nMon, Oct 21\n📘 ims - chp 13.1-13.3\n\nStatistical Inference: Mathematical models\n\n\n\n\nAssign PS4\n\n\n\n\n\nWed, Oct 23\n📘 ims - chp 14\n\nStatistical Inference: Mathematical models\nDecision Errors\n\n\n\n\n\n\n\n\n\nFri, Oct 25\n\n\n\nQuiz 6 \nLab 6:  Inference with mathematical models (infer package)\n\n\n\n\nQuiz 6 in class\n\n\n\n10\nMon, Oct 28\n📘 ims - chp 24.1 - 24.3\n\nInferential modelling:  Inf. for linear regression with a single predictor\n\n\n\n\n\n\n\n\n\nWed, Oct 30\n📘 ims - chp 24.4 - 24.6\n\nInferential modelling: Inf. for linear regression with a single predictor\n\n\n\n\n\n\n\n\n\nFri, Nov 1\n\n\n\nQuiz 7:\nLab 7:  Inf. for linear regression (single predictor)\n\n\n\n\nQuiz 7 in class\n\n\n\n11\nMon, Nov 4\n📘 ims - chp 25.1 - 25.2\n\nInferential modelling: Inf. for linear regression (multiple predictors)\n\n\n\n\nPS4 due at 11.59 pm.\nAssign final project\n\n\n\n\n\nWed, Nov 6\n📘 ims - chp 25.3\n\nInferential modelling:  Inf. for linear regression (multiple predictors)\n\n\n\n\n\n\n\n\n\nFri, Nov 8\n\n\n\nQuiz 8\nLab 8:  Inferential modelling: Inf. for linear regression with multiple predictors\n\n\n\n\nQuiz 8 in class\n\n\n\n12\nMon, Nov 11\n📘 ims - chp 26.1\n\nInferential modelling:  Inf. for logistic regression\n\n\n\n\nProject proposal due\n\n\n\n\n\nWed, Nov 13\n📘 ims - chp 26.3\n\nInferential modelling:  Inf. for logistic regression\n\n\n\n\n\n\n\n\n\nFri, Nov 15\n\n\n\nQuiz 9\nLab 9:  Inferential modelling: Inf. for logistic regression\n\n\n\n\nQuiz 9 in class\n\n\n\n13\nMon, Nov 18\n\n\n\nExam 2 Review session\n\n\n\n\nFinal project draft due\n\n\n\n\n\nWed, Nov 20\n\n\n\nExam 2 (in-class) \nTake-home exam released\n\n\n\n\n\n\n\n\n\nFri, Nov 22\n\n\n\nStudent check-ins\n\n\n\n\nPeer Review due at 11.59 pm\n\n\n\n14\nMon, Nov 25\n\n\n\n🌴 No classes - Thanksgiving Break\n\n\n\n\n\n\n\n\n\nWed, Nov 27\n\n\n\n🌴 No classes - Thanksgiving Break\n\n\n\n\n\n\n\n\n\nFri, Nov 29\n\n\n\n\n\n\n\n\n\n\n15\nMon, Dec 2\n\n\n\nProject Time (in-class)\n\n\n\n\n\n\n\n\n\nWed, Dec 4\n\n\n\nProject Time (in-class)\n\n\n\n\n\n\n\n\n\nFri, Dec 6\n\n\n\nProject presentations\n\n\n\n\n\n\n\n16\nMon, Dec 9\n\n\n\nFinal portfolio check ins\n\n\n\n\nFinal project due at 11.59 pm\n\n\n\n\n\nWed, Dec 11\n\n\n\nFinal portfolio check ins\n\n\n\n\n\n\n\n\n\nThu, Dec 12\n\n\n\n\n\n\n\nFinal portfolio due at 10 pm",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "exam-review/exam-2-review.html",
    "href": "exam-review/exam-2-review.html",
    "title": "Exam 2 Review",
    "section": "",
    "text": "Come back later!!",
    "crumbs": [
      "Exam review",
      "Exam 2"
    ]
  },
  {
    "objectID": "exam-review/exam-1-review-A.html",
    "href": "exam-review/exam-1-review-A.html",
    "title": "Exam 1 Review",
    "section": "",
    "text": "b, c, f, g -\n\nThe blizzard_salary dataset has 409 rows.\nThe percent_incr variable is numerical and continuous.\nThe salary_type variable is categorical.\n\nFigure 1 - A shared x-axis makes it easier to compare summary statistics for the variable on the x-axis.\nc - It’s a value higher than the median for hourly but lower than the mean for salaried.\nb - There is more variability around the mean compared to the hourly distribution.\na, b, e - Pie charts and waffle charts are for visualizing distributions of categorical data only. Scatterplots are for visualizing the relationship between two numerical variables.\nc - mutate() is used to create or modify a variable.\na - \"Poor\", \"Successful\", \"High\", \"Top\"\nb - Option 2. The plot in Option 1 shows the number of employees with a given performance rating for each salary type while the plot in Option 2 gives the proportion of employees with a given performance rating for each salary type. In order to assess the relationship between these variables (e.g., how much more likely is a Top rating among Salaried vs. Hourly workers), we need the proportions, not the counts.\nThere may be some NAs in these two variables that are not visible in the plot.\nThe proportions under Hourly would go in the Hourly bar, and those under Salaried would go in the Salaried bar.\nc - filter(salary_type != \"Hourly\" & performance_rating == \"Poor\") - There are 5 observations for “not Hourly” “and” Poor.\na - arrange() - The result is arranged in increasing order of annual_salary, which is the default for arrange().\nc, d, e, f.\nPart 1: The following should be fixed:\n\nThere should be a | after # before label\nThere should be a : after label, not =\nThere shouldn’t be a space in the chunk label, it should be plot-blizzard\nThere should be spaces after commas in the code\nThere should be spaces on both sides of = in the code\nThere should be a space before +\ngeom_boxplot() should be on the next line and indented\nThere should be a + at the end of the geom_boxplot() line\nlabs() should be indented\n\nPart 2: The warning is caused by NA in the data. It means that 39 observations were NAs and are not plotted/represented on the plot.\nPart 1:\n\nRender: Run all of the code and render all of the text in the document and produce an output.\nCommit: Take a snapshot of your changes in Git with an appropriate message.\nPush: Send your changes off to GitHub.\n\nPart 2: c - Rendering or committing isn’t sufficient to send your changes to your GitHub repository, a push is needed. A pull is also not needed to view the changes in the browser."
  },
  {
    "objectID": "slides/mlr.html#but-why-mlr",
    "href": "slides/mlr.html#but-why-mlr",
    "title": "Multiple Linear Regression",
    "section": "But, why MLR??",
    "text": "But, why MLR??\n\nIn the previous chapter, we learned about the techniques for modeling the relationship between two variables (a predictor and an outcome variable).\nSpecifically, we saw that linear regression provides powerful tools for predicting the values of an outcome variable given one predictor variable (numerical or categorical).\nIn real life situations, however, variables are related in a much more complex manner.\nFor example, assuming that the price of a house depends on the school district only may mask the influence of other variables such as the size of the house, the number of bedrooms, the location, and the year it was built, among others."
  },
  {
    "objectID": "slides/mlr.html#structure-of-mlr-models",
    "href": "slides/mlr.html#structure-of-mlr-models",
    "title": "Multiple Linear Regression",
    "section": "Structure of MLR models",
    "text": "Structure of MLR models\n\nWhen you are modelling connection between one predictor variable and one outcome variable, the equation takes the form of a line: \\(\\widehat{Outcome} = \\beta_0 + \\beta_1\\times predictor\\).\nThis can also be written as \\(\\widehat{y} = \\beta_0 + \\beta_1x\\).\nIn MLR, the equation takes the form of a plane: \\[\\widehat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2+ ... \\beta_n x_n.\\]\nUnlike the SLR model (single predictor variable), this one has \\(n\\) variables and \\(n\\) slopes (\\(n\\) can be any whole number greater than 1). Each of these slopes represent the weights of the corresponding variables.\nWhile it was possible to draw a scatterplot to visualize the relationship in the case of a single variable, doing so for multiple regression is often complicated due to the many variables (requires 3 or more dimensions).\nLike the SLR models, MLR models have one intercept."
  },
  {
    "objectID": "slides/mlr.html#example",
    "href": "slides/mlr.html#example",
    "title": "Multiple Linear Regression",
    "section": "Example",
    "text": "Example\nWhen you apply for a loan, the bank determines your interest rate based on several factors such as credit score, loan_type, credit checks previous default rate, credit utility, bankruptcy records, income, length of the loan, among others. You can think of these factors as predictor variables and the interest rate as an outcome variable.\n\nNote that in MLR, the outcome variable must be numerical but the predictor variables do not have to be. In fact, you can have a mix of numerical and categorical variables in the same multiple regression model.\nFor simplicity, let’s consider a model with two predictor variables- default rate, and credit score.\nGiven data from several individuals on the above variables, we can use software to generate the multiple regression model (the equation) &gt;&gt;"
  },
  {
    "objectID": "slides/mlr.html#cont",
    "href": "slides/mlr.html#cont",
    "title": "Multiple Linear Regression",
    "section": "Cont…",
    "text": "Cont…\nSuppose we come up with the following model:\n\\[ \\widehat{Rate} = 12.1+2.34\\times \\text{default_Rate}-.00013\\times\\text{Credit_Score}\\]\nInterpretation\n\nThe intercept (12.1) represents the base interest rate. On average, individuals with no default rate and no credit score would get an interest rate of 12.1%.\nThe slope for Credit_Score(-0.00013) indicates that for individuals with the same default rate (i.e., default rate kept constant), interest rate decreases by 0.00013% on average for every additional unit in default rate.\nThe slope for Default_Rate (2.34) indicates that for individuals with the same credit score (i.e., credit score kept constant), interest rate increases by 2.34% on average for every additional unit in default rate."
  },
  {
    "objectID": "slides/mlr.html#cont-1",
    "href": "slides/mlr.html#cont-1",
    "title": "Multiple Linear Regression",
    "section": "Cont…",
    "text": "Cont…\nWe want to use the above model to predict the interest rate for an individual with a default rate of 0.5 and a credit score of 714.\n\nPlug in the values of the predictor variables into the equation and solve for the outcome variable (interest rate).\n\n\\[\\begin{aligned}\n\\widehat{Rate} &= 12.1 - (.00013\\times 714)+(2.34\\times 0.5)\\\\\n               &= 12.1+ .09282 - 1.17\\\\\n               &= 13.18\\%\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/mlr.html#categorical-variables-in-mlr",
    "href": "slides/mlr.html#categorical-variables-in-mlr",
    "title": "Multiple Linear Regression",
    "section": "Categorical Variables in MLR",
    "text": "Categorical Variables in MLR\n\nWe saw in the case of regression with a single predictor that when you have a categorical variable with two levels (e.g., yes and no) we convert the variable into indicator variable by assigning one level a 1 and the other a 0. These numerical values are then used to compute the regression model.\nIn the case of a categorical variable with, say, 3 levels, one of the levels is chosen and is assigned zero while the other two are treated as separate predictor variables.\nThe level that is assigned a zero is called the reference level.\nGenerally, if a categorical variable has p levels, the model shows p−1 indicator variables."
  },
  {
    "objectID": "slides/mlr.html#example-1",
    "href": "slides/mlr.html#example-1",
    "title": "Multiple Linear Regression",
    "section": "Example",
    "text": "Example\nSuppose we decide to add the categorical variable loan_type into our model in the previous example. Suppose also that loan_type has three levels (i.e., personal, auto, home). If the level “personal” is chosen as the reference level, the new model would look something like this:\n\\[\\begin{aligned}\n\\widehat{Rate} &= 11.2 - .00019\\times \\text{Credit_Score}\\\\\n                  & +(2.09\\times \\text{Default_Rate})\\\\\n                  & + 0.511\\times \\text{Loan_type}\\hspace{.1cm}_{Auto}\\\\\n                  & + 2.3\\times \\text{Loan_type}\\hspace{.1cm}_{Home}\\\\\n\\end{aligned}\\]\n\n\n\n\n\n\nWarning\n\n\n\nThe intercept and the slope in the SLR model may change upon adding a new variable(s) to get a MLR model. This should be expected!\nThe level “personal” is not included in the model."
  },
  {
    "objectID": "slides/mlr.html#interpreting-categorical-variables",
    "href": "slides/mlr.html#interpreting-categorical-variables",
    "title": "Multiple Linear Regression",
    "section": "Interpreting categorical variables",
    "text": "Interpreting categorical variables\n\nThe model intercept and slopes for the numerical variables are interpreted as before.\nThe slope for loan_type_auto means that when all other variables are held constant, the average interest rate for auto loans is 0.511% higher on average than the interest rate for personal loans.\nThe slope for loan_type_home means that when all other variables are held constant, the interest rate for an home loans is 2.34% lower on average than the interest rate for personal loans."
  },
  {
    "objectID": "slides/mlr.html#describing-the-strength-of-fit-in-mlr",
    "href": "slides/mlr.html#describing-the-strength-of-fit-in-mlr",
    "title": "Multiple Linear Regression",
    "section": "Describing the Strength of Fit in MLR",
    "text": "Describing the Strength of Fit in MLR\n\nIn SLR, we used the coefficient of determination (\\(R^2\\)) to describe the strength of the relationship between the predictor and the outcome variable.\nIn MLR, we use the adjusted \\(R^2\\) which is a modified version of \\(R^2\\) that has been adjusted for the number of predictors in the model. We use the formula:\n\n\\[Adjusted\\hspace{.1cm}R^2 = 1 - \\frac{s^2_{residuals}}{s^2_{outcome}}\\times\\frac{n-1}{n-k}\\]\nWhere \\(n\\) is the number of observations and \\(k\\) is the number of predictor variables in the model.\n\n\n\n\n\n\nCaution!!\n\n\nRecall that a categorical variable with \\(p\\) levels is represented by \\(p-1\\) indicator variables. - A deeper discussion of the reasoning behind the formula will be done in class."
  },
  {
    "objectID": "slides/mlr.html#model-selection-in-mlr",
    "href": "slides/mlr.html#model-selection-in-mlr",
    "title": "Multiple Linear Regression",
    "section": "Model Selection in MLR",
    "text": "Model Selection in MLR\n\nThe best model is not always the most complicated. Sometimes including variables that are not evidently important can actually reduce the accuracy of predictions.\nA model with all potential predictors of a given outcome variable is called a full model.\nA parsimonious model is one that has undergone a “screening” process (model selection) that drops unimportant predictors.\nThere are two main types of model screening to obtain a parsimonious model. These are:\n\nForward selection\nBackward elimination"
  },
  {
    "objectID": "slides/mlr.html#cont-.-.-.",
    "href": "slides/mlr.html#cont-.-.-.",
    "title": "Multiple Linear Regression",
    "section": "Cont . . .",
    "text": "Cont . . .\n\nBackward elimination: starts with the full model (the model that includes all potential predictor variables and then eliminates predictors one-at-a-time from the model until we cannot improve the model any further.\nForward selection: the opposite of the backward elimination. It starts with the model that includes no predictors and then adds predictors one-at-a-time to the model until we cannot improve the model any further.\n\n\n\nIf your goal for model selection is to find a model with the best fit, you may want to choose a model with the highest adjusted \\(R^2\\). Other criteria that are commonly used include relying on expert opinion or using P-values (will be discussd in later sections)."
  },
  {
    "objectID": "slides/mlr.html#conditionsassumptions-for-mlr",
    "href": "slides/mlr.html#conditionsassumptions-for-mlr",
    "title": "Multiple Linear Regression",
    "section": "Conditions/Assumptions for MLR",
    "text": "Conditions/Assumptions for MLR\n\nBefore running an analysis to create a multiple regression model, certain conditions and assumptions need to be met. We often use computer software to run most of these checks. These checks are the same as those fro SLR but with a few additions.\nThe assumptions are:\n\nLinearity: The relationship between the predictor variables and the outcome variable is linear.\nIndependence: of observations (usually a matter of study design)\nNormality of residuals\nEqual Variance: aka homoscedasticity - the variance of the residuals is constant across all levels of the predictor variables.\nMulticollinearity: The predictor variables should not be highly correlated with each other."
  },
  {
    "objectID": "slides/mlr.html#cont-.-.-.-1",
    "href": "slides/mlr.html#cont-.-.-.-1",
    "title": "Multiple Linear Regression",
    "section": "Cont . . .",
    "text": "Cont . . .\nSome of these conditions (e.g., multicollinearity) are often difficult to prevent in practice especially when doing observational studies. It is possible, however, to control these when doing an experimental study.\n\nTo check for nearly normal residuals, we usually create a histogram of the residuals.\nTo check for homoscedasticity, we create plot for residual vs fitted/predicted values. A plot of residuals vs one variable doesn’t give a full picture.\nTo check normality of residuals, we may use residuals vs each predictor variable.\nTo check for multicollinearity, we may use a scatterplot/correlation matrix."
  },
  {
    "objectID": "slides/00-welcome-246.html#meet-the-prof.",
    "href": "slides/00-welcome-246.html#meet-the-prof.",
    "title": "Welcome to MATH 246",
    "section": "Meet the prof.",
    "text": "Meet the prof.\n\n\nDr. Joash Geteregechi\nAss. Professor of Mathematics\nOffice: Williams 311E"
  },
  {
    "objectID": "slides/00-welcome-246.html#meet-the-ta",
    "href": "slides/00-welcome-246.html#meet-the-ta",
    "title": "Welcome to MATH 246",
    "section": "Meet the TA",
    "text": "Meet the TA\n\n\nName: Earth Sonrod,\n\nHave image of Earth here"
  },
  {
    "objectID": "slides/00-welcome-246.html#meet-each-other",
    "href": "slides/00-welcome-246.html#meet-each-other",
    "title": "Welcome to MATH 246",
    "section": "Meet each other!",
    "text": "Meet each other!\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/00-welcome-246.html#meet-inter.-stats",
    "href": "slides/00-welcome-246.html#meet-inter.-stats",
    "title": "Welcome to MATH 246",
    "section": "Meet Inter. Stats",
    "text": "Meet Inter. Stats\n\nStatistics is an exciting discipline that draws useful insights from data.\nWe’re going to learn to do this using a statistical programming language called R – more on that later!\nThis is an intermediate statistics course, so the assumption is that you already know some basic concepts in statistics.\nThe first problem set will serve as a review of basic statistics concepts.\nDuring the semester, I will make efforts to remind you of the most important concepts that are needed in the course."
  },
  {
    "objectID": "slides/00-welcome-246.html#why-not-just-excel",
    "href": "slides/00-welcome-246.html#why-not-just-excel",
    "title": "Welcome to MATH 246",
    "section": "Why not just Excel?",
    "text": "Why not just Excel?"
  },
  {
    "objectID": "slides/00-welcome-246.html#why-r",
    "href": "slides/00-welcome-246.html#why-r",
    "title": "Welcome to MATH 246",
    "section": "Why R?",
    "text": "Why R?"
  },
  {
    "objectID": "slides/00-welcome-246.html#r-via-rstudio",
    "href": "slides/00-welcome-246.html#r-via-rstudio",
    "title": "Welcome to MATH 246",
    "section": "R via RStudio",
    "text": "R via RStudio"
  },
  {
    "objectID": "slides/00-welcome-246.html#data-science-life-cycle-1",
    "href": "slides/00-welcome-246.html#data-science-life-cycle-1",
    "title": "Welcome to MATH 246",
    "section": "Data science life cycle",
    "text": "Data science life cycle"
  },
  {
    "objectID": "slides/00-welcome-246.html#import",
    "href": "slides/00-welcome-246.html#import",
    "title": "Welcome to MATH 246",
    "section": "Import",
    "text": "Import"
  },
  {
    "objectID": "slides/00-welcome-246.html#tidy-transform",
    "href": "slides/00-welcome-246.html#tidy-transform",
    "title": "Welcome to MATH 246",
    "section": "Tidy + transform",
    "text": "Tidy + transform"
  },
  {
    "objectID": "slides/00-welcome-246.html#visualize",
    "href": "slides/00-welcome-246.html#visualize",
    "title": "Welcome to MATH 246",
    "section": "Visualize",
    "text": "Visualize"
  },
  {
    "objectID": "slides/00-welcome-246.html#model",
    "href": "slides/00-welcome-246.html#model",
    "title": "Welcome to MATH 246",
    "section": "Model",
    "text": "Model"
  },
  {
    "objectID": "slides/00-welcome-246.html#understand",
    "href": "slides/00-welcome-246.html#understand",
    "title": "Welcome to MATH 246",
    "section": "Understand",
    "text": "Understand"
  },
  {
    "objectID": "slides/00-welcome-246.html#real-world-example",
    "href": "slides/00-welcome-246.html#real-world-example",
    "title": "Welcome to MATH 246",
    "section": "Real World Example",
    "text": "Real World Example"
  },
  {
    "objectID": "slides/00-welcome-246.html#communicate",
    "href": "slides/00-welcome-246.html#communicate",
    "title": "Welcome to MATH 246",
    "section": "Communicate",
    "text": "Communicate"
  },
  {
    "objectID": "slides/00-welcome-246.html#understand-communicate",
    "href": "slides/00-welcome-246.html#understand-communicate",
    "title": "Welcome to MATH 246",
    "section": "Understand + communicate",
    "text": "Understand + communicate"
  },
  {
    "objectID": "slides/00-welcome-246.html#program",
    "href": "slides/00-welcome-246.html#program",
    "title": "Welcome to MATH 246",
    "section": "Program",
    "text": "Program"
  },
  {
    "objectID": "slides/00-welcome-246.html#homepage",
    "href": "slides/00-welcome-246.html#homepage",
    "title": "Welcome to MATH 246",
    "section": "Homepage",
    "text": "Homepage\nhttps://jmochogi.github.io/intermediate-stats/\n\nMost course materials (over 90%) will be posted here.\nLinks to Canvas, GitHub, RStudio website, etc."
  },
  {
    "objectID": "slides/00-welcome-246.html#course-toolkit",
    "href": "slides/00-welcome-246.html#course-toolkit",
    "title": "Welcome to MATH 246",
    "section": "Course toolkit",
    "text": "Course toolkit\nAll linked from the course website:\n\nAccess RStudio at posit cloud\nDiscussions: Canvas Discussion Forums\nAssignment submission and feedback: Canvas Gradebook"
  },
  {
    "objectID": "slides/00-welcome-246.html#workflow",
    "href": "slides/00-welcome-246.html#workflow",
    "title": "Welcome to MATH 246",
    "section": "Workflow",
    "text": "Workflow\n\nGet introduce new content and prepare for lectures by completing the readings and/or watching the videos.\nCheck your understanding by completing CPA assignment\nAttend class and participate actively to dive deeper into the material and learn how to apply it in the real-world contexts.\nPractice what you’ve learned by completing the labs and problem sets."
  },
  {
    "objectID": "slides/00-welcome-246.html#exams",
    "href": "slides/00-welcome-246.html#exams",
    "title": "Welcome to MATH 246",
    "section": "Exams",
    "text": "Exams\n\nTwo midterm exams.\nEach exam comprised of two parts:\n\nIn class: 55 minute in-class exam. Closed book, one sheet of notes (“cheat sheet”, no larger than A4, both sides, will be allowed this must be prepared by you) – 60% of the grade.\nTake home: The take home portion will comprise of data analysis is R. Copilot may be used, but all the prompts should be written down as comments. If the code produced is incorrect for some reason, state the errors and explain how you fixed them – 40% of the grade.\n\n\n\n\n\n\n\n\nCaution\n\n\nNo make-up exams will be given. If you anticipate that you will miss an exam, please let me know as soon as possible."
  },
  {
    "objectID": "slides/00-welcome-246.html#projects",
    "href": "slides/00-welcome-246.html#projects",
    "title": "Welcome to MATH 246",
    "section": "Projects",
    "text": "Projects\n\nTwo projects\nDataset of your choice (collect your own or use existing data), method of your choice.\nTeamwork. Every team member must list their role in the project.\nPresentation and write-up.\nPeer review other team’s work, peer evaluation for team contribution.\nSome lab sessions may be allocated to working on projects, doing peer review, and getting feedback."
  },
  {
    "objectID": "slides/00-welcome-246.html#teams",
    "href": "slides/00-welcome-246.html#teams",
    "title": "Welcome to MATH 246",
    "section": "Teams",
    "text": "Teams\n\nAssigned randomly by prof. at beginning of semester\nWill be shuffled mid-way into the semester with input from you\nProject: Each team will meet to discuss another team’s project and provide written feedback. Each team member can choose a specific role in peer review.\nExpectations and roles\n\nEveryone is expected to contribute equal effort\nEveryone is expected to understand all code used to do the analyses of your report.\nIndividual contribution evaluated by team members, my observations, etc."
  },
  {
    "objectID": "slides/00-welcome-246.html#grading",
    "href": "slides/00-welcome-246.html#grading",
    "title": "Welcome to MATH 246",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nCPA’s\n10%\n\n\nProblem Sets\n15%\n\n\nProjects\n20%\n\n\nMidterm Exams\n20%\n\n\nQuizzes\n10%\n\n\nLabs\n15%\n\n\nFinal Portfolio\n15%\n\n\n\nSee course syllabus for how the final letter grade will be determined."
  },
  {
    "objectID": "slides/00-welcome-246.html#support",
    "href": "slides/00-welcome-246.html#support",
    "title": "Welcome to MATH 246",
    "section": "Support",
    "text": "Support\n\nAttend open hours\nMath Support Center (Will Hall rm. 209)\nAsk and answer questions on the Canvas discussion forum\nReserve email for questions on personal matters and/or grades\nRead the course support page"
  },
  {
    "objectID": "slides/00-welcome-246.html#announcements",
    "href": "slides/00-welcome-246.html#announcements",
    "title": "Welcome to MATH 246",
    "section": "Announcements",
    "text": "Announcements\n\nPosted on Canvas (Announcements tool) and/or sent via email, be sure to check both regularly.\nI’ll (try my best to) send a weekly update announcement each Friday, outlining the plan for the following week and reminding you what you need to do to stay on track."
  },
  {
    "objectID": "slides/00-welcome-246.html#diversity-inclusion",
    "href": "slides/00-welcome-246.html#diversity-inclusion",
    "title": "Welcome to MATH 246",
    "section": "Diversity + Inclusion",
    "text": "Diversity + Inclusion\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\n\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. I want to be a resource for you. If you prefer to speak with someone outside of the course, your advisors, and deans are excellent resources."
  },
  {
    "objectID": "slides/00-welcome-246.html#accessibility",
    "href": "slides/00-welcome-246.html#accessibility",
    "title": "Welcome to MATH 246",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Accessibility Services (SAS) is available to ensure that students are able to engage with their courses and related assignments.\nI am committed to making all course materials accessible and I’m always learning how to do this better. If any course component is not accessible to you in any way, please don’t hesitate to let me know."
  },
  {
    "objectID": "slides/00-welcome-246.html#late-work-mobile-phones-.-.-.",
    "href": "slides/00-welcome-246.html#late-work-mobile-phones-.-.-.",
    "title": "Welcome to MATH 246",
    "section": "Late work, mobile phones, . . .",
    "text": "Late work, mobile phones, . . .\n\nWe have policies!\nRead about them on the course syllabus and refer back to them when you need it."
  },
  {
    "objectID": "slides/00-welcome-246.html#this-weeks-tasks",
    "href": "slides/00-welcome-246.html#this-weeks-tasks",
    "title": "Welcome to MATH 246",
    "section": "This week’s tasks",
    "text": "This week’s tasks\n\nRead the syllabus\nComplete quiz 1\nComplete Labs:\n\nLab 0: Meet R (Wed)\nLab 1: Meet Quarto (Friday)\n\nStart readings for next week"
  },
  {
    "objectID": "project/2-proposal.html",
    "href": "project/2-proposal.html",
    "title": "Proposal",
    "section": "",
    "text": "The goals of this milestone are as follows:\n\nDiscuss topics you’re interested in investigating and find data sets on those topics.\nIdentify 3 data sets you’re interested in potentially using for the project.\nGet these datasets into R.\nWrite up reasons and justifications for why you want to work with these datasets.\nReview your team contract.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou must use one of the data sets in the proposal for the final project, unless instructed otherwise when given feedback."
  },
  {
    "objectID": "project/2-proposal.html#criteria-for-datasets",
    "href": "project/2-proposal.html#criteria-for-datasets",
    "title": "Proposal",
    "section": "Criteria for datasets",
    "text": "Criteria for datasets\nThe data sets should meet the following criteria:\n\nAt least 500 observations\nAt least 8 columns\nAt least 6 of the columns must be useful and unique explanatory variables.\n\nIdentifier variables such as “name”, “social security number”, etc. are not useful explanatory variables.\nIf you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique explanatory variables.\n\nYou may not use data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\nYou can curate one of your datasets via web scraping.\n\nPlease ask a member of the teaching team if you’re unsure whether your data set meets the criteria.\nIf you set your hearts on a dataset that has fewer observations or variables than what’s suggested here, that might still be ok; use these numbers as guidance for a successful proposal, not as minimum requirements."
  },
  {
    "objectID": "project/2-proposal.html#resources-for-datasets",
    "href": "project/2-proposal.html#resources-for-datasets",
    "title": "Proposal",
    "section": "Resources for datasets",
    "text": "Resources for datasets\nYou can find data wherever you like, but here are some recommendations to get you started. You shouldn’t feel constrained to datasets that are already in a tidy format, you can start with data that needs cleaning and tidying, scrape data off the web, or collect your own data.\n\nAwesome public datasets\nBikeshare data portal\nCDC\nData.gov\nData is Plural\nDurham Open Data Portal\nEdinburgh Open Data\nElection Studies\nEuropean Statistics\nCORGIS: The Collection of Really Great, Interesting, Situated Datasets\nGeneral Social Survey\nGoogle Dataset Search\nHarvard Dataverse\nInternational Monetary Fund\nIPUMS survey data from around the world\nLos Angeles Open Data\nNHS Scotland Open Data\nNYC OpenData\nOpen access to Scotland’s official statistics\nPew Research\nPRISM Data Archive Project\nStatistics Canada\nTidyTuesday\nThe National Bureau of Economic Research\nUCI Machine Learning Repository\nUK Government Data\nUNICEF Data\nUnited Nations Data\nUnited Nations Statistics Division\nUS Census Data\nUS Government Data\nWorld Bank Data\nYouth Risk Behavior Surveillance System (YRBSS)\nFRED Economic Data"
  },
  {
    "objectID": "project/2-proposal.html#introduction-and-data",
    "href": "project/2-proposal.html#introduction-and-data",
    "title": "Proposal",
    "section": "Introduction and data",
    "text": "Introduction and data\nFor each data set:\n\nIdentify the source of the data.\nState when and how it was originally collected (by the original data curator, not necessarily how you found the data).\nWrite a brief description of the observations.\nAddress ethical concerns about the data, if any."
  },
  {
    "objectID": "project/2-proposal.html#research-question",
    "href": "project/2-proposal.html#research-question",
    "title": "Proposal",
    "section": "Research question",
    "text": "Research question\nYour research question should contain at least three variables, and should be a mix of categorical and quantitative variables. When writing a research question, please think about the following:\n\nWhat is your target population?\nIs the question original?\nCan the question be answered?\n\nFor each data set, include the following:\n\nA well formulated research question. (You may include more than one research question if you want to receive feedback on different ideas for your project. However, one per data set is required.)\nStatement on why this question is important.\nA description of the research topic along with a concise statement of your hypotheses on this topic.\nIdentify the types of variables in your research question. Categorical? Quantitative?"
  },
  {
    "objectID": "project/2-proposal.html#glimpse-of-data",
    "href": "project/2-proposal.html#glimpse-of-data",
    "title": "Proposal",
    "section": "Glimpse of data",
    "text": "Glimpse of data\nFor each data set:\n\nPlace the file containing your data in the data folder of the project repo.\nUse the glimpse() function to provide a glimpse of the data set."
  },
  {
    "objectID": "project/4-writeup-presentation.html",
    "href": "project/4-writeup-presentation.html",
    "title": "Write-up and presentation",
    "section": "",
    "text": "Your written report must be completed in the index.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\nBefore you finalize your write up, make sure the printing of code chunks is off with the option echo: false in the YAML.\nThe mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long (if it were to be printed). There is no minimum page requirement; however, you should comprehensively address all of the analysis in your report.\nTo check how many pages your report is, open it in your browser and go to File &gt; Print &gt; Save as PDF and review the number of pages.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\n\n\n\n\n\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The exploratory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\n\nThis section includes a brief description of your analysis process. Explain the reasoning for the types of analyses you do, exploratory, inferential, or modeling. If you’ve chosen to do inference, make sure to include a justification for why that inferential approach is appropriate. If you’ve chosen to do modeling, describe the model(s) you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\n\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to determine analyses types and addressed any concerns over appropriateness of analyses chosen.\n\n\n\n\nThis is where you will discuss your overall finding and describe the key results from your analysis. The goal is not to interpret every single element of an output shown, but instead to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\n\nThe analysis results are clearly assesses and interesting findings from the analysis are described. Interpretations are used to to support the key findings and conclusions, rather than merely listing, e.g., the interpretation of every model coefficient.\n\n\n\n\nIn this section you’ll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\n\nOverall conclusions from analysis are clearly described, and the analysis results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\n\nThis is an assessment of the overall presentation and formatting of the written report.\n\n\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages.\n\n\n\n\n\nThe write-up is worth 40 points, broken down as follows:\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion\n6 pts\n\n\nOrganization + formatting\n4 pts"
  },
  {
    "objectID": "project/4-writeup-presentation.html#expectations",
    "href": "project/4-writeup-presentation.html#expectations",
    "title": "Write-up and presentation",
    "section": "",
    "text": "Your written report must be completed in the index.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\nBefore you finalize your write up, make sure the printing of code chunks is off with the option echo: false in the YAML.\nThe mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long (if it were to be printed). There is no minimum page requirement; however, you should comprehensively address all of the analysis in your report.\nTo check how many pages your report is, open it in your browser and go to File &gt; Print &gt; Save as PDF and review the number of pages.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit."
  },
  {
    "objectID": "project/4-writeup-presentation.html#components",
    "href": "project/4-writeup-presentation.html#components",
    "title": "Write-up and presentation",
    "section": "",
    "text": "This section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The exploratory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\n\nThis section includes a brief description of your analysis process. Explain the reasoning for the types of analyses you do, exploratory, inferential, or modeling. If you’ve chosen to do inference, make sure to include a justification for why that inferential approach is appropriate. If you’ve chosen to do modeling, describe the model(s) you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\n\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to determine analyses types and addressed any concerns over appropriateness of analyses chosen.\n\n\n\n\nThis is where you will discuss your overall finding and describe the key results from your analysis. The goal is not to interpret every single element of an output shown, but instead to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\n\nThe analysis results are clearly assesses and interesting findings from the analysis are described. Interpretations are used to to support the key findings and conclusions, rather than merely listing, e.g., the interpretation of every model coefficient.\n\n\n\n\nIn this section you’ll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\n\nOverall conclusions from analysis are clearly described, and the analysis results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\n\nThis is an assessment of the overall presentation and formatting of the written report.\n\n\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages."
  },
  {
    "objectID": "project/4-writeup-presentation.html#grading",
    "href": "project/4-writeup-presentation.html#grading",
    "title": "Write-up and presentation",
    "section": "",
    "text": "The write-up is worth 40 points, broken down as follows:\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion\n6 pts\n\n\nOrganization + formatting\n4 pts"
  },
  {
    "objectID": "project/4-writeup-presentation.html#slides",
    "href": "project/4-writeup-presentation.html#slides",
    "title": "Write-up and presentation",
    "section": "Slides",
    "text": "Slides\nIn addition to the written report, your team will also create presentation slides and deliver presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. These slides should serve as a brief visual addition to your written report and will be graded for content and quality.\nYou can create your slides with any software you like (Keynote, PowerPoint, Google Slides, etc.). We recommend choosing an option that’s easy to collaborate with, e.g., Google Slides. If you choose this option, save the slides as PDF and upload it to your repo as presentation.pdf.\nYou can also use Quarto to make your slides! While we won’t be covering making slides with Quarto in the class, we would be happy to help you with it in office hours. It’s no different than writing other documents with Quarto, so the learning curve will not be steep!\nThe slide deck should be roughly 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4-5: Inference/modeling/other analysis\nSlide 6: Conclusions + future work"
  },
  {
    "objectID": "project/4-writeup-presentation.html#presentation",
    "href": "project/4-writeup-presentation.html#presentation",
    "title": "Write-up and presentation",
    "section": "Presentation",
    "text": "Presentation\nPresentations will take place in class during the last lab of the semester. The presentation must be no longer than 5 minutes. You can choose to present live in class (recommended) or pre-record a video to be shown in class. Either way you must attend the lab session for the Q&A following your presentation.\nIf you choose to pre-record your presentation, you may use can use any platform that works best for your group to record your presentation. Below are a few resources on recording videos:\n\nRecording presentations in Zoom\nApple Quicktime for screen recording\nWindows 10 built-in screen recording functionality\nKap for screen recording\n\nOnce your video is ready, upload the video to Warpwire or another video platform (e.g., YouTube), then add a link to your video in your repo README.\nTo upload your video to Warpwire:\n\nClick the Warpwire tab in the course Canvas site.\nClick the “+” and select “Upload files”.\nLocate the video on your computer and click to upload.\nOnce you’ve uploaded the video to Warpwire, click to share the video and copy the video’s URL. You will need this when you post the video in the discussion forum."
  },
  {
    "objectID": "project/4-writeup-presentation.html#grading-1",
    "href": "project/4-writeup-presentation.html#grading-1",
    "title": "Write-up and presentation",
    "section": "Grading",
    "text": "Grading\nThe presentation is worth 25 points, broken down as follows:\n\n\n\nTotal\n25 pts\n\n\n\n\nSlides\n10 pts\n\n\nPresentation\n15 pts\n\n\n\n\nSlides\nAre the slides well organized, readable, not full of text, featuring figures with legible labels, legends, etc.?\n\n\nPresentation\n\nTime management: Did the team divide the time well amongst themselves or got cut off going over time?\nProfessionalism: How well did the team present? Does the presentation appear to be well practiced? Did everyone get a chance to say something meaningful about the project?\nTeamwork: Did the team present a unified story, or did it seem like independent pieces of work patched together?\nCreativity and critical thought: Is the project carefully thought out? Does it appear that time and effort went into the planning and implementation of the project?\nContent: Including, but not limited to the following:\n\nIs the question well articulated in the presentation?\nCan the question be answered with the data?\nDoes the analysis answer the question?\nAre the conclusion(s) made based on the analysis justifiable?\nAre the limitations carefully considered and articulated?"
  },
  {
    "objectID": "project/3-peer-review.html",
    "href": "project/3-peer-review.html",
    "title": "Peer review",
    "section": "",
    "text": "During the peer feedback process, you will be provided read-only access to your partner team’s GitHub repo. You will provide your feedback in the form of GitHub issues to your partner team’s GitHub repo."
  },
  {
    "objectID": "project/3-peer-review.html#part-1",
    "href": "project/3-peer-review.html#part-1",
    "title": "Peer review",
    "section": "Part 1",
    "text": "Part 1\nDuring the first ~30 minutes, make progress on your own project.\n\nReview the feedback from your TA.\nChoose the dataset you want to work with.\nMake progress on your analysis and write-up in index.qmd:\n\nAt a minimum, write a plan that peers can give feedback on.\nTime permitting, start the analysis.\n\nRender the project, commit and push all your changes.\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure you always render before you commit and push. If you don’t render first, you’re going to end up with merge conflicts that are difficult to resolve in auxiliary files created during the render process."
  },
  {
    "objectID": "project/3-peer-review.html#part-2",
    "href": "project/3-peer-review.html#part-2",
    "title": "Peer review",
    "section": "Part 2",
    "text": "Part 2\nReview one other team’s project. As a team you should spend ~30 minutes on each team’s project.\n\nFind the names of the teams whose projects you’re reviewing below. You should already have access to this team’s repo.\nEach team member should go to the repo of the team you’re reviewing.\nThen,\n\n1-2 team members clone the team’s project and renders it to check for reproducibility.\n1-2 team members open the team’s project in their browser and starts reading through the project draft.\n1 team member opens an issue on the team’s repo using the peer review template.\nAll team members discuss the project based on the prompts on the issue template and one team member records the feedback and submits the issue.\n\nTo open an issue in the repo you’re reviewing, click on New issue, and click on Get started for the Peer review issue. Fill out this issue, answering the following questions:\n\nPeer review by: [NAME OF TEAM DOING THE REVIEW]\nNames of team members that participated in this review: [FULL NAMES OF TEAM MEMBERS DOING THE REVIEW]\nDescribe the goal of the project.\nDescribe the data used or collected, if any. If the proposal does not include the use of a specific dataset, comment on whether the project would be strengthened by the inclusion of a dataset.\nDescribe the approaches, tools, and methods that will be used.\nProvide constructive feedback on how the team might be able to improve their project. Make sure your feedback includes at least one comment on the statistical reasoning aspect of the project, but do feel free to comment on aspects beyond the reasoning as well.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation.\nWere you able to reproduce the project by clicking on Render Website once you cloned it? Were there any issues with reproducibility?\nProvide constructive feedback on any issues with file and/or code organization.\nWhat have you learned from this team’s project that you are considering implementing in your own project?\n(Optional) Any further comments or feedback?"
  },
  {
    "objectID": "project/new.html",
    "href": "project/new.html",
    "title": "Project Overview",
    "section": "",
    "text": "The goal of this project is for you to demonstrate proficiency in the material we have covered in this class (and beyond, if you like) and apply them to a ‘novel’ dataset of your choosing in a meaningful way. You are not asked to do an exhaustive data analysis by generating every statistic/visualizations that we have learned of in the course. Rather, I expect you to demonstrate skills in asking meaningful questions and answering them using carefully selected data analysis techniques and tools. These analyses must be done in R in a reproducible manner (quarto).\nMultiple data frames are available but your group will choose just one to use for this project. After choosing the data frame, your will need to brainstorm on several issues about the data such as the sample and sampling methods that might have been used to gather the data, a possible target population, variables and their types (numerical vs categorical), among others. Your team will then come up with a research topic/goal and two research questions (directly related to the topic) that you will answer by conducting the analyses. Your analyses should include data visualizations, numerical statistics, and regression models. All these must be appropriately linked to help tell a coherent story."
  },
  {
    "objectID": "project/new.html#project-proposal",
    "href": "project/new.html#project-proposal",
    "title": "Project Overview",
    "section": "Milestone 1 - Proposal",
    "text": "Milestone 1 - Proposal\nThere are two main purposes of the project proposal:\n\nTo help you think about the project early, so you can get a head start on finding data, reading relevant literature, thinking about the questions you wish to answer, etc.\nTo ensure that the data you wish to analyze, methods you plan to use, and the scope of your analysis are feasible and will allow you to be successful for this project.\n\nThe project proposal will be due roughly one week after the project is assigned. It must contain the following details:\n\nTeam members and their roles: List the names of all team members and their roles in the project. Roles can include Data cleaning/wrangling, Data analysis (performing the analyses in RStudio), Writing (e.g., abstract, intro, literature review, findings, conclusion), Proof reading and typo fixing, Team leader (e.g., setting up meetings, follow up, communicate with professor), etc.\nResearch topic and Questions: As indicated earlier, this is a short statement that describes the general area of interest that your research questions will be addressing. You should have at least two research questions that you will answer by conducting the analyses.\nData: Describe the data you will be using. This should include the source of the data, the number of observations and variables, and a brief description of the variables and their types (e.g., numerical, categorical). You should also include a brief description of the data collection process. Note that the data collection process may not be available for all data sets. In this case, you are allowed to speculate (reasonably) on the data collection process and be sure to mention that in your proposal.\nAnalysis plan: Describe the analyses you plan to conduct. This should include the types of visualizations you plan to create, the numerical summaries you plan to calculate, and the regression models you plan to fit. You should also describe how you plan to link these analyses together to tell a coherent story.\n\nInstructions and grading criteria for this milestone are outlined in the grading rubric for Milestone 1."
  },
  {
    "objectID": "project/new.html#milestone-2---first-draft",
    "href": "project/new.html#milestone-2---first-draft",
    "title": "Project Overview",
    "section": "Milestone 2 - First Draft",
    "text": "Milestone 2 - First Draft\nYour project draft is a completed project, that we shall use to obtain feedback from the professor and peers in class. The draft should include the following sections:\n\nAbstract: This section should provide a brief overview of the research topic, questions, data, and analyses you are conducting. It should also provide a brief overview of the results you have obtained. Think of the abstract as an executive summary of your work that someone can read and understand without reading the rest of the report. You should write the abstract last, after you have completed the rest of the report.\nIntroduction: This section should introduce the research topic and questions you are addressing. It should also provide a brief overview of the data you are using and the analyses you plan to conduct. Cite a few sources that are relevant to your research topic.\nLiterature Review: This section should provide a brief overview of the literature related to your research topic. You should cite at least 5 sources and synthesize their methods and findings in relation to your own research. Guidelines for writing an effective literature review can be found here.\nMethods: This section should describe the data you are using and the analyses you plan to conduct. It should include the following subsections:\n\nData Description: This section should describe the data you are using. This should include the source of the data, the number of observations and variables, and a brief description of the variables and their types (e.g., numerical, categorical). You should also include a brief description of the data collection process. Note that the data collection process may not be available for all data sets. In this case, you are allowed to speculate (reasonably) on the data collection process and be sure to mention that in your proposal.\nResearch Questions and Analyses: This section should list the research questions you are addressing. You should also describe the variables of interest in your data set that will help you answer these questions. Discuus the methids you plan to use for each question and why you think they are appropriate.\nData Cleaning: This section should describe the steps you took to clean the data. This may include removing missing values, transforming variables, etc. Depending on your data set, you may not need to do much data cleaning. If this is the case, you should still describe the steps you took to clean the data, even if they are minimal.\n\nData Analysis and Results: This is one of the most important heavily weighted parts of your project. The analyses should be sound and justified appropriately (under methods). Here, you just need to implement the analyses described under the methodology section of your paper. Your analyses should demonstrate a thorough understanding of statistical concepts learned in this course (and beyond, if you like). Your data analyses must include both numerical statistics and data visualizations. You must also use regression modelling (linear or logistic) techniques as part of your analyses. Interpretations of visualizations created as well as the models created must be made in a coherent manner and in the context of the research questions. These are the results/findings of your project.\nConclusion: This section should summarize the research questions you addressed, the data you used, the analyses you conducted, and the results you obtained. Findings of your study must be tied back to those reported in the literature review; for example, do your findings concur with previous studies or do they contradict them? It should also discuss the implications of your results and suggest directions for future research. Be sure to state any limitations of your study and what would be done differently if you were to do the study again.\nReferences: This section should list all the sources you cited in your report. Use APA format for your references.\n\nInstructions and grading criteria for this milestone are outlined in Milestone 2: Proposal."
  },
  {
    "objectID": "project/new.html#peer-review",
    "href": "project/new.html#peer-review",
    "title": "Project Overview",
    "section": "Milestone 3 - Peer review",
    "text": "Milestone 3 - Peer review\nCritically reviewing others’ work is a crucial part of the scientific process, and MATH 246 is no exception. Your team will be assigned at least one project from another team to review. Team members should read individually before meeting during which the team will come up with collective feedback for the other team. The peer review process will be double blinded, meaning that you will not know who is reviewing your project, and you will not know whose project you are reviewing. This feedback is intended to help you create a high quality final project, as well as give you experience reading and constructively critiquing the work of others.\nInstructions and grading criteria for this milestone are outlined in Milestone 3: Peer review."
  },
  {
    "objectID": "project/new.html#milestone-4---final-writeup",
    "href": "project/new.html#milestone-4---final-writeup",
    "title": "Project Overview",
    "section": "Milestone 4 - Final Writeup",
    "text": "Milestone 4 - Final Writeup\nThe final write-up is the culmination of your project. It should include all the sections of the first draft, but with revisions based on the feedback you received from your peers and the professor. The final write-up should be a polished document that clearly communicates your research topic, questions, data, analyses, and results. The final write-up should be written in a clear, concise, and professional manner. It should be free of grammatical errors and typos. The final write-up should also include a title page with the title of your project, the names of all team members, and the date. A quarto template for the final write-up will be provided.\nInstructions and grading criteria for this milestone are outlined in Milestone 4: Writeup + presentation."
  },
  {
    "objectID": "project/new.html#milestone-5---presentation",
    "href": "project/new.html#milestone-5---presentation",
    "title": "Project Overview",
    "section": "Milestone 5 - Presentation",
    "text": "Milestone 5 - Presentation\nThe final milestone is a presentation of your project. The presentation should be about 10 minutes long and should include slides that summarize the key points and findings of your project. The presentation should be clear, concise, and professional. The presentation will be graded on the clarity of the slides, the clarity of the presentation, and the ability to communicate your research topic, questions, data, analyses, and results to an audience. The presentation will be followed by a question and answer session with the professor and your peers."
  },
  {
    "objectID": "project/new.html#overall-grading",
    "href": "project/new.html#overall-grading",
    "title": "Project Overview",
    "section": "Overall grading",
    "text": "Overall grading\nThe overall grade breakdown by milestone (\\(M_i\\)) is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nM1: Project Proposal\n10 pts\n\n\nM2: First Draft\n20 pts\n\n\nM3: Peer Review\n10 pts\n\n\nM4: Final Writeup\n40 pts\n\n\nM5: Slides + Presentation\n10 pts\n\n\nCollaboration & Teamwork\n10 pts"
  },
  {
    "objectID": "project/new.html#grading-milestone-details",
    "href": "project/new.html#grading-milestone-details",
    "title": "Project Overview",
    "section": "Grading Milestone details",
    "text": "Grading Milestone details\n\n\\(M_1\\) - Project Proposal\n\nCompletion (2pts) - Did the team complete all parts of the proposal as required?\nContent (2pts) - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness (2pts) - Are the statistical procedures sound and reasonable for the stated questions?\nCommunication & Writing (2pts) - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought (2pts) - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\n\n\n\\(M_2\\) - First Draft\n\nAbstract (2 pts) - Does the abstract provide a brief overview of the research topic, questions, data, and analyses you are conducting?\nIntroduction (2 pts) - Does the introduction articulate the research topic and questions you are addressing? Does it provide a brief overview of the data? Does it provide a couple of citations to help you make the case for your chosen topic of study?\nLiterature Review (3 pts) - Does the literature review provide relevant in-text citations? Does it examine and synthesize the sources in context of the current study?\nMethods (3 pts) - Does the methods section describe the sample, the data, and the analysis methods used? Does it justify the choice of analysis methods? Does it describe the data cleaning processes?\nData Analysis and Results (6 pts) - Are the analyses implemented correctly? Are the interpretations for both research questions done correctly and in context of the study? Do the analyses demonstrate a thorough understanding of statistical concepts learned in the course?\nConclusion (2 pts) - Does the conclusion summarize the research questions you addressed, the data you used, the analyses you conducted, and the results you obtained? Does it discuss the implications of your results and suggest directions for future research?\nReferences (2 pts) - Are all the sources you cited in your report listed in the references section? Are they in APA format?\n\n\n\n\\(M_3\\) - Peer Review\nPeer reviews will be graded on the extent to which it comprehensively and constructively addresses the components of the reviewee’s team’s report.\nOnly the team members participating in the review process are eligible for points for the peer review. Teams may choose to meet in-person or virtually. The team should submit a single review document that includes the following:\n\nQuality of feedback (3pts) - Did the team provide constructive and actionable feedback? Please note that actionable does not mean telling the reviewee team what to do, but rather providing feedback that the reviewee team can use to improve its work.\nCorrectness (2pts) - Are the critiques provided sound and reasonable?\nCommunication & Writing (2pts) - Is the feedback report understandable and free from grammatical errors and typos?\nCompletion (2pts) - Did the team complete the review as required?\n\n\n\n\\(M_4\\) - Final Writeup\nThe grading of the final writeup follows the same grading criteria as Milestone 2 but gets additional points. To get maximum points on milestone 4, you should address the feedback given to you on your project draft by your peers and/or professor. If there is any feedback that you think does not need to be addressed, please provide a justification for why you think so.\n\nAbstract (4 pts) - Does the abstract provide a brief overview of the research topic, questions, data, and analyses you are conducting?\nIntroduction (4 pts) - Does the introduction articulate the research topic and questions you are addressing? Does it provide a brief overview of the data? Does it provide a couple of citations to help you make the case for your chosen topic of study?\nLiterature Review (6 pts) - Does the literature review provide relevant in-text citations? Does it examine and synthesize the sources in context of the current study?\nMethods (6 pts) - Does the methods section describe the sample, the data, and the analysis methods used? Does it justify the choice of analysis methods? Does it describe the data cleaning processes?\nData Analysis and Results (14 pts) - Are the analyses implemented correctly? Are the interpretations for both research questions done correctly and in context of the study? Do the analyses demonstrate a thorough understanding of statistical concepts learned in the course?\nConclusion (4 pts) - Does the conclusion summarize the research questions you addressed, the data you used, the analyses you conducted, and the results you obtained? Does it discuss the implications of your results and suggest directions for future research?\nReferences (2 pts) - Are all the sources you cited in your report listed in the references section? Are they in APA format?\n\n\n\n\\(M_5\\) - Presentation\n\nSlides (3 pts) - Are the slides clear, well-organized, and concise? Do they summarize the key points and findings of the project?\nPresentation (4 pts) - Is the presentation clear and does it show a thorough understanding of the material learned? Are speakers audible enough and facing the audience? Are they engaging and do they maintain eye contact with the audience?\nQuestion and Answer (Q&A) (3 pts) - How well does the team answer questions about their project? Are they able to communicate their research topic, questions, data, analyses, and results to an audience?"
  },
  {
    "objectID": "project/new.html#submission",
    "href": "project/new.html#submission",
    "title": "Project Overview",
    "section": "Submission",
    "text": "Submission\nSubmissions for all milestones should be made through Canvas. You will see the assignments created in Canvas for each milestone with more detailed instructions."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course Support",
    "section": "",
    "text": "There are various resources available to help you succeed in this course and in college in general. Should you feel like you are struggling too much, please don’t hesitate to reach out to me so we can discuss possible ways forward. Below are some support services available to you.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#open-hours",
    "href": "course-support.html#open-hours",
    "title": "Course Support",
    "section": "Open Hours",
    "text": "Open Hours\n\nI will be available during open hours (Mon 12.00 - 01.00 pm) to answer questions or concerns that you may have in the course. You can simply walk in during the stated time above. Even if you don’t have any question, you can just stop by to say hi. If the above posted times don’t work for you, please click this link to see more options. Open hours may be held in-person or virtually depending on your preference and the circumstances of the day. Below are the zoom link and passcode for virtual meetings:\nZoom Link: click here\nPasscode: 850 424",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#mathematics-support-center",
    "href": "course-support.html#mathematics-support-center",
    "title": "Course Support",
    "section": "Mathematics Support Center",
    "text": "Mathematics Support Center\nThe mathematics department is committed to the success of all students enrolled in mathematics courses. Free one-on-one support for your mathematics coursework is available during select daytime and evening hours Monday-Friday at the Mathematics Room (Williams Hall 209). The Mathematics Room is staffed by mathematics faculty and vetted students. Student tutors offer support to fellow students in courses numbered 200 and below while math faculty offer support in any of the math courses. For more information and the schedule, please visit the Math Support Center.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#discussion-forums",
    "href": "course-support.html#discussion-forums",
    "title": "Course Support",
    "section": "Discussion Forums",
    "text": "Discussion Forums\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The Canvas Discussion forums, might be the best venue for these! There is a chance another student has already asked a similar question, so please check the other posts on before asking a new question. If you know the answer to a question that is posted, I encourage you to respond! In Canvas discussion section, anybody is able to start threads and others can jump in with replies.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course Support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go to Canvas Discussion forums), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). My email address is jgeteregechi@ithaca.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “MATH 246” in the subject line. Barring extenuating circumstances, I will respond to MATH 246 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday. If I take too long, please reach out again or talk to me before or after class as I may have missed your email.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#tutoring-and-academic-enrichment-services",
    "href": "course-support.html#tutoring-and-academic-enrichment-services",
    "title": "Course Support",
    "section": "Tutoring and Academic Enrichment Services",
    "text": "Tutoring and Academic Enrichment Services\nAs a supplement to faculty advising and office hours, Tutoring and Academic Enrichment Services offers exceptional peer resources free of charge. Learning Coaches provide content-specific peer tutoring in a variety of courses. Peer Success Coaches mentor students who wish to develop collegiate-level academic and social engagement skills. To access these courses and for more information, please visit the Center for Student Success.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#writing-center",
    "href": "course-support.html#writing-center",
    "title": "Course Support",
    "section": "Writing Center",
    "text": "Writing Center\nThe Writing Center aims to help students from all disciplines, backgrounds, and experiences to develop greater independence as writers. We are committed to helping students see writing as central to critical and creative thinking. The physical location in Smiddy 107 will not be open to clients. For more information and scheduling appointments please visit the writing center website.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course Support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Please visit the Mental Health Services for more information. Ithaca College encourages all students to access these resources.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-costs",
    "href": "course-support.html#course-costs",
    "title": "Course Support",
    "section": "Course costs",
    "text": "Course costs\n\nTextbooks: The textbooks for this course are open-source and freely available on the web.\nLaptops: Each student is expected to have a laptop they can bring to each lecture and lab.\nSoftware subscriptions:\n\nRStudio: RStudio cloud (posit cloud) is available for a $5 per month subscription for students. I may be able to have funds to sign up about 16 students this semester. The criteria is to have at least two group members with a full licence for the semester. Although the software has a free version, it allows only 25 hours of use per month. The positive side is that it does come with all the functionality that we will need for the course.\nGitHub: You will need to sign up for a free GitHub account. We will not use GitHub as much but you will need to sign up for a student developer pack to get access to GitHub copilot for free as a student. Be sure to use your Ithaca College email while signing up.\nGitHub Copilot: GitHub Copilot is free for students. You will need to sign up for the student developer pack before you sign up for a free GitHub copilot account.\n\nThe detailed instructions for signing up for GitHub copilot are available on this site",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-canvas",
    "href": "course-support.html#assistance-with-zoom-or-canvas",
    "title": "Course Support",
    "section": "Assistance with Zoom or Canvas",
    "text": "Assistance with Zoom or Canvas\nFor technical help with Canvas or Zoom, contact the Ithaca College IT support team at (607) 274-1000 or email them at servicedesk@ithaca.edu",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "Quizzes/Quiz_1.html",
    "href": "Quizzes/Quiz_1.html",
    "title": "Quiz 1: Syllabus Quiz",
    "section": "",
    "text": "What is the attendance policy for this class? Under what circumstances may an absence be excused? State at least 2.\nUnder what circumstances may an assignment be submitted late? Are there some assignments that cannot be submitted late?\n\n\nWhat is the policy on academic integrity for this class? What are the consequences of academic dishonesty?\nWhat is the policy on electronic devices in class?\nWhere would a student find course materials, and assignment deadlines?\nWhat support resources are available to help a student who is struggling with course material?\nStudent X gets a cumulative numerical grade of 82.99 what would the student’s final letter grade?"
  },
  {
    "objectID": "problem-sets/ps-2.html",
    "href": "problem-sets/ps-2.html",
    "title": "Problem Set 2: Linear Regression",
    "section": "",
    "text": "Due to COVID-19 pandemic, many states made alternatives in voting, such as voting by mail, more widely available for the 2020 U.S. election. The general consensus was that voters who were more Democratic leaning would be more likely to vote by mail, while more Republican leaning voters would largely vote in-person. This was supported by multiple surveys, including this survey conducted by Pew Research.\nThe goal of this analysis is to use regression analysis to explore the relationship between a county’s political leanings and the proportion of votes cast in-person in 2020. The ultimate question we want to answer is Did counties with more Republican leanings have a larger proportion of votes cast in-person in the 2020 election?\nWe will use the proportion of votes cast for Donald Trump in 2016 (pctTrump_2016) as a measure of a county’s political leaning. Counties with a higher proportion of votes for Trump in 2016 are considered to have more Republican leanings.\nWe will focus on the following variables for this analysis:",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problem-sets/ps-2.html#importing-the-data",
    "href": "problem-sets/ps-2.html#importing-the-data",
    "title": "Problem Set 2: Linear Regression",
    "section": "Importing the Data",
    "text": "Importing the Data\nUp to this point in this course we have been using data frames that we either created in R or loaded from some package. In this problem set, you will import a dataset in csv format into your RStudio cloud and into your quarto work space before you proceed. The data set is available on this link",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problem-sets/ps-2.html#problem-1-data-visualization-summary-stats",
    "href": "problem-sets/ps-2.html#problem-1-data-visualization-summary-stats",
    "title": "Problem Set 2: Linear Regression",
    "section": "Problem 1 (data visualization & summary stats)",
    "text": "Problem 1 (data visualization & summary stats)\n\nCreate a visualization for the variable inperson_pct and calculate at least two summary statistics to measure the central tendency, as well as one summary statistic to measure the spread. Then, interpret both the visualization and the summary statistics in context of the data.\nCreate a visualization of the relationship between inperson_pct and pctTrump_2016. Based on the visualization, how would you describe the relationship?\nCompute the correlation coefficient between inperson_pct and pctTrump_2016. Interpret the value in the context of the data.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problem-sets/ps-2.html#problem-2-linear-regression",
    "href": "problem-sets/ps-2.html#problem-2-linear-regression",
    "title": "Problem Set 2: Linear Regression",
    "section": "Problem 2 (linear regression)",
    "text": "Problem 2 (linear regression)\n\nFit the linear model to understand variability in the percent of in-person votes based on the percent of votes for Trump in the 2016 election. Write down the equation of the model.\nInterpret the slope of the model. The interpretation should be written in a way that is meaningful in the context of the data.\nDoes it make sense to interpret the intercept of the model? If so, write the interpretation in the context of the data. Otherwise, briefly explain why not.\nCalculate the R-squared value for the model. Interpret the value of the R-squared in the context of the data.\nUse the linear model to predict the proportion of in-person votes in a county where 60% of the votes were cast for Trump in 2016.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problem-sets/ps-2.html#problem-3-model-diagnostics",
    "href": "problem-sets/ps-2.html#problem-3-model-diagnostics",
    "title": "Problem Set 2: Linear Regression",
    "section": "Problem 3 (model diagnostics)",
    "text": "Problem 3 (model diagnostics)\n\nCalculate the residuals for the linear model. Create a histogram of the residuals and describe the distribution of the residuals.\nCreate a residual plot - scatterplot of the residuals against the fitted values. Describe the relationship between the residuals and the fitted values.\nAssuming that the cases (counties) in the dataset are independent, explain whether the assumptions of the linear regression model are met.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problem-sets/ps-2.html#problem-4-categorical-predictor",
    "href": "problem-sets/ps-2.html#problem-4-categorical-predictor",
    "title": "Problem Set 2: Linear Regression",
    "section": "Problem 4 (categorical predictor)",
    "text": "Problem 4 (categorical predictor)\n\nCreate a binary variable calledpctTrump_2016_high with levels pro_trump and anti_trump that indicates whether the proportion of votes for Trump in 2016 was above 50% in a county.\nFit a linear regression model to predict the proportion of in-person votes based on the binary variable pctTrump_2016_high. Write down the equation of the model.\nInterpret the slope of the model. The interpretation should be written in a way that is meaningful in the context of the data.\nWhich model (the one in Problem 2 or the one in Problem 4) is more ‘’appropriate’’ for predicting the proportion of in-person votes in the 2020 election? Justify your answer.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "problem-sets/ps-4.html",
    "href": "problem-sets/ps-4.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "Please come back later!",
    "crumbs": [
      "Problem Sets",
      "Problem Set 4"
    ]
  }
]