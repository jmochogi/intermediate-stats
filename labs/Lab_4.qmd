---
title: "Lab 4 - Multiple Linear Regression"
categories: "Lab"
---

```{r, include=FALSE}
library(openintro)
library(tidyverse)
library(broom)
library(statsr)
library(GGally)
```

## [Introduction]{style="color:#4166f5;"}

Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics such as the physical appearance of the instructor. The article titled, ***“Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity”*** by Hamermesh and Parker, found that instructors who are viewed to be better looking receive higher instructional ratings. In this lab, you will analyze the data from this study in order to learn what goes into a positive professor evaluation. You will also use the GGally package for visualization relationships between many variables pairs at once and the broom package to tidy your regression outputs. The goal is to perform model selection in order to isolate the "most important "variables.

## [Creating a quarto file]{style="color:#4166f5;"}

-   Create a new Quarto document with the title **Multiple Linear Regression**. Change the output format to **pdf** (note that it is set to html by default). Refer to lab_00_Guide if you don't remember how to create a new quarto file. Save the file as `lab_04`.

-   Note that if you created the file correctly, it should appear under files with a `.qmd` extension(i.e., `lab_04.qmd`). If you do not have this file exactly as described, please **STOP** and make sure you have it done correctly before you proceed.

-   After correctly creating the file, click on `Render` to see the output in pdf format. Note that it may pop up in a new window. This step is just for ensuring that your document created properly and that you are able to generate a `pdf`, the format in which you will submit the your lab.

## [Packages]{style="color:#4166f5;"}

You will need the following packages for this lab: `openintro`, `tidyverse`, `statsr`, `GGally`, and `broom`. Recall that we have previously installed all these packages. All we need to do is load them into our work space. Use code below:

``` toml
library(openintro)
library(tidyverse)
library(statsr)
library(broom)
library(GGally)
```

Be sure to run the packages code chunk above to ensure that they are all loaded correctly. Remember to use `include=F` option (i.e., `{r, include=F}`) to prevent the code output for loading packages from showing up in your rendered report.

## [Loading (and viewing) Data]{style="color:#4166f5;"}

We will use a data frame called `evals` contained in the **openintro** package. The data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors’ physical appearance (`bty`). The result is a data frame where each row contains a different course and columns represent variables about the courses and professors.

Since we have already activated `openintro`, we can load the data by running the `data` command. See below:

```{r}
data(evals)
```

Once you run the chunk above, you should be able to see a new object called **evals** in the environment area. Click on it to examine the data.

First, let us learn more about the data by running the following command in the **console**.

``` toml
?evals
```

## [Examining Relationships]{style="color:#4166f5;"}

It is often useful to examine relationships between variables before using them to run regression models. The most commonly used tool for this are scatter plots. The problem here is that we have several variables and we would have to make several combinations of two. We want to use the `GGally` package to create a scatter plot matrix, that allows us to see the relationships between all the variables in the data set at once. To achieve this, we use the `pairs` function from the `GGally` package as shown below. Notice that we first select the columns of interest:

```{r}
evals|>
  select(score,rank,gender,ethnicity,language,cls_perc_eval,cls_level,cls_students,cls_credits,bty_avg,cls_credits)|>
  pairs()
```

## [Multiple Linear Regression Modeling]{style="color:#4166f5;"}

We want to start by creating a full model (we can name it `m_full`) that predicts professor `score` based on the following predictors: `rank`, `gender`, `ethnicity`, `language` (language of the university where they got their degree), `age`, `cls_perc_eval` (proportion of students that filled out evaluations), `cls_students` (number of students in class), `cls_level` (course level), `cls_profs` (number of professors), `cls_credits` (number of credits), and `bty_avg` (average beauty rating). Note that some of these variables are categorical and others are numerical. You can learn more about these variables including how they were measured by checking the documentation (you can run `?evals` in the console).

Before running the code below, answer the following question:

How many variables do you expect to be in the multiple regression model? How do you know?

```{r}
m_full <- lm(score ~ rank
                  + gender 
                  + ethnicity 
                  + language 
                  + age 
                  + cls_perc_eval 
                  + cls_students 
                  + cls_level 
                  + cls_profs 
                  + cls_credits 
                  + bty_avg, data = evals)
tidy(m_full)
```

**Questions:**

-   How many variables are in the model? Is this what you expected?

-   Interpret the coefficient associated with the `ethnicity` variable.

## [Model Pruning]{style="color:#4166f5;"}

The above model is called a **full model** because it contains all possible predictors. The full model is not always the best. We can improve on this model by dropping certain predictors that are not ***adding value*** to the model. We are going to use one of the **step-wise selection** methods (backward elimination) to prune the above model in order to improve it. We are trying to increase the adjusted $R^2$.

### [Step 1]{style="color:#4166f5;"}

We will start with the **full model** (`m_full`) and its adjusted $R^2$. Run the `glance` function to check the adjusted $R^2$ of the model above (m_full). Interpret the value in context.

```{r}
glance(m_full)
```

The output shows that the adjusted $R^2$ is $14.12\%$. We use this number as our baseline and drop predictors **one at a time**, each time checking on the improvement in the adjusted $R^2$. Our new model will be one that leads to the highest improvement in the adjusted $R^2$.

Let us start by dropping `rank` (we can call the model without rank `rm_rank` (meaning remove rank). We also run the `glance` command to get the adjusted $R^2$ for the model. See the code below:

```{r}
rm_rank <- lm(score ~ gender 
                    + ethnicity 
                    + language 
                    + age 
                    + cls_perc_eval 
                    + cls_students 
                    + cls_level 
                    + cls_profs 
                    + cls_credits 
                    + bty_avg, data = evals)
glance(rm_rank)
```

Dropping rank from the model gives an adjusted $R^2$ value of 14.18%. This is an improvement of 0.06 (14.18 - 14.12).

Next, we want to check the effect of dropping `gender` instead. See below:

```{r}
rm_gender <- lm(score ~ rank 
                      + ethnicity 
                      + language 
                      + age 
                      + cls_perc_eval 
                      + cls_students 
                      + cls_level 
                      + cls_profs 
                      + cls_credits 
                      + bty_avg, data = evals)
#tidy(rm_gender)
glance(rm_gender)
```

Note that dropping gender leads to a reduced adjusted $R^2$. So, this may not be a good idea at this stage.

Next, let us drop `ethnicity` and see what happens:

```{r}
rm_ethnicity <- lm(score ~ rank
                          + gender 
                          + language 
                          + age 
                          + cls_perc_eval 
                          + cls_students 
                          + cls_level 
                          + cls_profs 
                          + cls_credits 
                          + bty_avg, data = evals)
glance(rm_ethnicity)
```

What can you comment about the effect of dropping `ethnicity`?

Next, drop `language`:

```{r}
rm_language <- lm(score ~ rank
                        + ethnicity
                        + gender 
                        + age 
                        + cls_perc_eval 
                        + cls_students 
                        + cls_level 
                        + cls_profs 
                        + cls_credits 
                        + bty_avg, data = evals)
#tidy(rm_language)
glance(rm_language)
```

What is the effect of dropping language?

Next, drop `age`:

```{r}
rm_age <- lm(score ~ rank
                  + ethnicity
                  + gender
                  + language
                  + cls_perc_eval 
                  + cls_students 
                  + cls_level 
                  + cls_profs 
                  + cls_credits 
                  + bty_avg, data = evals)
glance(rm_age)
```

Comment on the effect of dropping `age`:

Next, drop `cls_perc_eval` (i.e., proportion of students that filled out evaluations):

```{r}
rm_cls_perc_eval <- lm(score ~ rank
                              + ethnicity
                              + age
                              + gender
                              + language
                              + cls_students 
                              + cls_level 
                              + cls_profs 
                              + cls_credits 
                              + bty_avg, data = evals)
glance(rm_cls_perc_eval)
```

Comment appropriately.

Next, drop `cls_students` (class size):

```{r}
rm_cls_students <- lm(score ~ rank
                            + ethnicity
                            + age
                            + gender
                            + cls_perc_eval
                            + language
                            + cls_level 
                            + cls_profs 
                            + cls_credits 
                            + bty_avg, data = evals)
glance(rm_cls_students)
```

Comment on the effect of dropping class size.

Next, drop `cls_level` (course level):

```{r}
rm_cls_level <- lm(score ~ rank
                          + ethnicity
                          + age
                          + gender
                          + cls_students
                          + cls_perc_eval
                          + language
                          + cls_profs 
                          + cls_credits 
                          + bty_avg, data = evals)
glance(rm_cls_level)
```

Does dropping course level lead to an increase in the adjusted $R^2$. if so, by how much?

<!-- 14.29-14.12=.17-->

We are almost done. Next, drop `cls_profs` (number of professors):

```{r}
rm_cls_profs <- lm(score ~ rank
                          + ethnicity
                          + age
                          + gender
                          + cls_students
                          + cls_level
                          + cls_perc_eval
                          + language
                          + cls_credits 
                          + bty_avg, data = evals)
glance(rm_cls_profs)
```

Does dropping `cls_profs` lead to an increase in adjusted $R^2$? By how much?

<!-- 14.3-14.12=.18-->

Next, drop `cls_credits` (i.e., number of credits):

```{r}
rm_cls_credits <- lm(score ~ rank
                          + ethnicity
                          + age
                          + gender
                          + cls_students
                          + cls_level
                          + cls_perc_eval
                          + language
                          + cls_profs
                          + bty_avg, data = evals)
glance(rm_cls_credits)
```

Comment on the effect of dropping `cls_credits`.

Finally, drop `bty_avg` (i.e., average beauty score):

```{r}
rm_bty_avg <- lm(score ~ rank
                        + ethnicity
                        + age
                        + gender
                        + cls_students
                        + cls_level
                        + cls_perc_eval
                        +cls_credits
                        + language
                        + cls_profs, data = evals)
glance(rm_bty_avg)
```

As you have seen, dropping `class_profs` leads to the most improvement to the model (the adjusted $R^2$ improves from 14.12% to 14.31%). So, we create a new model (and name it `m_prunned_1`) without `cls_profs` and check its adjusted $R^2$. See code below:

```{r}
m_prunned_1 <- lm(score ~ rank 
                        + gender 
                        + ethnicity 
                        + language 
                        + age 
                        + cls_perc_eval 
                        + cls_students 
                        + cls_level 
                        + cls_credits 
                        + bty_avg, data = evals)
#tidy(m_prunned_1)
glance(m_prunned_1)
```

The new adjusted $R^2$ is $14.31\%$. We will repeat the process above using 14.31% as the base value. We drop variables one at a time.

### [Step 2]{style="color:#4166f5;"}

We repeat the process above using `m_prunned_1` as the baseline. Remember, its adjusted $R^2$ is $14.31\%$.

First, drop gender:

```{r}
rm_gender <- lm(score ~ rank 
                      + ethnicity 
                      + language 
                      + age 
                      + cls_perc_eval 
                      + cls_students 
                      + cls_level 
                      + cls_credits 
                      + bty_avg, data = evals)
#tidy(rm_gender)
glance(rm_gender)
```

No improvement when we drop gender from the model. Next, let us remove rank:

```{r}
rm_rank <- lm(score ~ gender 
                    + ethnicity 
                    + language 
                    + age 
                    + cls_perc_eval 
                    + cls_students 
                    + cls_level 
                    + cls_credits 
                    + bty_avg, data = evals)
#tidy(rm_rank)
glance(rm_rank)
```

Improved: 14.36-14.31=0.05

Next, remove ethnicity,

```{r}
rm_ethnicity <- lm(score ~ rank 
                        + gender 
                        + language 
                        + age 
                        + cls_perc_eval 
                        + cls_students 
                        + cls_level 
                        + cls_credits 
                        + bty_avg, data = evals)
#tidy(rm_ethnicity)
glance(rm_ethnicity)
```

Next, remove `language`:

```{r}
rm_language <- lm(score ~ rank 
                        + gender 
                        + ethnicity 
                        + age 
                        + cls_perc_eval 
                        + cls_students 
                        + cls_level 
                        + cls_credits 
                        + bty_avg, data = evals)
#tidy(rm_language)
glance(rm_language)
```

No improvement.

Next, remove age:

```{r}
rm_age <- lm(score ~ rank 
                  + gender 
                  + ethnicity 
                  + language 
                  + cls_perc_eval 
                  + cls_students 
                  + cls_level 
                  + cls_credits 
                  + bty_avg, data = evals)
#tidy(rm_age)
glance(rm_age)
```

No improvement.

Next, remove `cls_perc_eval`:

```{r}
rm_cls_per_eval <- lm(score ~ rank 
                            + gender 
                            + ethnicity 
                            + language 
                            + age 
                            + cls_students 
                            + cls_level 
                            + cls_credits 
                            + bty_avg, data = evals)
#tidy(rm_cls_per_eval)
glance(rm_cls_per_eval)
```

No improvement.

Next, remove `cls_students`

```{r}
rm_cls_st <- lm(score ~ rank 
                      + gender 
                      + ethnicity 
                      + language 
                      + age 
                      + cls_perc_eval 
                      + cls_level 
                      + cls_credits 
                      + bty_avg, data = evals)
#tidy(rm_cls_st)
glance(rm_cls_st)
```

No improvement.

Next, remove `cls_level`

```{r}
rm_cls_level <- lm(score ~ rank 
                          + gender 
                          + ethnicity 
                          + language 
                          + age 
                          + cls_perc_eval 
                          + cls_students 
                          + cls_credits 
                          + bty_avg, data = evals)
#tidy(rm_cls_level)
glance(rm_cls_level)
```

Model improves by: 14.47-14.31=.16%

Next, remove `cls_credits`:

```{r}
rm_cls_credits <- lm(score ~ rank 
                          + gender 
                          + ethnicity 
                          + language 
                          + age 
                          + cls_perc_eval 
                          + cls_students 
                          + cls_level 
                          + bty_avg, data = evals)
#tidy(rm_cls_credits)
glance(rm_cls_credits)
```

No improvement.

Finally, we remove `bty_avg`:

```{r}
rm_bty_avg <- lm(score ~ rank 
                      + gender 
                      + ethnicity 
                      + language 
                      + age 
                      + cls_perc_eval 
                      + cls_students 
                      + cls_level 
                      + cls_credits, data = evals)
#tidy(rm_bty_avg)
glance(rm_bty_avg)
```

No improvement when you drop `bty_avg.`

We notice that removing `cls_level` leads to the most improvement in the adjusted $R^2$ of .16 ($14.47-14.31=.16$). So, our new improved model (call it `m_prunned_2`) is:

```{r}
m_prunned_2 <- lm(score ~ rank 
                        + gender 
                        + ethnicity 
                        + language 
                        + age 
                        + cls_perc_eval 
                        + cls_students 
                        + cls_credits 
                        + bty_avg, data = evals)
#tidy(rm_prunned_2)
glance(m_prunned_2)

```

### [Step 3]{style="color:#4166f5;"}

We use the model from step 2 above along with its adjusted $R^2$ (14.48%) as the baseline for step 3. The rest of the work is left as an exercise.

## [Exercises]{style="color:#4166f5;"}

1.  ***(8 pts)*** In class, we began the backward elimination method (based on adjusted R-squared) for model selection. Complete the steps to come up with the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on.

2.  ***(4 pts)*** Based on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a high evaluation score. What characteristics are associated with low course evaluation scores?

3.  ***(4 pts)*** Pick one slope for a numerical variable in your model and one for a categorical variable and interpret them in context.

4.  ***(4 pts)*** Would you be comfortable generalizing your conclusions to professors generally (at any university)? Why or why not?

## Submission

Once you are finished with the lab, you will submit your final PDF document to Canvas.

::: callout-warning
Before you wrap up the assignment, make sure all questions are numbered appropriately. Delete any code chunks that were not used.

You must turn in a PDF file to the Canvas page by the submission deadline to be considered "on time".
:::

::: callout-important
## Checklist

Make sure you have:

-   attempted all questions
-   rendered your Quarto document to PDF
-   uploaded your PDF to Canvas
:::
